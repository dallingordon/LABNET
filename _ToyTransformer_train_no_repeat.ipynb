{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6a04cb7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "99b445af",
   "metadata": {},
   "outputs": [],
   "source": [
    "from NoiseKD import Teacher,ToyTransformer, count_parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5a11b24",
   "metadata": {},
   "source": [
    "First hurdle. the embedding layer.  It requires the ints as inputs, not one hot.  i need one hot.  i will also need to rewire the LLM i use to do the same.  hmm.  save the weights, load it into a model that has the same shapes, but accepts one hot arrays, not vocab_indexes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cca2a6b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 16\n",
    "num_heads = 8\n",
    "hidden_dim  = 11\n",
    "num_layers = 2\n",
    "dropout = 0.1\n",
    "vocab_size = 80\n",
    "class_num = vocab_size\n",
    "batch_size = 50\n",
    "sequence_length = 160"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a7b5a7d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "TT = ToyTransformer(vocab_size, embedding_dim, num_heads, hidden_dim, num_layers, dropout,sequence_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a3ba38e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "215702"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_parameters(TT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "c613c4cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "teacher_toy = Teacher(TT,(sequence_length,)) #don't specify batch!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "5e4db9fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "##some of these configs made for more diverse outputs in teachers:\n",
    "config_args = {\"dist_type\" : \"ints\" ##worked well\n",
    "                      , \"gen_m\" : vocab_size\n",
    "                      , \"gen_n\" : 100\n",
    "                      , \"gen_epochs\" : 100\n",
    "                      , \"gen_lr\" : 0.001\n",
    "                      , \"random_shuffle\" : 0.1\n",
    "                      , \"out_type\" : \"one-hot\" }\n",
    "\n",
    "config_args_high_epochs = {\"dist_type\" : \"ints\" ##okay, but not as well as config_args.\n",
    "                      , \"gen_m\" : vocab_size\n",
    "                      , \"gen_n\" : 1000\n",
    "                      , \"gen_epochs\" : 100\n",
    "                      , \"gen_lr\" : 0.001\n",
    "                      , \"random_shuffle\" : 0.1\n",
    "                      , \"out_type\" : \"one-hot\" }\n",
    "\n",
    "config_args_higher_lr = {\"dist_type\" : \"ints\" ##lower was worse.  raise it. 0.003 looks great.  this is the best.\n",
    "                      , \"gen_m\" : vocab_size\n",
    "                      , \"gen_n\" : 2000\n",
    "                      , \"gen_epochs\" : 50\n",
    "                      , \"gen_lr\" :  0.003 ##0.003\n",
    "                      , \"random_shuffle\" : 0.8\n",
    "                      , \"out_type\" : \"one-hot\" }\n",
    "\n",
    "config_args_less_data = {\"dist_type\" : \"ints\" ##worse\n",
    "                      , \"gen_m\" : vocab_size\n",
    "                      , \"gen_n\" : 500\n",
    "                      , \"gen_epochs\" : 100\n",
    "                      , \"gen_lr\" : 0.003\n",
    "                      , \"random_shuffle\" : 0.1\n",
    "                      , \"out_type\" : \"one-hot\" }\n",
    "\n",
    "config_args_high_shuffle = {\"dist_type\" : \"ints\" ##one bar..\n",
    "                      , \"gen_m\" : vocab_size\n",
    "                      , \"gen_n\" : 10_000\n",
    "                      , \"gen_epochs\" : 10\n",
    "                      , \"gen_lr\" : 0.05\n",
    "                      , \"random_shuffle\" : 0.9\n",
    "                      , \"out_type\" : \"one-hot\" }\n",
    "\n",
    "config_args_small_batch = {\"dist_type\" : \"ints\" ##this one was the first to do well.  not just one bar and the rest nearly zero.\n",
    "                      , \"gen_m\" : vocab_size\n",
    "                      , \"gen_n\" : 10_000\n",
    "                      , \"gen_epochs\" : 10\n",
    "                      , \"gen_lr\" : 0.05\n",
    "                      , \"random_shuffle\" : 0.5\n",
    "                      , \"batch_size\" : 10\n",
    "                      , \"out_type\" : \"one-hot\" }\n",
    "\n",
    "config_ab = {\"dist_type\" : \"ints\" ##worked well\n",
    "                      , \"gen_m\" : vocab_size\n",
    "                      , \"gen_n\" : 500\n",
    "                      , \"gen_epochs\" : 5\n",
    "                      , \"gen_lr\" : 0.005\n",
    "                      , \"random_shuffle\" : 0.0\n",
    "                      , \"out_type\" : \"one-hot\" \n",
    "                      , \"dist_type\" : 'hetero'\n",
    "                      , \"alpha\" : 4\n",
    "                      , \"beta\" : 6} #maybe increase epochs?\n",
    "config_debug = {\"dist_type\" : \"ints\" ##worked well\n",
    "                      , \"gen_m\" : vocab_size\n",
    "                      , \"gen_n\" : 5\n",
    "                      , \"gen_epochs\" : 2\n",
    "                      , \"gen_lr\" : 0.005\n",
    "                      , \"random_shuffle\" : 0.0\n",
    "                      , \"out_type\" : \"one-hot\" \n",
    "                      , \"dist_type\" : 'hetero'\n",
    "                      , \"alpha\" : 1\n",
    "                      , \"beta\" : 4}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "d700f138",
   "metadata": {},
   "outputs": [],
   "source": [
    "#config should be on device, and, so should generate.  deal with train on your own for now.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "0fb0ccfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expected tensor for argument #1 'indices' to have one of the following scalar types: Long, Int; but got torch.FloatTensor instead (while checking arguments for embedding)\n",
      "lets try ints!\n"
     ]
    }
   ],
   "source": [
    "#teacher_toy.configure(**config_ab) #this is dying.  might be time for colab!!\n",
    "teacher_toy.load_state_dict('good_toy.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "77f954d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Generating val data ::   0%|                             | 0/20 [00:00<?, ?it/s]\u001b[A\n",
      "Generating val data ::  15%|███▏                 | 3/20 [00:00<00:00, 28.89it/s]\u001b[A\n",
      "Generating val data ::  30%|██████▎              | 6/20 [00:00<00:00, 28.64it/s]\u001b[A\n",
      "Generating val data ::  45%|█████████▍           | 9/20 [00:00<00:00, 29.22it/s]\u001b[A\n",
      "Generating val data ::  65%|█████████████       | 13/20 [00:00<00:00, 29.71it/s]\u001b[A\n",
      "Generating val data :: 100%|████████████████████| 20/20 [00:00<00:00, 30.06it/s]\u001b[A\n"
     ]
    }
   ],
   "source": [
    "\n",
    "args = { 'val_train' : \"val\"\n",
    "                      , 'n' : 1000\n",
    "                      , 'dist_type' : 'ints'\n",
    "                      , 'm' : vocab_size\n",
    "                      , 'std': 1.0\n",
    "                      , 'batch_size' : 50\n",
    "                      , 'store_outputs': True\n",
    "        }\n",
    "teacher_toy.generate_data(**args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "f3ca1f01",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.2303008 , -1.1400923 , -2.3738654 , ...,  1.1755785 ,\n",
       "         0.05687125, -1.4886448 ],\n",
       "       [-0.01727519,  0.68126124,  1.9395509 , ..., -0.11208937,\n",
       "        -1.2799459 ,  0.05062845],\n",
       "       [-1.6923524 , -0.71483594,  0.7488729 , ...,  0.14639653,\n",
       "         2.0501509 , -0.73531675],\n",
       "       ...,\n",
       "       [ 0.34590563,  1.0685613 ,  1.5206655 , ...,  0.572574  ,\n",
       "        -0.24566738,  0.67347604],\n",
       "       [ 0.29129675,  0.5064824 ,  0.6569628 , ..., -0.06007009,\n",
       "         1.4017551 ,  2.1171217 ],\n",
       "       [-0.5423141 , -0.48402527,  0.70683235, ..., -0.38188922,\n",
       "        -1.2272888 , -2.1976929 ]], dtype=float32)"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "teacher_toy.model.embedding.weight.detach().numpy() #use this to make the matrix, just linear? not sure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "513eec1f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 2880x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "teacher_toy.graph_dataset_dist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "871af3f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "##file_path = 'good_toy_5.pth'\n",
    "\n",
    "# Save the state_dict to the specified file\n",
    "torch.save(teacher_toy.model.state_dict(), file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "61a33f74",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1000, 160]), torch.Size([1000, 80]))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "teacher_toy.val_inputs.shape,teacher_toy.val_targets.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9b7d79bc",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'teacher_slm' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [13]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mteacher_slm\u001b[49m\u001b[38;5;241m.\u001b[39mval_inputs\u001b[38;5;241m.\u001b[39mshape, teacher_slm\u001b[38;5;241m.\u001b[39mval_targets\u001b[38;5;241m.\u001b[39mshape\n",
      "\u001b[0;31mNameError\u001b[0m: name 'teacher_slm' is not defined"
     ]
    }
   ],
   "source": [
    "teacher_slm.val_inputs.shape, teacher_slm.val_targets.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9bc7ec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.all(teacher_slm.model(teacher_slm.val_inputs[1]) ==  teacher_slm.val_targets[1]) ##torch.allclose doesn't work either"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5916bb4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "##this function works, it is in LABNET.  \n",
    "def one_hot_last_dim(tensor_shape):\n",
    "    num_classes = tensor_shape[-1]\n",
    "    random_idx = np.random.randint(1, num_classes + 1, size=tensor_shape[:-1])\n",
    "    zero_tensor = np.zeros(tensor_shape, dtype=int)\n",
    "    last_dim_indices = np.arange(num_classes)\n",
    "    zero_tensor[..., :, last_dim_indices] = (random_idx[..., np.newaxis] == last_dim_indices)\n",
    "    return zero_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e221c7c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "import math\n",
    "\n",
    "warmup_steps = 60_000  # Adjust this as needed steps is total items passed through\n",
    "total_steps = 200_000  # Adjust this as needed i'd like to calculate this....\n",
    "\n",
    "\n",
    "# Define a learning rate scheduler with warmup\n",
    "def lr_lambda(current_step):\n",
    "    if current_step < warmup_steps:\n",
    "        # During warmup, increase learning rate linearly\n",
    "        return float(current_step) / float(max(1, warmup_steps))\n",
    "    else:\n",
    "        # After warmup, decrease learning rate using some schedule\n",
    "        # You can use any LR schedule you prefer here\n",
    "        # For example, you can use a learning rate schedule like CosineAnnealing\n",
    "        return 0.5 * (1 + math.cos(math.pi * (current_step - warmup_steps) / (total_steps - warmup_steps)))\n",
    "\n",
    "\n",
    "\n",
    "\"\"\" Training loop\n",
    "for step in range(total_steps):\n",
    "    optimizer.zero_grad()\n",
    "    # Compute your loss and backpropagation here\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    scheduler.step()\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "b44d8243",
   "metadata": {},
   "outputs": [],
   "source": [
    "student = ToyTransformer(vocab_size, embedding_dim, num_heads, hidden_dim, num_layers, dropout,sequence_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "951ad666",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ToyTransformer(\n",
       "  (embedding): Embedding(80, 16)\n",
       "  (transformer_encoder): TransformerEncoder(\n",
       "    (layers): ModuleList(\n",
       "      (0): TransformerEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=16, out_features=16, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=16, out_features=11, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (linear2): Linear(in_features=11, out_features=16, bias=True)\n",
       "        (norm1): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (1): TransformerEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=16, out_features=16, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=16, out_features=11, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (linear2): Linear(in_features=11, out_features=16, bias=True)\n",
       "        (norm1): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (fc1): Linear(in_features=2560, out_features=80, bias=True)\n",
       "  (fc2): Linear(in_features=80, out_features=80, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "student"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19e56b2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#############non-repeating train\n",
    "learning_rate = 0.001 #0.00025 this shit is slooooow\n",
    "momentum = 0.95\n",
    "val_batch_size = 10\n",
    "\n",
    "\n",
    "num_epochs = 50\n",
    "batches_per_epoch = 100\n",
    "batch_size = 1000\n",
    "data_per_batch = batch_size\n",
    "\n",
    "criterion =  nn.MSELoss() #nn.KLDivLoss() #nn.CrossEntropyLoss()  #i think stick to mse for now.  this probs just needs lots of time to start learning.  like s4 lol.\n",
    "optimizer = optim.Adam(student.parameters(), lr=learning_rate)\n",
    "#scheduler = LambdaLR(optimizer, lr_lambda=lr_lambda)\n",
    "#optimizer = optim.SGD(student.parameters(), lr=learning_rate, momentum=momentum)\n",
    "#no train data loader here.\n",
    "val_data = list(zip(teacher_toy.val_inputs, teacher_toy.val_targets))\n",
    "val_input_tensors = torch.stack([torch.Tensor(x[0]) for x in val_data])\n",
    "val_target_tensors = torch.stack([torch.Tensor(x[1]) for x in val_data])\n",
    "val_dataset = TensorDataset(val_input_tensors, val_target_tensors)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=val_batch_size, shuffle=True)\n",
    "accuracy_threshold = 0.5\n",
    "print_every = 1 #its working, its just lots of data mama.\n",
    "validation_every = 1\n",
    "\n",
    "\n",
    "gen_args  = { 'val_train' : \"train\"\n",
    "              , 'n' : data_per_batch\n",
    "              , 'dist_type' : 'ints' #this is generating ints for inputs, the outputs are logits.  hmmmm\n",
    "              , 'm' : vocab_size\n",
    "              , 'std': 1.0\n",
    "              , 'display_progress' : False\n",
    "              , 'store_outputs' : True\n",
    "        }\n",
    "\n",
    "losses = []  # List to store losses\n",
    "accuracies = []\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "student = student.to(device)\n",
    "for epoch in range(num_epochs):\n",
    "    student.train()\n",
    "    total_loss = 0\n",
    "    correct_predictions = 0\n",
    "    total_samples = 0\n",
    "\n",
    "    for i in range(batches_per_epoch):\n",
    "        optimizer.zero_grad()  # Zero the gradients\n",
    "\n",
    "        teacher_toy.generate_data(**gen_args)\n",
    "        data_e = list(zip(teacher_toy.train_inputs, teacher_toy.train_targets))\n",
    "        input_tensors_e = torch.stack([torch.Tensor(x[0]) for x in data_e])\n",
    "        target_tensors_e = torch.stack([torch.Tensor(x[1]) for x in data_e])\n",
    "        dataset_e = TensorDataset(input_tensors_e, target_tensors_e)\n",
    "        dataloader_e = DataLoader(dataset_e, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "        for input_batch_e, target_batch_e in dataloader_e:\n",
    "            optimizer.zero_grad()  # Zero the gradients\n",
    "            input_batch_e = input_batch_e.to(device)\n",
    "            target_batch_e = target_batch_e.to(device)\n",
    "            output = student(input_batch_e)  # Forward pass\n",
    "            loss = criterion(output, target_batch_e)  # Compute the loss\n",
    "            loss.backward()  # Backpropagation\n",
    "            optimizer.step()  # Update the weights\n",
    "\n",
    "            #scheduler.step() ##this does the warmup step.  \n",
    "\n",
    "            total_loss += loss.item()\n",
    "            #add early stopping...\n",
    "            #and validation at each step.\n",
    "            # Calculate accuracy\n",
    "            predictions = (output > accuracy_threshold).float()  # Assuming a threshold of 0.5 for binary classification\n",
    "            correct_predictions += (predictions == target_batch_e).sum().item()\n",
    "            total_samples += input_batch_e.size(0)\n",
    "\n",
    "    # Print the average loss for this epoch\n",
    "    avg_loss = total_loss / ( batches_per_epoch * data_per_batch)\n",
    "    losses.append(avg_loss)\n",
    "\n",
    "    accuracy = correct_predictions / total_samples\n",
    "    accuracies.append(accuracy)\n",
    "\n",
    "    if (epoch + 1) % print_every == 0:\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {total_loss}, Train Accuracy: {accuracy:.4f}')\n",
    "        current_lr = optimizer.param_groups[0]['lr']\n",
    "        print(current_lr)\n",
    "    \n",
    "    if (epoch + 1) % validation_every == 0:\n",
    "        student.eval()\n",
    "\n",
    "        total_val_samples = 0\n",
    "        correct_val_predictions = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for val_input_batch, val_target_batch in val_dataloader:\n",
    "                val_input_batch = val_input_batch.to(device)\n",
    "                val_target_batch = val_target_batch.to(device)\n",
    "                val_output = student(val_input_batch)\n",
    "                val_predictions = (val_output > accuracy_threshold).float()  # Assuming a threshold of 0.5 for binary classification\n",
    "                correct_val_predictions += (val_predictions == val_target_batch).sum().item()\n",
    "                total_val_samples += val_input_batch.size(0)\n",
    "\n",
    "        # Calculate validation accuracy\n",
    "        val_accuracy = correct_val_predictions / total_val_samples\n",
    "\n",
    "        # Print the validation accuracy for this epoch\n",
    "        print(f'\\t\\tValidation Accuracy: {val_accuracy:.4f}')\n",
    "\n",
    "        # Set the model back to training mode\n",
    "        student.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "4a4c7d9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class ToyTransformer(nn.Module):\n",
      "    def __init__(self, vocab_size, embedding_dim, num_heads, hidden_dim, num_layers, dropout,sequence_length):\n",
      "        super(ToyTransformer, self).__init__()\n",
      "\n",
      "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
      "        self.transformer_encoder = nn.TransformerEncoder(\n",
      "            nn.TransformerEncoderLayer(embedding_dim, num_heads, hidden_dim, dropout, batch_first=True),\n",
      "            num_layers\n",
      "        )\n",
      "        self.fc1 = nn.Linear(embedding_dim * sequence_length, vocab_size)  # Intermediate linear layer\n",
      "        self.fc2 = nn.Linear(vocab_size, vocab_size)  # Final linear layer\n",
      "\n",
      "    def forward(self, x):\n",
      "        x = self.embedding(x)\n",
      "        x = self.transformer_encoder(x)\n",
      "        x = torch.flatten(x, start_dim=1)  # Flatten the output\n",
      "        x = F.relu(self.fc1(x))  # Apply the intermediate linear layer with ReLU activation\n",
      "        x = self.fc2(x)  # Apply the final linear layer\n",
      "        x = F.softmax(x, dim=1)  # Apply softmax activation\n",
      "        return x\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import inspect #just fell in love with this baby\n",
    "forward_source = inspect.getsource(ToyTransformer)\n",
    "print(forward_source)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6764fdbe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
