{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "6a04cb7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "99b445af",
   "metadata": {},
   "outputs": [],
   "source": [
    "from NoiseKD import Teacher,ToyTransformer, count_parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5a11b24",
   "metadata": {},
   "source": [
    "First hurdle. the embedding layer.  It requires the ints as inputs, not one hot.  i need one hot.  i will also need to rewire the LLM i use to do the same.  hmm.  save the weights, load it into a model that has the same shapes, but accepts one hot arrays, not vocab_indexes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cca2a6b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 16\n",
    "num_heads = 8\n",
    "hidden_dim  = 11\n",
    "num_layers = 2\n",
    "dropout = 0.1\n",
    "vocab_size = 80\n",
    "class_num = vocab_size\n",
    "batch_size = 50\n",
    "sequence_length = 160"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a7b5a7d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "TT = ToyTransformer(vocab_size, embedding_dim, num_heads, hidden_dim, num_layers, dropout,sequence_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a3ba38e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "215702"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_parameters(TT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c613c4cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "teacher_toy = Teacher(TT,(sequence_length,)) #don't specify batch!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5e4db9fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "##some of these configs made for more diverse outputs in teachers:\n",
    "config_args = {\"dist_type\" : \"ints\" ##worked well\n",
    "                      , \"gen_m\" : vocab_size\n",
    "                      , \"gen_n\" : 100\n",
    "                      , \"gen_epochs\" : 100\n",
    "                      , \"gen_lr\" : 0.001\n",
    "                      , \"random_shuffle\" : 0.1\n",
    "                      , \"out_type\" : \"one-hot\" }\n",
    "\n",
    "config_args_high_epochs = {\"dist_type\" : \"ints\" ##okay, but not as well as config_args.\n",
    "                      , \"gen_m\" : vocab_size\n",
    "                      , \"gen_n\" : 1000\n",
    "                      , \"gen_epochs\" : 100\n",
    "                      , \"gen_lr\" : 0.001\n",
    "                      , \"random_shuffle\" : 0.1\n",
    "                      , \"out_type\" : \"one-hot\" }\n",
    "\n",
    "config_args_higher_lr = {\"dist_type\" : \"ints\" ##lower was worse.  raise it. 0.003 looks great.  this is the best.\n",
    "                      , \"gen_m\" : vocab_size\n",
    "                      , \"gen_n\" : 2000\n",
    "                      , \"gen_epochs\" : 50\n",
    "                      , \"gen_lr\" :  0.003 ##0.003\n",
    "                      , \"random_shuffle\" : 0.8\n",
    "                      , \"out_type\" : \"one-hot\" }\n",
    "\n",
    "config_args_less_data = {\"dist_type\" : \"ints\" ##worse\n",
    "                      , \"gen_m\" : vocab_size\n",
    "                      , \"gen_n\" : 500\n",
    "                      , \"gen_epochs\" : 100\n",
    "                      , \"gen_lr\" : 0.003\n",
    "                      , \"random_shuffle\" : 0.1\n",
    "                      , \"out_type\" : \"one-hot\" }\n",
    "\n",
    "config_args_high_shuffle = {\"dist_type\" : \"ints\" ##one bar..\n",
    "                      , \"gen_m\" : vocab_size\n",
    "                      , \"gen_n\" : 10_000\n",
    "                      , \"gen_epochs\" : 10\n",
    "                      , \"gen_lr\" : 0.05\n",
    "                      , \"random_shuffle\" : 0.9\n",
    "                      , \"out_type\" : \"one-hot\" }\n",
    "\n",
    "config_args_small_batch = {\"dist_type\" : \"ints\" ##this one was the first to do well.  not just one bar and the rest nearly zero.\n",
    "                      , \"gen_m\" : vocab_size\n",
    "                      , \"gen_n\" : 10_000\n",
    "                      , \"gen_epochs\" : 10\n",
    "                      , \"gen_lr\" : 0.05\n",
    "                      , \"random_shuffle\" : 0.5\n",
    "                      , \"batch_size\" : 10\n",
    "                      , \"out_type\" : \"one-hot\" }\n",
    "\n",
    "config_ab = {\"dist_type\" : \"ints\" ##worked well\n",
    "                      , \"gen_m\" : vocab_size\n",
    "                      , \"gen_n\" : 5000\n",
    "                      , \"gen_epochs\" : 200\n",
    "                      , \"gen_lr\" : 0.005\n",
    "                      , \"random_shuffle\" : 0.0\n",
    "                      , \"out_type\" : \"one-hot\" \n",
    "                      , \"dist_type\" : 'hetero'\n",
    "                      , \"alpha\" : 1\n",
    "                      , \"beta\" : 4} #maybe increase epochs?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "0fb0ccfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expected tensor for argument #1 'indices' to have one of the following scalar types: Long, Int; but got torch.FloatTensor instead (while checking arguments for embedding)\n",
      "lets try ints!\n"
     ]
    }
   ],
   "source": [
    "#teacher_toy.configure(**config_ab) #this is dying.  might be time for colab!!\n",
    "teacher_toy.load_state_dict('good_toy.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "77f954d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Generating val data ::   0%|                             | 0/20 [00:00<?, ?it/s]\u001b[A\n"
     ]
    }
   ],
   "source": [
    "\n",
    "args = { 'val_train' : \"val\"\n",
    "                      , 'n' : 1000\n",
    "                      , 'dist_type' : 'ints'\n",
    "                      , 'm' : vocab_size\n",
    "                      , 'std': 1.0\n",
    "                      , 'batch_size' : 50\n",
    "                      , 'store_outputs': True\n",
    "        }\n",
    "teacher_toy.generate_data(**args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "f3ca1f01",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.13697533,  1.0160749 ,  0.42515135, ...,  2.1265104 ,\n",
       "         0.01124284, -1.2953788 ],\n",
       "       [ 0.59546024, -0.99750274,  1.1170036 , ..., -1.6646637 ,\n",
       "         0.45393214, -0.55875623],\n",
       "       [ 0.6230322 ,  0.17974639,  0.8337519 , ...,  0.78358537,\n",
       "        -0.32227072, -0.48803455],\n",
       "       ...,\n",
       "       [ 0.06030137, -0.20708236,  1.5042213 , ..., -1.0101138 ,\n",
       "        -0.70308393, -0.0137438 ],\n",
       "       [ 0.17904644, -0.83012843,  0.9453919 , ...,  0.38582733,\n",
       "         0.7353223 , -0.42883456],\n",
       "       [-2.56302   ,  0.89828813, -0.46527362, ...,  0.16203569,\n",
       "        -0.34777856,  0.22505838]], dtype=float32)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "teacher_toy.model.embedding.weight.detach().numpy() #use this to make the matrix, just linear? not sure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "513eec1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Configuring Teacher::   4%|â–Š                  | 8/200 [03:23<1:21:18, 25.41s/it]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 2880x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "teacher_toy.graph_dataset_dist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "61a33f74",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1000, 160]), torch.Size([1000, 80]))"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "teacher_toy.val_inputs.shape,teacher_toy.val_targets.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b7d79bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "teacher_slm.val_inputs.shape, teacher_slm.val_targets.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9bc7ec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.all(teacher_slm.model(teacher_slm.val_inputs[1]) ==  teacher_slm.val_targets[1]) ##torch.allclose doesn't work either"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "5916bb4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "##this function works, it is in LABNET.  \n",
    "def one_hot_last_dim(tensor_shape):\n",
    "    num_classes = tensor_shape[-1]\n",
    "    random_idx = np.random.randint(1, num_classes + 1, size=tensor_shape[:-1])\n",
    "    zero_tensor = np.zeros(tensor_shape, dtype=int)\n",
    "    last_dim_indices = np.arange(num_classes)\n",
    "    zero_tensor[..., :, last_dim_indices] = (random_idx[..., np.newaxis] == last_dim_indices)\n",
    "    return zero_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "e221c7c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' Training loop\\nfor step in range(total_steps):\\n    optimizer.zero_grad()\\n    # Compute your loss and backpropagation here\\n    loss.backward()\\n    optimizer.step()\\n    scheduler.step()\\n'"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "import math\n",
    "\n",
    "warmup_steps = 60_000  # Adjust this as needed steps is total items passed through\n",
    "total_steps = 200_000  # Adjust this as needed i'd like to calculate this....\n",
    "\n",
    "\n",
    "# Define a learning rate scheduler with warmup\n",
    "def lr_lambda(current_step):\n",
    "    if current_step < warmup_steps:\n",
    "        # During warmup, increase learning rate linearly\n",
    "        return float(current_step) / float(max(1, warmup_steps))\n",
    "    else:\n",
    "        # After warmup, decrease learning rate using some schedule\n",
    "        # You can use any LR schedule you prefer here\n",
    "        # For example, you can use a learning rate schedule like CosineAnnealing\n",
    "        return 0.5 * (1 + math.cos(math.pi * (current_step - warmup_steps) / (total_steps - warmup_steps)))\n",
    "\n",
    "\n",
    "\n",
    "\"\"\" Training loop\n",
    "for step in range(total_steps):\n",
    "    optimizer.zero_grad()\n",
    "    # Compute your loss and backpropagation here\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    scheduler.step()\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "b44d8243",
   "metadata": {},
   "outputs": [],
   "source": [
    "student = ToyTransformer(vocab_size, embedding_dim, num_heads, hidden_dim, num_layers, dropout,sequence_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "19e56b2b",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [62]\u001b[0m, in \u001b[0;36m<cell line: 45>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     65\u001b[0m output \u001b[38;5;241m=\u001b[39m student(input_batch_e)  \u001b[38;5;66;03m# Forward pass\u001b[39;00m\n\u001b[1;32m     66\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(output, target_batch_e)  \u001b[38;5;66;03m# Compute the loss\u001b[39;00m\n\u001b[0;32m---> 67\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Backpropagation\u001b[39;00m\n\u001b[1;32m     68\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()  \u001b[38;5;66;03m# Update the weights\u001b[39;00m\n\u001b[1;32m     70\u001b[0m \u001b[38;5;66;03m#scheduler.step() ##this does the warmup step.  \u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    477\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    478\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    479\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    480\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    485\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    486\u001b[0m     )\n\u001b[0;32m--> 487\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    488\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    489\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/autograd/__init__.py:197\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    192\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    194\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    195\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    196\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 197\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    198\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    199\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#############non-repeating train\n",
    "learning_rate = 0.001 #0.00025 this shit is slooooow\n",
    "momentum = 0.95\n",
    "val_batch_size = 10\n",
    "\n",
    "\n",
    "num_epochs = 50\n",
    "batches_per_epoch = 100\n",
    "batch_size = 1000\n",
    "data_per_batch = batch_size\n",
    "\n",
    "criterion =  nn.MSELoss() #nn.KLDivLoss() #nn.CrossEntropyLoss()  #i think stick to mse for now.  this probs just needs lots of time to start learning.  like s4 lol.\n",
    "optimizer = optim.Adam(student.parameters(), lr=learning_rate)\n",
    "#scheduler = LambdaLR(optimizer, lr_lambda=lr_lambda)\n",
    "#optimizer = optim.SGD(student.parameters(), lr=learning_rate, momentum=momentum)\n",
    "#no train data loader here.\n",
    "val_data = list(zip(teacher_toy.val_inputs, teacher_toy.val_targets))\n",
    "val_input_tensors = torch.stack([torch.Tensor(x[0]) for x in val_data])\n",
    "val_target_tensors = torch.stack([torch.Tensor(x[1]) for x in val_data])\n",
    "val_dataset = TensorDataset(val_input_tensors, val_target_tensors)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=val_batch_size, shuffle=True)\n",
    "accuracy_threshold = 0.5\n",
    "print_every = 1 #its working, its just lots of data mama.\n",
    "validation_every = 1\n",
    "\n",
    "\n",
    "gen_args  = { 'val_train' : \"train\"\n",
    "              , 'n' : data_per_batch\n",
    "              , 'dist_type' : 'ints' #this is generating ints for inputs, the outputs are logits.  hmmmm\n",
    "              , 'm' : vocab_size\n",
    "              , 'std': 1.0\n",
    "              , 'display_progress' : False\n",
    "              , 'store_outputs' : True\n",
    "        }\n",
    "\n",
    "losses = []  # List to store losses\n",
    "accuracies = []\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "student = student.to(device)\n",
    "for epoch in range(num_epochs):\n",
    "    student.train()\n",
    "    total_loss = 0\n",
    "    correct_predictions = 0\n",
    "    total_samples = 0\n",
    "\n",
    "    for i in range(batches_per_epoch):\n",
    "        optimizer.zero_grad()  # Zero the gradients\n",
    "\n",
    "        teacher_toy.generate_data(**gen_args)\n",
    "        data_e = list(zip(teacher_toy.train_inputs, teacher_toy.train_targets))\n",
    "        input_tensors_e = torch.stack([torch.Tensor(x[0]) for x in data_e])\n",
    "        target_tensors_e = torch.stack([torch.Tensor(x[1]) for x in data_e])\n",
    "        dataset_e = TensorDataset(input_tensors_e, target_tensors_e)\n",
    "        dataloader_e = DataLoader(dataset_e, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "        for input_batch_e, target_batch_e in dataloader_e:\n",
    "            optimizer.zero_grad()  # Zero the gradients\n",
    "            input_batch_e = input_batch_e.to(device)\n",
    "            target_batch_e = target_batch_e.to(device)\n",
    "            output = student(input_batch_e)  # Forward pass\n",
    "            loss = criterion(output, target_batch_e)  # Compute the loss\n",
    "            loss.backward()  # Backpropagation\n",
    "            optimizer.step()  # Update the weights\n",
    "\n",
    "            #scheduler.step() ##this does the warmup step.  \n",
    "\n",
    "            total_loss += loss.item()\n",
    "            #add early stopping...\n",
    "            #and validation at each step.\n",
    "            # Calculate accuracy\n",
    "            predictions = (output > accuracy_threshold).float()  # Assuming a threshold of 0.5 for binary classification\n",
    "            correct_predictions += (predictions == target_batch_e).sum().item()\n",
    "            total_samples += input_batch_e.size(0)\n",
    "\n",
    "    # Print the average loss for this epoch\n",
    "    avg_loss = total_loss / ( batches_per_epoch * data_per_batch)\n",
    "    losses.append(avg_loss)\n",
    "\n",
    "    accuracy = correct_predictions / total_samples\n",
    "    accuracies.append(accuracy)\n",
    "\n",
    "    if (epoch + 1) % print_every == 0:\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {total_loss}, Train Accuracy: {accuracy:.4f}')\n",
    "        current_lr = optimizer.param_groups[0]['lr']\n",
    "        print(current_lr)\n",
    "    \n",
    "    if (epoch + 1) % validation_every == 0:\n",
    "        student.eval()\n",
    "\n",
    "        total_val_samples = 0\n",
    "        correct_val_predictions = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for val_input_batch, val_target_batch in val_dataloader:\n",
    "                val_input_batch = val_input_batch.to(device)\n",
    "                val_target_batch = val_target_batch.to(device)\n",
    "                val_output = student(val_input_batch)\n",
    "                val_predictions = (val_output > accuracy_threshold).float()  # Assuming a threshold of 0.5 for binary classification\n",
    "                correct_val_predictions += (val_predictions == val_target_batch).sum().item()\n",
    "                total_val_samples += val_input_batch.size(0)\n",
    "\n",
    "        # Calculate validation accuracy\n",
    "        val_accuracy = correct_val_predictions / total_val_samples\n",
    "\n",
    "        # Print the validation accuracy for this epoch\n",
    "        print(f'\\t\\tValidation Accuracy: {val_accuracy:.4f}')\n",
    "\n",
    "        # Set the model back to training mode\n",
    "        student.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a4c7d9d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
