{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6a04cb7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "99b445af",
   "metadata": {},
   "outputs": [],
   "source": [
    "from NoiseKD import Teacher,ToyTransformer, count_parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5a11b24",
   "metadata": {},
   "source": [
    "First hurdle. the embedding layer.  It requires the ints as inputs, not one hot.  i need one hot.  i will also need to rewire the LLM i use to do the same.  hmm.  save the weights, load it into a model that has the same shapes, but accepts one hot arrays, not vocab_indexes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cca2a6b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 16\n",
    "num_heads = 8\n",
    "hidden_dim  = 11\n",
    "num_layers = 2\n",
    "dropout = 0.1\n",
    "vocab_size = 80\n",
    "class_num = vocab_size\n",
    "batch_size = 50\n",
    "sequence_length = 160"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a7b5a7d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "TT = ToyTransformer(vocab_size, embedding_dim, num_heads, hidden_dim, num_layers, dropout,sequence_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a3ba38e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "215702"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_parameters(TT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c613c4cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "teacher_toy = Teacher(TT,(sequence_length,)) #don't specify batch!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5e4db9fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "##some of these configs made for more diverse outputs in teachers:\n",
    "config_args = {\"dist_type\" : \"ints\" ##worked well\n",
    "                      , \"gen_m\" : vocab_size\n",
    "                      , \"gen_n\" : 100\n",
    "                      , \"gen_epochs\" : 100\n",
    "                      , \"gen_lr\" : 0.001\n",
    "                      , \"random_shuffle\" : 0.1\n",
    "                      , \"out_type\" : \"one-hot\" }\n",
    "\n",
    "config_args_high_epochs = {\"dist_type\" : \"ints\" ##okay, but not as well as config_args.\n",
    "                      , \"gen_m\" : vocab_size\n",
    "                      , \"gen_n\" : 1000\n",
    "                      , \"gen_epochs\" : 100\n",
    "                      , \"gen_lr\" : 0.001\n",
    "                      , \"random_shuffle\" : 0.1\n",
    "                      , \"out_type\" : \"one-hot\" }\n",
    "\n",
    "config_args_higher_lr = {\"dist_type\" : \"ints\" ##lower was worse.  raise it. 0.003 looks great.  this is the best.\n",
    "                      , \"gen_m\" : vocab_size\n",
    "                      , \"gen_n\" : 2000\n",
    "                      , \"gen_epochs\" : 50\n",
    "                      , \"gen_lr\" :  0.003 ##0.003\n",
    "                      , \"random_shuffle\" : 0.8\n",
    "                      , \"out_type\" : \"one-hot\" }\n",
    "\n",
    "config_args_less_data = {\"dist_type\" : \"ints\" ##worse\n",
    "                      , \"gen_m\" : vocab_size\n",
    "                      , \"gen_n\" : 500\n",
    "                      , \"gen_epochs\" : 100\n",
    "                      , \"gen_lr\" : 0.003\n",
    "                      , \"random_shuffle\" : 0.1\n",
    "                      , \"out_type\" : \"one-hot\" }\n",
    "\n",
    "config_args_high_shuffle = {\"dist_type\" : \"ints\" ##one bar..\n",
    "                      , \"gen_m\" : vocab_size\n",
    "                      , \"gen_n\" : 10_000\n",
    "                      , \"gen_epochs\" : 10\n",
    "                      , \"gen_lr\" : 0.05\n",
    "                      , \"random_shuffle\" : 0.9\n",
    "                      , \"out_type\" : \"one-hot\" }\n",
    "\n",
    "config_args_small_batch = {\"dist_type\" : \"ints\" ##this one was the first to do well.  not just one bar and the rest nearly zero.\n",
    "                      , \"gen_m\" : vocab_size\n",
    "                      , \"gen_n\" : 10_000\n",
    "                      , \"gen_epochs\" : 10\n",
    "                      , \"gen_lr\" : 0.05\n",
    "                      , \"random_shuffle\" : 0.5\n",
    "                      , \"batch_size\" : 10\n",
    "                      , \"out_type\" : \"one-hot\" }\n",
    "\n",
    "config_ab = {\"dist_type\" : \"ints\" ##worked well\n",
    "                      , \"gen_m\" : vocab_size\n",
    "                      , \"gen_n\" : 5000\n",
    "                      , \"gen_epochs\" : 200\n",
    "                      , \"gen_lr\" : 0.005\n",
    "                      , \"random_shuffle\" : 0.0\n",
    "                      , \"out_type\" : \"one-hot\" \n",
    "                      , \"dist_type\" : 'hetero'\n",
    "                      , \"alpha\" : 1\n",
    "                      , \"beta\" : 4} #maybe increase epochs?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0fb0ccfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expected tensor for argument #1 'indices' to have one of the following scalar types: Long, Int; but got torch.FloatTensor instead (while checking arguments for embedding)\n",
      "lets try ints!\n"
     ]
    }
   ],
   "source": [
    "#teacher_toy.configure(**config_ab) #this is dying.  might be time for colab!!\n",
    "teacher_toy.load_state_dict('good_toy.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "77f954d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating val data :: 100%|████████████████████| 20/20 [00:00<00:00, 30.32it/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "args = { 'val_train' : \"val\"\n",
    "                      , 'n' : 1000\n",
    "                      , 'dist_type' : 'ints'\n",
    "                      , 'm' : vocab_size\n",
    "                      , 'std': 1.0\n",
    "                      , 'batch_size' : 50\n",
    "                      , 'store_outputs': True\n",
    "        }\n",
    "teacher_toy.generate_data(**args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f3ca1f01",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.13697533,  1.0160749 ,  0.42515135, ...,  2.1265104 ,\n",
       "         0.01124284, -1.2953788 ],\n",
       "       [ 0.59546024, -0.99750274,  1.1170036 , ..., -1.6646637 ,\n",
       "         0.45393214, -0.55875623],\n",
       "       [ 0.6230322 ,  0.17974639,  0.8337519 , ...,  0.78358537,\n",
       "        -0.32227072, -0.48803455],\n",
       "       ...,\n",
       "       [ 0.06030137, -0.20708236,  1.5042213 , ..., -1.0101138 ,\n",
       "        -0.70308393, -0.0137438 ],\n",
       "       [ 0.17904644, -0.83012843,  0.9453919 , ...,  0.38582733,\n",
       "         0.7353223 , -0.42883456],\n",
       "       [-2.56302   ,  0.89828813, -0.46527362, ...,  0.16203569,\n",
       "        -0.34777856,  0.22505838]], dtype=float32)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "teacher_toy.model.embedding.weight.detach().numpy() #use this to make the matrix, just linear? not sure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "513eec1f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 2880x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "teacher_toy.graph_dataset_dist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61a33f74",
   "metadata": {},
   "outputs": [],
   "source": [
    "teacher_toy.val_inputs.shape,teacher_toy.val_targets.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b7d79bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "teacher_slm.val_inputs.shape, teacher_slm.val_targets.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9bc7ec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.all(teacher_slm.model(teacher_slm.val_inputs[1]) ==  teacher_slm.val_targets[1]) ##torch.allclose doesn't work either"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5916bb4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "##this function works, it is in LABNET.  \n",
    "def one_hot_last_dim(tensor_shape):\n",
    "    num_classes = tensor_shape[-1]\n",
    "    random_idx = np.random.randint(1, num_classes + 1, size=tensor_shape[:-1])\n",
    "    zero_tensor = np.zeros(tensor_shape, dtype=int)\n",
    "    last_dim_indices = np.arange(num_classes)\n",
    "    zero_tensor[..., :, last_dim_indices] = (random_idx[..., np.newaxis] == last_dim_indices)\n",
    "    return zero_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e221c7c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' Training loop\\nfor step in range(total_steps):\\n    optimizer.zero_grad()\\n    # Compute your loss and backpropagation here\\n    loss.backward()\\n    optimizer.step()\\n    scheduler.step()\\n'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "import math\n",
    "\n",
    "warmup_steps = 60_000  # Adjust this as needed steps is total items passed through\n",
    "total_steps = 200_000  # Adjust this as needed i'd like to calculate this....\n",
    "\n",
    "\n",
    "# Define a learning rate scheduler with warmup\n",
    "def lr_lambda(current_step):\n",
    "    if current_step < warmup_steps:\n",
    "        # During warmup, increase learning rate linearly\n",
    "        return float(current_step) / float(max(1, warmup_steps))\n",
    "    else:\n",
    "        # After warmup, decrease learning rate using some schedule\n",
    "        # You can use any LR schedule you prefer here\n",
    "        # For example, you can use a learning rate schedule like CosineAnnealing\n",
    "        return 0.5 * (1 + math.cos(math.pi * (current_step - warmup_steps) / (total_steps - warmup_steps)))\n",
    "\n",
    "\n",
    "\n",
    "\"\"\" Training loop\n",
    "for step in range(total_steps):\n",
    "    optimizer.zero_grad()\n",
    "    # Compute your loss and backpropagation here\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    scheduler.step()\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b44d8243",
   "metadata": {},
   "outputs": [],
   "source": [
    "student = ToyTransformer(vocab_size, embedding_dim, num_heads, hidden_dim, num_layers, dropout,sequence_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "19e56b2b",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [14]\u001b[0m, in \u001b[0;36m<cell line: 45>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     65\u001b[0m output \u001b[38;5;241m=\u001b[39m student(input_batch_e)  \u001b[38;5;66;03m# Forward pass\u001b[39;00m\n\u001b[1;32m     66\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(output, target_batch_e)  \u001b[38;5;66;03m# Compute the loss\u001b[39;00m\n\u001b[0;32m---> 67\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Backpropagation\u001b[39;00m\n\u001b[1;32m     68\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()  \u001b[38;5;66;03m# Update the weights\u001b[39;00m\n\u001b[1;32m     70\u001b[0m \u001b[38;5;66;03m#scheduler.step() ##this does the warmup step.  \u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    477\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    478\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    479\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    480\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    485\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    486\u001b[0m     )\n\u001b[0;32m--> 487\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    488\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    489\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/autograd/__init__.py:197\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    192\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    194\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    195\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    196\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 197\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    198\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    199\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#############non-repeating train\n",
    "learning_rate = 0.001 #0.00025 this shit is slooooow\n",
    "momentum = 0.95\n",
    "val_batch_size = 10\n",
    "\n",
    "\n",
    "num_epochs = 50\n",
    "batches_per_epoch = 100\n",
    "batch_size = 1000\n",
    "data_per_batch = batch_size\n",
    "\n",
    "criterion =  nn.MSELoss() #nn.KLDivLoss() #nn.CrossEntropyLoss()  #i think stick to mse for now.  this probs just needs lots of time to start learning.  like s4 lol.\n",
    "optimizer = optim.Adam(student.parameters(), lr=learning_rate)\n",
    "#scheduler = LambdaLR(optimizer, lr_lambda=lr_lambda)\n",
    "#optimizer = optim.SGD(student.parameters(), lr=learning_rate, momentum=momentum)\n",
    "#no train data loader here.\n",
    "val_data = list(zip(teacher_toy.val_inputs, teacher_toy.val_targets))\n",
    "val_input_tensors = torch.stack([torch.Tensor(x[0]) for x in val_data])\n",
    "val_target_tensors = torch.stack([torch.Tensor(x[1]) for x in val_data])\n",
    "val_dataset = TensorDataset(val_input_tensors, val_target_tensors)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=val_batch_size, shuffle=True)\n",
    "accuracy_threshold = 0.5\n",
    "print_every = 1 #its working, its just lots of data mama.\n",
    "validation_every = 1\n",
    "\n",
    "\n",
    "gen_args  = { 'val_train' : \"train\"\n",
    "              , 'n' : data_per_batch\n",
    "              , 'dist_type' : 'ints' #this is generating ints for inputs, the outputs are logits.  hmmmm\n",
    "              , 'm' : vocab_size\n",
    "              , 'std': 1.0\n",
    "              , 'display_progress' : False\n",
    "              , 'store_outputs' : True\n",
    "        }\n",
    "\n",
    "losses = []  # List to store losses\n",
    "accuracies = []\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "student = student.to(device)\n",
    "for epoch in range(num_epochs):\n",
    "    student.train()\n",
    "    total_loss = 0\n",
    "    correct_predictions = 0\n",
    "    total_samples = 0\n",
    "\n",
    "    for i in range(batches_per_epoch):\n",
    "        optimizer.zero_grad()  # Zero the gradients\n",
    "\n",
    "        teacher_toy.generate_data(**gen_args)\n",
    "        data_e = list(zip(teacher_toy.train_inputs, teacher_toy.train_targets))\n",
    "        input_tensors_e = torch.stack([torch.Tensor(x[0]) for x in data_e])\n",
    "        target_tensors_e = torch.stack([torch.Tensor(x[1]) for x in data_e])\n",
    "        dataset_e = TensorDataset(input_tensors_e, target_tensors_e)\n",
    "        dataloader_e = DataLoader(dataset_e, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "        for input_batch_e, target_batch_e in dataloader_e:\n",
    "            optimizer.zero_grad()  # Zero the gradients\n",
    "            input_batch_e = input_batch_e.to(device)\n",
    "            target_batch_e = target_batch_e.to(device)\n",
    "            output = student(input_batch_e)  # Forward pass\n",
    "            loss = criterion(output, target_batch_e)  # Compute the loss\n",
    "            loss.backward()  # Backpropagation\n",
    "            optimizer.step()  # Update the weights\n",
    "\n",
    "            #scheduler.step() ##this does the warmup step.  \n",
    "\n",
    "            total_loss += loss.item()\n",
    "            #add early stopping...\n",
    "            #and validation at each step.\n",
    "            # Calculate accuracy\n",
    "            predictions = (output > accuracy_threshold).float()  # Assuming a threshold of 0.5 for binary classification\n",
    "            correct_predictions += (predictions == target_batch_e).sum().item()\n",
    "            total_samples += input_batch_e.size(0)\n",
    "\n",
    "    # Print the average loss for this epoch\n",
    "    avg_loss = total_loss / ( batches_per_epoch * data_per_batch)\n",
    "    losses.append(avg_loss)\n",
    "\n",
    "    accuracy = correct_predictions / total_samples\n",
    "    accuracies.append(accuracy)\n",
    "\n",
    "    if (epoch + 1) % print_every == 0:\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {total_loss}, Train Accuracy: {accuracy:.4f}')\n",
    "        current_lr = optimizer.param_groups[0]['lr']\n",
    "        print(current_lr)\n",
    "    \n",
    "    if (epoch + 1) % validation_every == 0:\n",
    "        student.eval()\n",
    "\n",
    "        total_val_samples = 0\n",
    "        correct_val_predictions = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for val_input_batch, val_target_batch in val_dataloader:\n",
    "                val_input_batch = val_input_batch.to(device)\n",
    "                val_target_batch = val_target_batch.to(device)\n",
    "                val_output = student(val_input_batch)\n",
    "                val_predictions = (val_output > accuracy_threshold).float()  # Assuming a threshold of 0.5 for binary classification\n",
    "                correct_val_predictions += (val_predictions == val_target_batch).sum().item()\n",
    "                total_val_samples += val_input_batch.size(0)\n",
    "\n",
    "        # Calculate validation accuracy\n",
    "        val_accuracy = correct_val_predictions / total_val_samples\n",
    "\n",
    "        # Print the validation accuracy for this epoch\n",
    "        print(f'\\t\\tValidation Accuracy: {val_accuracy:.4f}')\n",
    "\n",
    "        # Set the model back to training mode\n",
    "        student.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a4c7d9d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
