{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6a04cb7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "99b445af",
   "metadata": {},
   "outputs": [],
   "source": [
    "from NoiseKD import Teacher,ToyTransformer, count_parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5a11b24",
   "metadata": {},
   "source": [
    "First hurdle. the embedding layer.  It requires the ints as inputs, not one hot.  i need one hot.  i will also need to rewire the LLM i use to do the same.  hmm.  save the weights, load it into a model that has the same shapes, but accepts one hot arrays, not vocab_indexes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cca2a6b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 16\n",
    "num_heads = 8\n",
    "hidden_dim  = 11\n",
    "num_layers = 2\n",
    "dropout = 0.1\n",
    "vocab_size = 80\n",
    "class_num = vocab_size\n",
    "batch_size = 50\n",
    "sequence_length = 160"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a7b5a7d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "TT = ToyTransformer(vocab_size, embedding_dim, num_heads, hidden_dim, num_layers, dropout,sequence_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a3ba38e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "215702"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_parameters(TT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c613c4cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "teacher_toy = Teacher(TT,(sequence_length,)) #don't specify batch!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5e4db9fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "##some of these configs made for more diverse outputs in teachers:\n",
    "config_args = {\"dist_type\" : \"ints\" ##worked well\n",
    "                      , \"gen_m\" : vocab_size\n",
    "                      , \"gen_n\" : 100\n",
    "                      , \"gen_epochs\" : 100\n",
    "                      , \"gen_lr\" : 0.001\n",
    "                      , \"random_shuffle\" : 0.1\n",
    "                      , \"out_type\" : \"one-hot\" }\n",
    "\n",
    "config_args_high_epochs = {\"dist_type\" : \"ints\" ##okay, but not as well as config_args.\n",
    "                      , \"gen_m\" : vocab_size\n",
    "                      , \"gen_n\" : 1000\n",
    "                      , \"gen_epochs\" : 100\n",
    "                      , \"gen_lr\" : 0.001\n",
    "                      , \"random_shuffle\" : 0.1\n",
    "                      , \"out_type\" : \"one-hot\" }\n",
    "\n",
    "config_args_higher_lr = {\"dist_type\" : \"ints\" ##lower was worse.  raise it. 0.003 looks great.  this is the best.\n",
    "                      , \"gen_m\" : vocab_size\n",
    "                      , \"gen_n\" : 2000\n",
    "                      , \"gen_epochs\" : 50\n",
    "                      , \"gen_lr\" :  0.003 ##0.003\n",
    "                      , \"random_shuffle\" : 0.8\n",
    "                      , \"out_type\" : \"one-hot\" }\n",
    "\n",
    "config_args_less_data = {\"dist_type\" : \"ints\" ##worse\n",
    "                      , \"gen_m\" : vocab_size\n",
    "                      , \"gen_n\" : 500\n",
    "                      , \"gen_epochs\" : 100\n",
    "                      , \"gen_lr\" : 0.003\n",
    "                      , \"random_shuffle\" : 0.1\n",
    "                      , \"out_type\" : \"one-hot\" }\n",
    "\n",
    "config_args_high_shuffle = {\"dist_type\" : \"ints\" ##one bar..\n",
    "                      , \"gen_m\" : vocab_size\n",
    "                      , \"gen_n\" : 10_000\n",
    "                      , \"gen_epochs\" : 10\n",
    "                      , \"gen_lr\" : 0.05\n",
    "                      , \"random_shuffle\" : 0.9\n",
    "                      , \"out_type\" : \"one-hot\" }\n",
    "\n",
    "config_args_small_batch = {\"dist_type\" : \"ints\" ##this one was the first to do well.  not just one bar and the rest nearly zero.\n",
    "                      , \"gen_m\" : vocab_size\n",
    "                      , \"gen_n\" : 10_000\n",
    "                      , \"gen_epochs\" : 10\n",
    "                      , \"gen_lr\" : 0.05\n",
    "                      , \"random_shuffle\" : 0.5\n",
    "                      , \"batch_size\" : 10\n",
    "                      , \"out_type\" : \"one-hot\" }\n",
    "\n",
    "config_ab = {\"dist_type\" : \"ints\" ##worked well\n",
    "                      , \"gen_m\" : vocab_size\n",
    "                      , \"gen_n\" : 5000\n",
    "                      , \"gen_epochs\" : 200\n",
    "                      , \"gen_lr\" : 0.005\n",
    "                      , \"random_shuffle\" : 0.0\n",
    "                      , \"out_type\" : \"one-hot\" \n",
    "                      , \"dist_type\" : 'hetero'\n",
    "                      , \"alpha\" : 1\n",
    "                      , \"beta\" : 4} #maybe increase epochs?\n",
    "config_debug = {\"dist_type\" : \"ints\" ##worked well\n",
    "                      , \"gen_m\" : vocab_size\n",
    "                      , \"gen_n\" : 5\n",
    "                      , \"gen_epochs\" : 2\n",
    "                      , \"gen_lr\" : 0.005\n",
    "                      , \"random_shuffle\" : 0.0\n",
    "                      , \"out_type\" : \"one-hot\" \n",
    "                      , \"dist_type\" : 'hetero'\n",
    "                      , \"alpha\" : 1\n",
    "                      , \"beta\" : 4}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d700f138",
   "metadata": {},
   "outputs": [],
   "source": [
    "#config should be on device, and, so should generate.  deal with train on your own for now.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0fb0ccfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expected tensor for argument #1 'indices' to have one of the following scalar types: Long, Int; but got torch.FloatTensor instead (while checking arguments for embedding)\n",
      "lets try ints!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Configuring Teacher:: 100%|███████████████████████| 2/2 [00:00<00:00, 25.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU is not available, using CPU\n",
      "Teacher Configured\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "teacher_toy.configure(**config_debug) #this is dying.  might be time for colab!!\n",
    "#teacher_toy.load_state_dict('good_toy.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "77f954d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating val data ::  15%|███▏                 | 3/20 [00:00<00:00, 25.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU is not available, using CPU\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating val data :: 100%|████████████████████| 20/20 [00:00<00:00, 31.52it/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "args = { 'val_train' : \"val\"\n",
    "                      , 'n' : 1000\n",
    "                      , 'dist_type' : 'ints'\n",
    "                      , 'm' : vocab_size\n",
    "                      , 'std': 1.0\n",
    "                      , 'batch_size' : 50\n",
    "                      , 'store_outputs': True\n",
    "        }\n",
    "teacher_toy.generate_data(**args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3ca1f01",
   "metadata": {},
   "outputs": [],
   "source": [
    "teacher_toy.model.embedding.weight.detach().numpy() #use this to make the matrix, just linear? not sure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "513eec1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "teacher_toy.graph_dataset_dist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61a33f74",
   "metadata": {},
   "outputs": [],
   "source": [
    "teacher_toy.val_inputs.shape,teacher_toy.val_targets.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b7d79bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "teacher_slm.val_inputs.shape, teacher_slm.val_targets.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9bc7ec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.all(teacher_slm.model(teacher_slm.val_inputs[1]) ==  teacher_slm.val_targets[1]) ##torch.allclose doesn't work either"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5916bb4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "##this function works, it is in LABNET.  \n",
    "def one_hot_last_dim(tensor_shape):\n",
    "    num_classes = tensor_shape[-1]\n",
    "    random_idx = np.random.randint(1, num_classes + 1, size=tensor_shape[:-1])\n",
    "    zero_tensor = np.zeros(tensor_shape, dtype=int)\n",
    "    last_dim_indices = np.arange(num_classes)\n",
    "    zero_tensor[..., :, last_dim_indices] = (random_idx[..., np.newaxis] == last_dim_indices)\n",
    "    return zero_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e221c7c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "import math\n",
    "\n",
    "warmup_steps = 60_000  # Adjust this as needed steps is total items passed through\n",
    "total_steps = 200_000  # Adjust this as needed i'd like to calculate this....\n",
    "\n",
    "\n",
    "# Define a learning rate scheduler with warmup\n",
    "def lr_lambda(current_step):\n",
    "    if current_step < warmup_steps:\n",
    "        # During warmup, increase learning rate linearly\n",
    "        return float(current_step) / float(max(1, warmup_steps))\n",
    "    else:\n",
    "        # After warmup, decrease learning rate using some schedule\n",
    "        # You can use any LR schedule you prefer here\n",
    "        # For example, you can use a learning rate schedule like CosineAnnealing\n",
    "        return 0.5 * (1 + math.cos(math.pi * (current_step - warmup_steps) / (total_steps - warmup_steps)))\n",
    "\n",
    "\n",
    "\n",
    "\"\"\" Training loop\n",
    "for step in range(total_steps):\n",
    "    optimizer.zero_grad()\n",
    "    # Compute your loss and backpropagation here\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    scheduler.step()\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b44d8243",
   "metadata": {},
   "outputs": [],
   "source": [
    "student = ToyTransformer(vocab_size, embedding_dim, num_heads, hidden_dim, num_layers, dropout,sequence_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19e56b2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#############non-repeating train\n",
    "learning_rate = 0.001 #0.00025 this shit is slooooow\n",
    "momentum = 0.95\n",
    "val_batch_size = 10\n",
    "\n",
    "\n",
    "num_epochs = 50\n",
    "batches_per_epoch = 100\n",
    "batch_size = 1000\n",
    "data_per_batch = batch_size\n",
    "\n",
    "criterion =  nn.MSELoss() #nn.KLDivLoss() #nn.CrossEntropyLoss()  #i think stick to mse for now.  this probs just needs lots of time to start learning.  like s4 lol.\n",
    "optimizer = optim.Adam(student.parameters(), lr=learning_rate)\n",
    "#scheduler = LambdaLR(optimizer, lr_lambda=lr_lambda)\n",
    "#optimizer = optim.SGD(student.parameters(), lr=learning_rate, momentum=momentum)\n",
    "#no train data loader here.\n",
    "val_data = list(zip(teacher_toy.val_inputs, teacher_toy.val_targets))\n",
    "val_input_tensors = torch.stack([torch.Tensor(x[0]) for x in val_data])\n",
    "val_target_tensors = torch.stack([torch.Tensor(x[1]) for x in val_data])\n",
    "val_dataset = TensorDataset(val_input_tensors, val_target_tensors)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=val_batch_size, shuffle=True)\n",
    "accuracy_threshold = 0.5\n",
    "print_every = 1 #its working, its just lots of data mama.\n",
    "validation_every = 1\n",
    "\n",
    "\n",
    "gen_args  = { 'val_train' : \"train\"\n",
    "              , 'n' : data_per_batch\n",
    "              , 'dist_type' : 'ints' #this is generating ints for inputs, the outputs are logits.  hmmmm\n",
    "              , 'm' : vocab_size\n",
    "              , 'std': 1.0\n",
    "              , 'display_progress' : False\n",
    "              , 'store_outputs' : True\n",
    "        }\n",
    "\n",
    "losses = []  # List to store losses\n",
    "accuracies = []\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "student = student.to(device)\n",
    "for epoch in range(num_epochs):\n",
    "    student.train()\n",
    "    total_loss = 0\n",
    "    correct_predictions = 0\n",
    "    total_samples = 0\n",
    "\n",
    "    for i in range(batches_per_epoch):\n",
    "        optimizer.zero_grad()  # Zero the gradients\n",
    "\n",
    "        teacher_toy.generate_data(**gen_args)\n",
    "        data_e = list(zip(teacher_toy.train_inputs, teacher_toy.train_targets))\n",
    "        input_tensors_e = torch.stack([torch.Tensor(x[0]) for x in data_e])\n",
    "        target_tensors_e = torch.stack([torch.Tensor(x[1]) for x in data_e])\n",
    "        dataset_e = TensorDataset(input_tensors_e, target_tensors_e)\n",
    "        dataloader_e = DataLoader(dataset_e, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "        for input_batch_e, target_batch_e in dataloader_e:\n",
    "            optimizer.zero_grad()  # Zero the gradients\n",
    "            input_batch_e = input_batch_e.to(device)\n",
    "            target_batch_e = target_batch_e.to(device)\n",
    "            output = student(input_batch_e)  # Forward pass\n",
    "            loss = criterion(output, target_batch_e)  # Compute the loss\n",
    "            loss.backward()  # Backpropagation\n",
    "            optimizer.step()  # Update the weights\n",
    "\n",
    "            #scheduler.step() ##this does the warmup step.  \n",
    "\n",
    "            total_loss += loss.item()\n",
    "            #add early stopping...\n",
    "            #and validation at each step.\n",
    "            # Calculate accuracy\n",
    "            predictions = (output > accuracy_threshold).float()  # Assuming a threshold of 0.5 for binary classification\n",
    "            correct_predictions += (predictions == target_batch_e).sum().item()\n",
    "            total_samples += input_batch_e.size(0)\n",
    "\n",
    "    # Print the average loss for this epoch\n",
    "    avg_loss = total_loss / ( batches_per_epoch * data_per_batch)\n",
    "    losses.append(avg_loss)\n",
    "\n",
    "    accuracy = correct_predictions / total_samples\n",
    "    accuracies.append(accuracy)\n",
    "\n",
    "    if (epoch + 1) % print_every == 0:\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {total_loss}, Train Accuracy: {accuracy:.4f}')\n",
    "        current_lr = optimizer.param_groups[0]['lr']\n",
    "        print(current_lr)\n",
    "    \n",
    "    if (epoch + 1) % validation_every == 0:\n",
    "        student.eval()\n",
    "\n",
    "        total_val_samples = 0\n",
    "        correct_val_predictions = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for val_input_batch, val_target_batch in val_dataloader:\n",
    "                val_input_batch = val_input_batch.to(device)\n",
    "                val_target_batch = val_target_batch.to(device)\n",
    "                val_output = student(val_input_batch)\n",
    "                val_predictions = (val_output > accuracy_threshold).float()  # Assuming a threshold of 0.5 for binary classification\n",
    "                correct_val_predictions += (val_predictions == val_target_batch).sum().item()\n",
    "                total_val_samples += val_input_batch.size(0)\n",
    "\n",
    "        # Calculate validation accuracy\n",
    "        val_accuracy = correct_val_predictions / total_val_samples\n",
    "\n",
    "        # Print the validation accuracy for this epoch\n",
    "        print(f'\\t\\tValidation Accuracy: {val_accuracy:.4f}')\n",
    "\n",
    "        # Set the model back to training mode\n",
    "        student.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a4c7d9d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
