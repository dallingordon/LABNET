{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf4701eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from LABNET import Teacher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12e88d64",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch\n",
    "from collections import OrderedDict\n",
    "from torch.utils.data import DataLoader, TensorDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8875cc12",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_losses(losses, plot_size=(10, 5)):\n",
    "    \"\"\"\n",
    "    Plot a list of losses and adjust the shape of the plot.\n",
    "\n",
    "    Args:\n",
    "        losses (list): List of loss values to be plotted.\n",
    "        plot_size (tuple): Tuple specifying the size of the plot (width, height).\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    # Set the plot size based on the provided tuple\n",
    "    plt.figure(figsize=plot_size)\n",
    "    \n",
    "    # Create a line plot of the losses\n",
    "    plt.plot(losses, label='Loss', color='b')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Loss Over Epochs')\n",
    "    plt.legend()\n",
    "    \n",
    "    # Show the plot\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aebd5a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "layer_sizes = [8, 7,7,4]  # Inputs: 4, Hidden layers: [8, 8], Outputs: 3\n",
    "teacher = Teacher(layer_sizes)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eb431b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "teacher.configure()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96fc55af",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 5000 #number of samples to generate\n",
    "teacher.generate_data(\n",
    "    \"train\"\n",
    "    ,n\n",
    "    ,'normal'\n",
    "    , m =0.0\n",
    "    , std=1.0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d81bca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "teacher.train_inputs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b868471",
   "metadata": {},
   "outputs": [],
   "source": [
    "teacher.generate_data(\n",
    "    \"val\"\n",
    "    ,n\n",
    "    ,'normal'\n",
    "    , m =0.0\n",
    "    , std=1.0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b42b3df",
   "metadata": {},
   "outputs": [],
   "source": [
    "teacher.val_inputs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ae2e479",
   "metadata": {},
   "outputs": [],
   "source": [
    "teacher.model.state_dict()\n",
    "#torch.save(neural_network.model.state_dict(), 'model_weights.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2b2ec77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model\n",
    "class MyModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MyModel, self).__init__()\n",
    "        self.hidden_1 = nn.Linear(8, 7) ##make these all variables duh\n",
    "        self.hidden_2 = nn.Linear(7,7)\n",
    "        #self.hidden_3 = nn.Linear(7,7)\n",
    "        #self.hidden_4 = nn.Linear(7,7)\n",
    "        #self.hidden_5 = nn.Linear(7,7)\n",
    "        self.output = nn.Linear(7, 4)\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.hidden_1(x))\n",
    "        x = self.relu(self.hidden_2(x))\n",
    "        #x = self.relu(self.hidden_3(x))\n",
    "        #x = self.relu(self.hidden_4(x))\n",
    "        #x = self.relu(self.hidden_5(x))\n",
    "        x = self.output(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8807a787",
   "metadata": {},
   "outputs": [],
   "source": [
    "student = MyModel()\n",
    "print(student)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e4372c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "##STARTING with the right answuers gives some close to zero row_comp:3.5974587e-08,0.0,0.0; 0.0,2.9802322e-08,1.4901161e-08\n",
    "#interesting that you get a zero, and then a close to zero.  \n",
    "#perturb_weights(mymodel, std_dev = 0.005) # converges\n",
    "#perturb_weights(mymodel, std_dev = 0.009) # converges\n",
    "#perturb_weights(mymodel, std_dev = 0.05) doesn't converge\n",
    "#perturb_weights(mymodel, std_dev = 0.01) #converges\n",
    "#perturb_weights(mymodel, std_dev = 0.01) #not converging well, even with lower lr, 0.003. 1t 0.001\n",
    "\n",
    "\n",
    "# scale_weights(mymodel, scaling_factor = 1.1) ## converges, weights cross.  \n",
    "#scale_weights(mymodel, scaling_factor = 1.5) ## good validation, non zero row comapre still..\n",
    "\n",
    "#scale_weights(mymodel, scaling_factor = 2) ## good validation, row compare stays pretty close to the same...\n",
    "## scale_weights(mymodel, scaling_factor = 1.05) row compare barely improves!!\n",
    "student.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35f42444",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "num_epochs = 100\n",
    "batch_size = 32\n",
    "learning_rate = 0.00025 #was 0.001, 0.0005\n",
    "momentum = 0.95\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(student.parameters(), lr=learning_rate)\n",
    "\n",
    "optimizer = optim.SGD(student.parameters(), lr=learning_rate, momentum=momentum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5377e988",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = list(zip(teacher.train_inputs, teacher.train_targets))\n",
    "input_tensors = torch.stack([torch.Tensor(x[0]) for x in data])\n",
    "target_tensors = torch.stack([torch.Tensor(x[1]) for x in data])\n",
    "dataset = TensorDataset(input_tensors, target_tensors)\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28d81ffd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#conventional train (uses generated data once.)\n",
    "print_every = 10\n",
    "\n",
    "losses = []  # List to store losses\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    total_loss = 0\n",
    "    for input_batch, target_batch in dataloader:\n",
    "        optimizer.zero_grad()  # Zero the gradients\n",
    "        output = student(input_batch)  # Forward pass\n",
    "        loss = criterion(output, target_batch)  # Compute the loss\n",
    "        loss.backward()  # Backpropagation\n",
    "        optimizer.step()  # Update the weights\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    # Print the average loss for this epoch\n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    losses.append(avg_loss)\n",
    "    if (epoch + 1) % print_every == 0:\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {avg_loss:.8f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19fcd259",
   "metadata": {},
   "outputs": [],
   "source": [
    "###train with no repeating training data\n",
    "print_every = 10\n",
    "batch_eternal = 50\n",
    "data_n = 5000\n",
    "num_epochs = 400\n",
    "losses = []  # List to store losses\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    total_loss = 0\n",
    "    teacher.generate_data(\n",
    "    \"train\"\n",
    "    ,data_n\n",
    "    ,'normal'\n",
    "    , m =0.0\n",
    "    , std=1.0\n",
    "    )\n",
    "    data = list(zip(teacher.train_inputs, teacher.train_targets))\n",
    "    input_tensors = torch.stack([torch.Tensor(x[0]) for x in data])\n",
    "    target_tensors = torch.stack([torch.Tensor(x[1]) for x in data])\n",
    "    dataset = TensorDataset(input_tensors, target_tensors)\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_eternal, shuffle=True)\n",
    "    \n",
    "    for input_batch, target_batch in dataloader:\n",
    "        optimizer.zero_grad()  # Zero the gradients\n",
    "        output = student(input_batch)  # Forward pass\n",
    "        loss = criterion(output, target_batch)  # Compute the loss\n",
    "        loss.backward()  # Backpropagation\n",
    "        optimizer.step()  # Update the weights\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    # Print the average loss for this epoch\n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    losses.append(avg_loss)\n",
    "    if (epoch + 1) % print_every == 0:\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {avg_loss:.8f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac518f97",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plot_losses(losses, plot_size=(10, 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60b02a03",
   "metadata": {},
   "outputs": [],
   "source": [
    "v_batch_size = 10\n",
    "val_data = list(zip(teacher.val_inputs, teacher.val_targets))\n",
    "val_input_tensors = torch.stack([torch.Tensor(x[0]) for x in val_data])\n",
    "val_target_tensors = torch.stack([torch.Tensor(x[1]) for x in val_data])\n",
    "val_dataset = TensorDataset(val_input_tensors, val_target_tensors)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=v_batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4ca7f34",
   "metadata": {},
   "outputs": [],
   "source": [
    "student.eval()\n",
    "\n",
    "differences = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for val_batch in val_dataloader:\n",
    "        val_inputs, val_targets = val_batch\n",
    "        #val_inputs, val_targets = val_inputs\\, val_targets.to(device)  # If you are using GPU, move data to the GPU\n",
    "        # Forward pass\n",
    "        val_outputs = student(val_inputs)\n",
    "        \n",
    "        # Calculate the absolute differences between predicted and actual values\n",
    "        diff = val_outputs - val_targets\n",
    "        \n",
    "        # Extend the list of differences\n",
    "        differences.extend(diff.cpu().numpy().flatten())  # Convert to NumPy for histogram plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8131b9cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a histogram of the differences\n",
    "plt.hist(differences, bins=50)  # You can adjust the number of bins as needed\n",
    "plt.xlabel('Absolute Differences')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Histogram of Absolute Differences between Predicted and Actual Values')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adf88852",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_difference = np.mean(differences)\n",
    "std_deviation = np.std(differences)\n",
    "mean_difference,std_deviation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39efcd0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#what if it never sees the same data? don't need new val data, just new training data.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d7a5859",
   "metadata": {},
   "source": [
    "#compare all novel data to epochs, that is, say we do 100 epochs on a dataset of 1000 input examples.  vs 100 epochs of all novel data.  which performs better on a hold out set?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5b6e122",
   "metadata": {},
   "source": [
    "#is there any value to swapping between momentum and just adam? messing with the lr as well?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
