{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6a04cb7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "99b445af",
   "metadata": {},
   "outputs": [],
   "source": [
    "from NoiseKD import Teacher"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5a11b24",
   "metadata": {},
   "source": [
    "First hurdle. the embedding layer.  It requires the ints as inputs, not one hot.  i need one hot.  i will also need to rewire the LLM i use to do the same.  hmm.  save the weights, load it into a model that has the same shapes, but accepts one hot arrays, not vocab_indexes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dad448f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEncoder(nn.Module):\n",
    "    def __init__(self, embedding_dim, num_heads, hidden_dim, num_layers, dropout):\n",
    "        super(TransformerEncoder, self).__init__()\n",
    "        self.transformer = nn.TransformerEncoder(\n",
    "            nn.TransformerEncoderLayer(embedding_dim, num_heads, hidden_dim, dropout),\n",
    "            num_layers\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.transformer(x)\n",
    "\n",
    "class SimpleLanguageModel(nn.Module):\n",
    "    def __init__(self, vocab_size, sequence_length, embedding_dim, class_num, num_heads, hidden_dim, num_layers, dropout):\n",
    "        super(SimpleLanguageModel, self).__init__()\n",
    "\n",
    "        # Define the embedding layer\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        \n",
    "        # Define the transformer encoder\n",
    "        self.transformer_encoder = TransformerEncoder(embedding_dim, num_heads, hidden_dim, num_layers, dropout)\n",
    "        \n",
    "        self.fc1 = nn.Linear(sequence_length*embedding_dim, 1000)\n",
    "        self.fc2 = nn.Linear(1000, 500)\n",
    "        \n",
    "        self.output_layer = nn.Linear(500, class_num)\n",
    "        \n",
    "        \n",
    "\n",
    "    def forward(self, input_data):\n",
    "        # Input_data is of shape (batch_size, sequence_length)\n",
    "        # Apply embedding layer\n",
    "        #print(input_data.shape)\n",
    "        embedded = self.embedding(input_data)\n",
    "        #print(embedded.shape)\n",
    "        # Pass through the transformer encoder\n",
    "        transformed = self.transformer_encoder(embedded)\n",
    "        #print(transformed.shape) same as input duh: batch x sequence_length x embedding_dim\n",
    "        flattened_tensor = transformed.view(-1,sequence_length*embedding_dim)\n",
    "        f1 = nn.ReLU()(self.fc1(flattened_tensor))\n",
    "        f2 = nn.ReLU()(self.fc2(f1))\n",
    "        out = self.output_layer(f2)\n",
    "        # Apply the output layer\n",
    "        output = F.softmax(out,dim=1)\n",
    "\n",
    "        return output\n",
    "    \n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cca2a6b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 16\n",
    "num_heads = 8\n",
    "hidden_dim  = 11\n",
    "num_layers = 2\n",
    "dropout = 0.1\n",
    "vocab_size = 80\n",
    "class_num = vocab_size\n",
    "batch_size = 23\n",
    "sequence_length = 160"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a7b5a7d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "SLM = SimpleLanguageModel(vocab_size, sequence_length, embedding_dim, class_num, num_heads, hidden_dim, num_layers, dropout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d8fc6fde",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SimpleLanguageModel(\n",
       "  (embedding): Embedding(80, 16)\n",
       "  (transformer_encoder): TransformerEncoder(\n",
       "    (transformer): TransformerEncoder(\n",
       "      (layers): ModuleList(\n",
       "        (0): TransformerEncoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=16, out_features=16, bias=True)\n",
       "          )\n",
       "          (linear1): Linear(in_features=16, out_features=11, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (linear2): Linear(in_features=11, out_features=16, bias=True)\n",
       "          (norm1): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout1): Dropout(p=0.1, inplace=False)\n",
       "          (dropout2): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (1): TransformerEncoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=16, out_features=16, bias=True)\n",
       "          )\n",
       "          (linear1): Linear(in_features=16, out_features=11, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (linear2): Linear(in_features=11, out_features=16, bias=True)\n",
       "          (norm1): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout1): Dropout(p=0.1, inplace=False)\n",
       "          (dropout2): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (fc1): Linear(in_features=2560, out_features=1000, bias=True)\n",
       "  (fc2): Linear(in_features=1000, out_features=500, bias=True)\n",
       "  (output_layer): Linear(in_features=500, out_features=80, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "85578ee4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([23, 160])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([23, 80])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_data = torch.randint(low=0, high=vocab_size, size=(batch_size, sequence_length))\n",
    "print(input_data.shape)\n",
    "SLM(input_data).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a3ba38e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3105922"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_parameters(SLM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c613c4cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "teacher_slm = Teacher(SLM,(sequence_length,)) #don't specify batch!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5e4db9fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "##some of these configs made for more diverse outputs in teachers:\n",
    "config_args = {\"dist_type\" : \"ints\" ##worked well\n",
    "                      , \"gen_m\" : vocab_size\n",
    "                      , \"gen_n\" : 100\n",
    "                      , \"gen_epochs\" : 100\n",
    "                      , \"gen_lr\" : 0.001\n",
    "                      , \"random_shuffle\" : 0.1\n",
    "                      , \"out_type\" : \"one-hot\" }\n",
    "\n",
    "config_args_high_epochs = {\"dist_type\" : \"ints\" ##okay, but not as well as config_args.\n",
    "                      , \"gen_m\" : vocab_size\n",
    "                      , \"gen_n\" : 1000\n",
    "                      , \"gen_epochs\" : 100\n",
    "                      , \"gen_lr\" : 0.001\n",
    "                      , \"random_shuffle\" : 0.1\n",
    "                      , \"out_type\" : \"one-hot\" }\n",
    "\n",
    "config_args_higher_lr = {\"dist_type\" : \"ints\" ##lower was worse.  raise it. 0.003 looks great.  this is the best.\n",
    "                      , \"gen_m\" : vocab_size\n",
    "                      , \"gen_n\" : 2000\n",
    "                      , \"gen_epochs\" : 50\n",
    "                      , \"gen_lr\" :  0.003 ##0.003\n",
    "                      , \"random_shuffle\" : 0.8\n",
    "                      , \"out_type\" : \"one-hot\" }\n",
    "\n",
    "config_args_less_data = {\"dist_type\" : \"ints\" ##worse\n",
    "                      , \"gen_m\" : vocab_size\n",
    "                      , \"gen_n\" : 500\n",
    "                      , \"gen_epochs\" : 100\n",
    "                      , \"gen_lr\" : 0.003\n",
    "                      , \"random_shuffle\" : 0.1\n",
    "                      , \"out_type\" : \"one-hot\" }\n",
    "\n",
    "config_args_high_shuffle = {\"dist_type\" : \"ints\" ##one bar..\n",
    "                      , \"gen_m\" : vocab_size\n",
    "                      , \"gen_n\" : 10_000\n",
    "                      , \"gen_epochs\" : 10\n",
    "                      , \"gen_lr\" : 0.05\n",
    "                      , \"random_shuffle\" : 0.9\n",
    "                      , \"out_type\" : \"one-hot\" }\n",
    "\n",
    "config_args_small_batch = {\"dist_type\" : \"ints\" ##this one was the first to do well.  not just one bar and the rest nearly zero.\n",
    "                      , \"gen_m\" : vocab_size\n",
    "                      , \"gen_n\" : 10_000\n",
    "                      , \"gen_epochs\" : 10\n",
    "                      , \"gen_lr\" : 0.05\n",
    "                      , \"random_shuffle\" : 0.5\n",
    "                      , \"batch_size\" : 10\n",
    "                      , \"out_type\" : \"one-hot\" }\n",
    "\n",
    "config_ab = {\"dist_type\" : \"ints\" ##worked well\n",
    "                      , \"gen_m\" : vocab_size\n",
    "                      , \"gen_n\" : 5000\n",
    "                      , \"gen_epochs\" : 200\n",
    "                      , \"gen_lr\" : 0.005\n",
    "                      , \"random_shuffle\" : 0.0\n",
    "                      , \"out_type\" : \"one-hot\" \n",
    "                      , \"dist_type\" : 'hetero'\n",
    "                      , \"alpha\" : 1\n",
    "                      , \"beta\" : 4} #maybe increase epochs?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0fb0ccfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expected tensor for argument #1 'indices' to have one of the following scalar types: Long, Int; but got torch.FloatTensor instead (while checking arguments for embedding)\n",
      "lets try ints!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Configuring Teacher:: 100%|█████████████████████| 20/20 [01:37<00:00,  4.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Teacher Configured\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "teacher_slm.configure(**config_ab) #this is dying.  might be time for colab!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "77f954d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating val data :: 100%|████████████████████| 20/20 [00:00<00:00, 48.44it/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "args = { 'val_train' : \"val\"\n",
    "                      , 'n' : 1000\n",
    "                      , 'dist_type' : 'ints'\n",
    "                      , 'm' : vocab_size\n",
    "                      , 'std': 1.0\n",
    "                      , 'alpha' : 1\n",
    "                      , 'beta' : 3\n",
    "        }\n",
    "teacher_slm.generate_data(**args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f3ca1f01",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.3187903 ,  0.33806166, -0.80485404, ..., -0.30445045,\n",
       "        -0.12934707, -1.0667611 ],\n",
       "       [ 0.51193213, -0.26568168, -0.46757028, ...,  0.080157  ,\n",
       "         1.9483086 , -0.6070681 ],\n",
       "       [-0.84135073, -1.4381226 , -0.07764673, ..., -0.5033036 ,\n",
       "        -2.2838933 , -1.4296011 ],\n",
       "       ...,\n",
       "       [ 1.1632829 ,  1.9486086 ,  0.55538565, ...,  0.9299457 ,\n",
       "         0.87375116, -0.3640131 ],\n",
       "       [ 1.4722823 ,  0.68209755,  0.04361369, ..., -2.5488482 ,\n",
       "        -0.4152095 ,  0.12250213],\n",
       "       [-0.00295405,  0.57261395, -0.6826916 , ...,  0.8905758 ,\n",
       "        -0.2983099 , -0.42924026]], dtype=float32)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "teacher_slm.model.embedding.weight.detach().numpy() #use this to make the matrix, just linear? not sure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "513eec1f",
   "metadata": {},
   "outputs": [],
   "source": [
    " #the code is right, i just need this to be a better dist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "24c7711d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 2880x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "teacher_slm.graph_dataset_dist(val_train = 'val')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "0b6de08b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(True)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.all(teacher_slm.model(teacher_slm.in_test[50:100]) == teacher_slm.val_targets[50:100]) ##okay, this is weird.  this works\n",
    "#in test is concatenated the same way as the outputs in KDNoise.  only difference.  doesn't appear to be different.  \n",
    "torch.all(teacher_slm.model(teacher_slm.val_inputs[50:100]) == teacher_slm.val_targets[50:100]) ##yeah, val_inputs and in_test appear to be the same.\n",
    "#torch.all(teacher_slm.model(teacher_slm.in_test[:49]) == teacher_slm.val_targets[:49]) ##this doesn't/.  huh???!!\n",
    "#torch.all(teacher_slm.model(teacher_slm.in_test[:50]) == teacher_slm.model(teacher_slm.val_inputs[:50])) ##okay, these match too\n",
    "#torch.all(teacher_slm.model(teacher_slm.in_test[223:409]) == teacher_slm.model(teacher_slm.val_inputs[223:409])) #these match.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "9b7d79bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1000, 160]), torch.Size([1000, 80]))"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "teacher_slm.val_inputs.shape, teacher_slm.val_targets.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "b9bc7ec4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(False)"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.all(teacher_slm.model(teacher_slm.val_inputs[1]) ==  teacher_slm.val_targets[1]) ##torch.allclose doesn't work either"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "909987b0",
   "metadata": {},
   "source": [
    "so, it looks like it is predicting correctly, but there is something wrong with the batch? they match when you pass in 50, but more, or not at exactly the 50 marks, it doesn't all match.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6b1e92fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[51, 74, 44,  ..., 25, 60,  3],\n",
      "        [46,  1, 65,  ..., 19, 18, 49],\n",
      "        [79, 44, 77,  ..., 74, 47, 52],\n",
      "        ...,\n",
      "        [ 0, 12, 34,  ..., 24, 73, 48],\n",
      "        [65, 35, 55,  ..., 38, 72, 74],\n",
      "        [26, 50,  6,  ..., 31, 30, 66]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "#added teacher_slm.in_test and teacher_slm.dataloader\n",
    "teacher_slm.data_loader\n",
    "x = None\n",
    "y = None\n",
    "for batch_samples in teacher_slm.data_loader:\n",
    "    # Perform inference on each batch\n",
    "    batch_outputs = teacher_slm.model(batch_samples[0])  # Assuming samples are in the first element of the batch\n",
    "    print(batch_samples[0])\n",
    "    x = batch_samples[0]\n",
    "    print(batch_outputs)\n",
    "    y = batch_outputs\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "d7e63715",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(True), tensor(True), tensor(True))"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#okay, the first 50 match.  do they \n",
    "torch.all(x == teacher_slm.val_inputs[0:50]),torch.all(y == teacher_slm.val_targets[0:50]),torch.all(teacher_slm.model(x) == y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "92b1012b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([50, 80])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "444f7b2a",
   "metadata": {},
   "source": [
    "specify gen outputs.  we need one hots\n",
    "i can do this better than chat gpt lol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5916bb4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "##this function works, it is in LABNET.  \n",
    "def one_hot_last_dim(tensor_shape):\n",
    "    num_classes = tensor_shape[-1]\n",
    "    random_idx = np.random.randint(1, num_classes + 1, size=tensor_shape[:-1])\n",
    "    zero_tensor = np.zeros(tensor_shape, dtype=int)\n",
    "    last_dim_indices = np.arange(num_classes)\n",
    "    zero_tensor[..., :, last_dim_indices] = (random_idx[..., np.newaxis] == last_dim_indices)\n",
    "    return zero_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67fabd52",
   "metadata": {},
   "outputs": [],
   "source": [
    "axis_to_shuffle = 0\n",
    "\n",
    "# Shuffle the tensor along the specified axis\n",
    "shuffled_tensor = np.take(x, np.random.permutation(x.shape[0]), axis=0)\n",
    "\n",
    "print(shuffled_tensor)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba3f68eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "\n",
    "for i in range(10):\n",
    "    random_number = random.random()\n",
    "    print(random_number)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb166fea",
   "metadata": {},
   "outputs": [],
   "source": [
    "#next is to extract the parts.  we need to take out the embedding layer, make it linear, and then take out the layer before softmax.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef060bed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.distributions as dist\n",
    "def generate_heteroskedastic_ints(n, alpha, beta, total_classes):\n",
    "    # Generate random values from the beta distribution\n",
    "    values = np.random.beta(alpha, beta, n)\n",
    "    min_value = 0\n",
    "    max_value = total_classes\n",
    "    scaled_values = min_value + (max_value - min_value) * values\n",
    "    \n",
    "    # Round the scaled values to integers\n",
    "    ints = np.round(scaled_values).astype(int)\n",
    "    \n",
    "    return ints\n",
    "\n",
    "# Set your desired alpha and beta values\n",
    "alpha = 1  # Adjust this to control skewness\n",
    "beta = 10  # Adjust this to control skewness\n",
    "\n",
    "# Generate 100 random integers with the specified heteroskedasticity\n",
    "random_integers = generate_heteroskedastic_ints(10_000, alpha, beta, 100)\n",
    "\n",
    "# Print the generated integers\n",
    "print(random_integers)\n",
    "plt.hist(random_integers, bins=20, edgecolor='k')\n",
    "plt.xlabel('Value')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Frequency Distribution of Random Integers')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d87621b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_heteroskedastic_ints(shape, alpha, beta, total_classes):\n",
    "    # Generate random values from the beta distribution\n",
    "    values = np.random.beta(alpha, beta, shape)\n",
    "    \n",
    "    # Scale the values to integers in the desired range (e.g., 0 to 100)\n",
    "    min_value = 0\n",
    "    max_value = total_classes\n",
    "    scaled_values = min_value + (max_value - min_value) * values\n",
    "    \n",
    "    # Round the scaled values to integers\n",
    "    ints = np.round(scaled_values).astype(int)\n",
    "    \n",
    "    return ints\n",
    "\n",
    "# Set your desired alpha and beta values\n",
    "alpha = 1  # Adjust this to control skewness\n",
    "beta = 1   # Adjust this to control skewness\n",
    "\n",
    "# Specify the shape of the tensor (e.g., b x 200)\n",
    "b = 5\n",
    "shape = (b, 200)\n",
    "\n",
    "# Generate a tensor of random integers with the specified heteroskedasticity\n",
    "random_integers_tensor = generate_heteroskedastic_ints(shape, alpha, beta,20)\n",
    "\n",
    "# Print the generated tensor\n",
    "random_integers_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19e56b2b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
