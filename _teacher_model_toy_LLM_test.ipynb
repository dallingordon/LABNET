{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6a04cb7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "99b445af",
   "metadata": {},
   "outputs": [],
   "source": [
    "from NoiseKD import Teacher"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5a11b24",
   "metadata": {},
   "source": [
    "First hurdle. the embedding layer.  It requires the ints as inputs, not one hot.  i need one hot.  i will also need to rewire the LLM i use to do the same.  hmm.  save the weights, load it into a model that has the same shapes, but accepts one hot arrays, not vocab_indexes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dad448f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEncoder(nn.Module):\n",
    "    def __init__(self, embedding_dim, num_heads, hidden_dim, num_layers, dropout):\n",
    "        super(TransformerEncoder, self).__init__()\n",
    "        self.transformer = nn.TransformerEncoder(\n",
    "            nn.TransformerEncoderLayer(embedding_dim, num_heads, hidden_dim, dropout),\n",
    "            num_layers\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.transformer(x)\n",
    "\n",
    "class SimpleLanguageModel(nn.Module):\n",
    "    def __init__(self, vocab_size, sequence_length, embedding_dim, class_num, num_heads, hidden_dim, num_layers, dropout):\n",
    "        super(SimpleLanguageModel, self).__init__()\n",
    "\n",
    "        # Define the embedding layer\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        \n",
    "        # Define the transformer encoder\n",
    "        self.transformer_encoder = TransformerEncoder(embedding_dim, num_heads, hidden_dim, num_layers, dropout)\n",
    "        \n",
    "        self.fc1 = nn.Linear(sequence_length*embedding_dim, 1000)\n",
    "        self.fc2 = nn.Linear(1000, 500)\n",
    "        \n",
    "        self.output_layer = nn.Linear(500, class_num)\n",
    "        \n",
    "        \n",
    "\n",
    "    def forward(self, input_data):\n",
    "        # Input_data is of shape (batch_size, sequence_length)\n",
    "        # Apply embedding layer\n",
    "        #print(input_data.shape)\n",
    "        embedded = self.embedding(input_data)\n",
    "        #print(embedded.shape)\n",
    "        # Pass through the transformer encoder\n",
    "        transformed = self.transformer_encoder(embedded)\n",
    "        #print(transformed.shape) same as input duh: batch x sequence_length x embedding_dim\n",
    "        flattened_tensor = transformed.view(-1,sequence_length*embedding_dim)\n",
    "        f1 = nn.ReLU()(self.fc1(flattened_tensor))\n",
    "        f2 = nn.ReLU()(self.fc2(f1))\n",
    "        out = self.output_layer(f2)\n",
    "        # Apply the output layer\n",
    "        output = F.softmax(out,dim=1)\n",
    "\n",
    "        return output\n",
    "    \n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cca2a6b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 16\n",
    "num_heads = 8\n",
    "hidden_dim  = 11\n",
    "num_layers = 2\n",
    "dropout = 0.1\n",
    "vocab_size = 80\n",
    "class_num = vocab_size\n",
    "batch_size = 50\n",
    "sequence_length = 160"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a7b5a7d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "SLM = SimpleLanguageModel(vocab_size, sequence_length, embedding_dim, class_num, num_heads, hidden_dim, num_layers, dropout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d8fc6fde",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SimpleLanguageModel(\n",
       "  (embedding): Embedding(80, 16)\n",
       "  (transformer_encoder): TransformerEncoder(\n",
       "    (transformer): TransformerEncoder(\n",
       "      (layers): ModuleList(\n",
       "        (0): TransformerEncoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=16, out_features=16, bias=True)\n",
       "          )\n",
       "          (linear1): Linear(in_features=16, out_features=11, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (linear2): Linear(in_features=11, out_features=16, bias=True)\n",
       "          (norm1): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout1): Dropout(p=0.1, inplace=False)\n",
       "          (dropout2): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (1): TransformerEncoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=16, out_features=16, bias=True)\n",
       "          )\n",
       "          (linear1): Linear(in_features=16, out_features=11, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (linear2): Linear(in_features=11, out_features=16, bias=True)\n",
       "          (norm1): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout1): Dropout(p=0.1, inplace=False)\n",
       "          (dropout2): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (fc1): Linear(in_features=2560, out_features=1000, bias=True)\n",
       "  (fc2): Linear(in_features=1000, out_features=500, bias=True)\n",
       "  (output_layer): Linear(in_features=500, out_features=80, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "85578ee4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([50, 160])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([50, 80])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_data = torch.randint(low=0, high=vocab_size, size=(batch_size, sequence_length))\n",
    "print(input_data.shape)\n",
    "SLM(input_data).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a3ba38e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3105922"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_parameters(SLM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c613c4cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "teacher_slm = Teacher(SLM,(sequence_length,)) #don't specify batch!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5e4db9fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "##some of these configs made for more diverse outputs in teachers:\n",
    "config_args = {\"dist_type\" : \"ints\" ##worked well\n",
    "                      , \"gen_m\" : vocab_size\n",
    "                      , \"gen_n\" : 100\n",
    "                      , \"gen_epochs\" : 100\n",
    "                      , \"gen_lr\" : 0.001\n",
    "                      , \"random_shuffle\" : 0.1\n",
    "                      , \"out_type\" : \"one-hot\" }\n",
    "\n",
    "config_args_high_epochs = {\"dist_type\" : \"ints\" ##okay, but not as well as config_args.\n",
    "                      , \"gen_m\" : vocab_size\n",
    "                      , \"gen_n\" : 1000\n",
    "                      , \"gen_epochs\" : 100\n",
    "                      , \"gen_lr\" : 0.001\n",
    "                      , \"random_shuffle\" : 0.1\n",
    "                      , \"out_type\" : \"one-hot\" }\n",
    "\n",
    "config_args_higher_lr = {\"dist_type\" : \"ints\" ##lower was worse.  raise it. 0.003 looks great.  this is the best.\n",
    "                      , \"gen_m\" : vocab_size\n",
    "                      , \"gen_n\" : 2000\n",
    "                      , \"gen_epochs\" : 50\n",
    "                      , \"gen_lr\" :  0.003 ##0.003\n",
    "                      , \"random_shuffle\" : 0.8\n",
    "                      , \"out_type\" : \"one-hot\" }\n",
    "\n",
    "config_args_less_data = {\"dist_type\" : \"ints\" ##worse\n",
    "                      , \"gen_m\" : vocab_size\n",
    "                      , \"gen_n\" : 500\n",
    "                      , \"gen_epochs\" : 100\n",
    "                      , \"gen_lr\" : 0.003\n",
    "                      , \"random_shuffle\" : 0.1\n",
    "                      , \"out_type\" : \"one-hot\" }\n",
    "\n",
    "config_args_high_shuffle = {\"dist_type\" : \"ints\" ##one bar..\n",
    "                      , \"gen_m\" : vocab_size\n",
    "                      , \"gen_n\" : 10_000\n",
    "                      , \"gen_epochs\" : 10\n",
    "                      , \"gen_lr\" : 0.05\n",
    "                      , \"random_shuffle\" : 0.9\n",
    "                      , \"out_type\" : \"one-hot\" }\n",
    "\n",
    "config_args_small_batch = {\"dist_type\" : \"ints\" ##this one was the first to do well.  not just one bar and the rest nearly zero.\n",
    "                      , \"gen_m\" : vocab_size\n",
    "                      , \"gen_n\" : 10_000\n",
    "                      , \"gen_epochs\" : 10\n",
    "                      , \"gen_lr\" : 0.05\n",
    "                      , \"random_shuffle\" : 0.5\n",
    "                      , \"batch_size\" : 10\n",
    "                      , \"out_type\" : \"one-hot\" }\n",
    "\n",
    "config_ab = {\"dist_type\" : \"ints\" ##worked well\n",
    "                      , \"gen_m\" : vocab_size\n",
    "                      , \"gen_n\" : 5000\n",
    "                      , \"gen_epochs\" : 200\n",
    "                      , \"gen_lr\" : 0.005\n",
    "                      , \"random_shuffle\" : 0.0\n",
    "                      , \"out_type\" : \"one-hot\" \n",
    "                      , \"dist_type\" : 'hetero'\n",
    "                      , \"alpha\" : 1\n",
    "                      , \"beta\" : 4} #maybe increase epochs?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0fb0ccfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expected tensor for argument #1 'indices' to have one of the following scalar types: Long, Int; but got torch.FloatTensor instead (while checking arguments for embedding)\n",
      "lets try ints!\n"
     ]
    }
   ],
   "source": [
    "#teacher_slm.configure(**config_ab) #this is dying.  might be time for colab!!\n",
    "teacher_slm.load_state_dict('good_teacher.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "77f954d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "args = { 'val_train' : \"val\"\n",
    "                      , 'n' : 1000\n",
    "                      , 'dist_type' : 'ints'\n",
    "                      , 'm' : vocab_size\n",
    "                      , 'std': 1.0\n",
    "                      , 'batch_size' : 50\n",
    "        }\n",
    "teacher_slm.generate_data(**args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3de04e2a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.int64, torch.Size([1000, 160]))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "teacher_slm.val_inputs.dtype,teacher_slm.val_inputs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "eaf2dd67",
   "metadata": {},
   "outputs": [],
   "source": [
    "args_float_int = { 'val_train' : \"val\"\n",
    "                      , 'n' : 1000\n",
    "                      , 'dist_type' : 'linearized_ints'\n",
    "                      , 'm' : vocab_size\n",
    "                      , 'std': 1.0\n",
    "                      , 'batch_size' : 50\n",
    "        }\n",
    "\n",
    "teacher_slm.generate_data(**args_float_int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ef2a805a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.float32, torch.Size([1000, 160, 80]))"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "teacher_slm.val_inputs.dtype, teacher_slm.val_inputs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3ca1f01",
   "metadata": {},
   "outputs": [],
   "source": [
    "teacher_slm.model.embedding.weight.detach().numpy() #use this to make the matrix, just linear? not sure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "513eec1f",
   "metadata": {},
   "outputs": [],
   "source": [
    " #the code is right, i just need this to be a better dist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e9ef777",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# Assuming you have teacher_slm.val_inputs as a tensor of shape (1000, input_dim)\n",
    "val_inputs = teacher_slm.val_inputs\n",
    "\n",
    "# Create a DataLoader with a batch size\n",
    "batch_size = 1\n",
    "data_loader = DataLoader(TensorDataset(val_inputs), batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Initialize an empty list to store the results\n",
    "results = []\n",
    "\n",
    "# Set your model to evaluation mode\n",
    "teacher_slm.model.eval()\n",
    "\n",
    "# Iterate through the data loader in batches\n",
    "for batch in data_loader:\n",
    "    batch_inputs = batch[0]  # Get batch inputs\n",
    "    with torch.no_grad():\n",
    "        # Forward pass through the model\n",
    "        batch_outputs = teacher_slm.model(batch_inputs)\n",
    "    # Calculate the argmax along the last axis (-1)\n",
    "    #batch_argmax = torch.argmax(batch_outputs, dim=-1)\n",
    "    # Append the results to the list\n",
    "    results.append(batch_outputs)\n",
    "\n",
    "# Concatenate the results to get the final tensor of shape (1000, output_dim)\n",
    "final_results = torch.cat(results, dim=0)\n",
    "\n",
    "# Print the final_results tensor\n",
    "print(final_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e58647b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.all(final_results[15] == teacher_slm.model(teacher_slm.val_inputs[15]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06c7cbde",
   "metadata": {},
   "outputs": [],
   "source": [
    "teacher_slm.model(teacher_slm.val_inputs[0:39])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24c7711d",
   "metadata": {},
   "outputs": [],
   "source": [
    "teacher_slm.graph_dataset_dist(val_train = 'val')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b6de08b",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.all(teacher_slm.model(teacher_slm.in_test[50:100]) == teacher_slm.val_targets[50:100]) ##okay, this is weird.  this works\n",
    "#in test is concatenated the same way as the outputs in KDNoise.  only difference.  doesn't appear to be different.  \n",
    "torch.all(teacher_slm.model(teacher_slm.val_inputs[50:100]) == teacher_slm.val_targets[50:100]) ##yeah, val_inputs and in_test appear to be the same.\n",
    "#torch.all(teacher_slm.model(teacher_slm.in_test[:49]) == teacher_slm.val_targets[:49]) ##this doesn't/.  huh???!!\n",
    "#torch.all(teacher_slm.model(teacher_slm.in_test[:50]) == teacher_slm.model(teacher_slm.val_inputs[:50])) ##okay, these match too\n",
    "#torch.all(teacher_slm.model(teacher_slm.in_test[223:409]) == teacher_slm.model(teacher_slm.val_inputs[223:409])) #these match.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b7d79bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "teacher_slm.val_inputs.shape, teacher_slm.val_targets.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9bc7ec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.all(teacher_slm.model(teacher_slm.val_inputs[1]) ==  teacher_slm.val_targets[1]) ##torch.allclose doesn't work either"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "909987b0",
   "metadata": {},
   "source": [
    "so, it looks like it is predicting correctly, but there is something wrong with the batch? they match when you pass in 50, but more, or not at exactly the 50 marks, it doesn't all match.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b1e92fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#added teacher_slm.in_test and teacher_slm.dataloader\n",
    "teacher_slm.data_loader\n",
    "x = None\n",
    "y = None\n",
    "for batch_samples in teacher_slm.data_loader:\n",
    "    # Perform inference on each batch\n",
    "    batch_outputs = teacher_slm.model(batch_samples[0])  # Assuming samples are in the first element of the batch\n",
    "    print(batch_samples[0])\n",
    "    x = batch_samples[0]\n",
    "    print(batch_outputs)\n",
    "    y = batch_outputs\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7e63715",
   "metadata": {},
   "outputs": [],
   "source": [
    "#okay, the first 50 match.  do they \n",
    "torch.all(x == teacher_slm.val_inputs[0:50]),torch.all(y == teacher_slm.val_targets[0:50]),torch.all(teacher_slm.model(x) == y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92b1012b",
   "metadata": {},
   "outputs": [],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "444f7b2a",
   "metadata": {},
   "source": [
    "specify gen outputs.  we need one hots\n",
    "i can do this better than chat gpt lol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5916bb4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "##this function works, it is in LABNET.  \n",
    "def one_hot_last_dim(tensor_shape):\n",
    "    num_classes = tensor_shape[-1]\n",
    "    random_idx = np.random.randint(1, num_classes + 1, size=tensor_shape[:-1])\n",
    "    zero_tensor = np.zeros(tensor_shape, dtype=int)\n",
    "    last_dim_indices = np.arange(num_classes)\n",
    "    zero_tensor[..., :, last_dim_indices] = (random_idx[..., np.newaxis] == last_dim_indices)\n",
    "    return zero_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e221c7c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "import math\n",
    "\n",
    "warmup_steps = 60_000  # Adjust this as needed steps is total items passed through\n",
    "total_steps = 200_000  # Adjust this as needed i'd like to calculate this....\n",
    "\n",
    "\n",
    "# Define a learning rate scheduler with warmup\n",
    "def lr_lambda(current_step):\n",
    "    if current_step < warmup_steps:\n",
    "        # During warmup, increase learning rate linearly\n",
    "        return float(current_step) / float(max(1, warmup_steps))\n",
    "    else:\n",
    "        # After warmup, decrease learning rate using some schedule\n",
    "        # You can use any LR schedule you prefer here\n",
    "        # For example, you can use a learning rate schedule like CosineAnnealing\n",
    "        return 0.5 * (1 + math.cos(math.pi * (current_step - warmup_steps) / (total_steps - warmup_steps)))\n",
    "\n",
    "\n",
    "\n",
    "\"\"\" Training loop\n",
    "for step in range(total_steps):\n",
    "    optimizer.zero_grad()\n",
    "    # Compute your loss and backpropagation here\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    scheduler.step()\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19e56b2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#############non-repeating train\n",
    "learning_rate = 0.001 #0.00025 this shit is slooooow\n",
    "momentum = 0.95\n",
    "val_batch_size = 10\n",
    "\n",
    "\n",
    "num_epochs = 50\n",
    "batches_per_epoch = 100\n",
    "batch_size = 1000\n",
    "data_per_batch = batch_size\n",
    "\n",
    "criterion =  nn.MSELoss() #nn.KLDivLoss() #nn.CrossEntropyLoss()  #i think stick to mse for now.  this probs just needs lots of time to start learning.  like s4 lol.\n",
    "optimizer = optim.Adam(student.parameters(), lr=learning_rate)\n",
    "scheduler = LambdaLR(optimizer, lr_lambda=lr_lambda)\n",
    "#optimizer = optim.SGD(student.parameters(), lr=learning_rate, momentum=momentum)\n",
    "#no train data loader here.\n",
    "val_data = list(zip(teacher_slm.val_inputs, teacher_slm.val_targets))\n",
    "val_input_tensors = torch.stack([torch.Tensor(x[0]) for x in val_data])\n",
    "val_target_tensors = torch.stack([torch.Tensor(x[1]) for x in val_data])\n",
    "val_dataset = TensorDataset(val_input_tensors, val_target_tensors)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=val_batch_size, shuffle=True)\n",
    "accuracy_threshold = 0.5\n",
    "print_every = 1 #its working, its just lots of data mama.\n",
    "validation_every = 1\n",
    "\n",
    "\n",
    "gen_args  = { 'val_train' : \"train\"\n",
    "              , 'n' : data_per_batch\n",
    "              , 'dist_type' : 'ints' #this is generating ints for inputs, the outputs are logits.  hmmmm\n",
    "              , 'm' : slm_init_config['vocab_size']\n",
    "              , 'std': 1.0\n",
    "              , 'display_progress' : False\n",
    "        }\n",
    "\n",
    "losses = []  # List to store losses\n",
    "accuracies = []\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "student = student.to(device)\n",
    "for epoch in range(num_epochs):\n",
    "    student.train()\n",
    "    total_loss = 0\n",
    "    correct_predictions = 0\n",
    "    total_samples = 0\n",
    "\n",
    "    for i in range(batches_per_epoch):\n",
    "        optimizer.zero_grad()  # Zero the gradients\n",
    "\n",
    "        teacher_slm.generate_data(**gen_args)\n",
    "        data_e = list(zip(teacher_slm.train_inputs, teacher_slm.train_targets))\n",
    "        input_tensors_e = torch.stack([torch.Tensor(x[0]) for x in data_e])\n",
    "        target_tensors_e = torch.stack([torch.Tensor(x[1]) for x in data_e])\n",
    "        dataset_e = TensorDataset(input_tensors_e, target_tensors_e)\n",
    "        dataloader_e = DataLoader(dataset_e, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "        for input_batch_e, target_batch_e in dataloader_e:\n",
    "            optimizer.zero_grad()  # Zero the gradients\n",
    "            input_batch_e = input_batch_e.to(device)\n",
    "            target_batch_e = target_batch_e.to(device)\n",
    "            output = student(input_batch_e)  # Forward pass\n",
    "            loss = criterion(output, target_batch_e)  # Compute the loss\n",
    "            loss.backward()  # Backpropagation\n",
    "            optimizer.step()  # Update the weights\n",
    "\n",
    "            scheduler.step() ##this does the warmup step.  \n",
    "\n",
    "            total_loss += loss.item()\n",
    "            #add early stopping...\n",
    "            #and validation at each step.\n",
    "            # Calculate accuracy\n",
    "            predictions = (output > accuracy_threshold).float()  # Assuming a threshold of 0.5 for binary classification\n",
    "            correct_predictions += (predictions == target_batch_e).sum().item()\n",
    "            total_samples += input_batch_e.size(0)\n",
    "\n",
    "    # Print the average loss for this epoch\n",
    "    avg_loss = total_loss / ( batches_per_epoch * data_per_batch)\n",
    "    losses.append(avg_loss)\n",
    "\n",
    "    accuracy = correct_predictions / total_samples\n",
    "    accuracies.append(accuracy)\n",
    "\n",
    "    if (epoch + 1) % print_every == 0:\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {total_loss}, Train Accuracy: {accuracy:.4f}')\n",
    "        current_lr = optimizer.param_groups[0]['lr']\n",
    "        print(current_lr)\n",
    "    \n",
    "    if (epoch + 1) % validation_every == 0:\n",
    "        student.eval()\n",
    "\n",
    "        total_val_samples = 0\n",
    "        correct_val_predictions = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for val_input_batch, val_target_batch in val_dataloader:\n",
    "                val_input_batch = val_input_batch.to(device)\n",
    "                val_target_batch = val_target_batch.to(device)\n",
    "                val_output = student(val_input_batch)\n",
    "                val_predictions = (val_output > accuracy_threshold).float()  # Assuming a threshold of 0.5 for binary classification\n",
    "                correct_val_predictions += (val_predictions == val_target_batch).sum().item()\n",
    "                total_val_samples += val_input_batch.size(0)\n",
    "\n",
    "        # Calculate validation accuracy\n",
    "        val_accuracy = correct_val_predictions / total_val_samples\n",
    "\n",
    "        # Print the validation accuracy for this epoch\n",
    "        print(f'\\t\\tValidation Accuracy: {val_accuracy:.4f}')\n",
    "\n",
    "        # Set the model back to training mode\n",
    "        student.train()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
