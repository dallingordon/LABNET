{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "6a04cb7a",
      "metadata": {
        "id": "6a04cb7a"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import TensorDataset\n",
        "from torch.utils.data import DataLoader\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "for next time.  this works.  i am going to need a way to go from the linearized embedding, back to an embedding layer.  i think.  actually, no i don't.  i can just pass in one hots.  thats fine.  okay.  what i do need tho, is the right way to validate.  i need to validate with the original model, which means i think, that i only need to generate the val data before i linearize? is that all? oh i will have to one hot it as well.  i might need to store that twice, the normal val data and the onehotted stuff.  i don't need to store the targets twice tho.   do i need to keep the original model in teacher? i think for now i'm good.  besides if you init with a model, you had to have the model in the first place.  don't duplicate it.  just keep linearized.  \n",
        "\n",
        "ALSO, the scheduler is not working like i expect.  check that."
      ],
      "metadata": {
        "id": "FK6zR97QV3y8"
      },
      "id": "FK6zR97QV3y8"
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import inspect\n",
        "import re\n",
        "\n",
        "github_raw_url = 'https://raw.githubusercontent.com/dallingordon/LABNET/noisekd/NoiseKD.py'  # Replace with the actual URL of the Python file\n",
        "response = requests.get(github_raw_url)\n",
        "\n",
        "if response.status_code == 200:\n",
        "    with open('NoiseKD.py', 'wb') as f:\n",
        "        f.write(response.content)\n",
        "else:\n",
        "    print(f\"Failed to download file from {github_raw_url}\")"
      ],
      "metadata": {
        "id": "t4iusnY50v2Z"
      },
      "id": "t4iusnY50v2Z",
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6ijOfpYUDOH3",
        "outputId": "071a063e-cd03-44f6-cfcc-45800151ff31"
      },
      "id": "6ijOfpYUDOH3",
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "99b445af",
      "metadata": {
        "id": "99b445af"
      },
      "outputs": [],
      "source": [
        "from NoiseKD import Teacher,ToyTransformer, count_parameters"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "##This is the code that lops off the softmax\n",
        "#use the same pretrained weights, that works\n",
        "\n",
        "#this takes the old code and replaces the softmax layer with that commented bit\n",
        "input_class = inspect.getsource(ToyTransformer)\n",
        "print(input_class)\n",
        "new_class_name = \"ToyTransformerNoSoft\"\n",
        "new_code = re.sub(\"ToyTransformer\", new_class_name,input_class,)\n",
        "new_code = re.sub(\"x = F\\.softmax\\(x, dim=1\\)\", \"# removed softmax\",new_code,)\n",
        "print(new_code)\n",
        "\n",
        "#this executes the class def, then you can treat it as if it was imported.\n",
        "try:\n",
        "    exec(new_code)\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred: {e}\")\n",
        "\n",
        "#this instantiates them\n",
        "#x = NewTransformer(vocab_size, embedding_dim, num_heads, hidden_dim, num_layers, dropout,sequence_length)\n",
        "#y = ToyTransformer(vocab_size, embedding_dim, num_heads, hidden_dim, num_layers, dropout,sequence_length)\n",
        "\n",
        "#this was juse me making sure the batching stuff works.\n",
        "#x.load_state_dict(torch.load('good_toy_5.pth')) ##doesn't have softmax\n",
        "#y.load_state_dict(torch.load('good_toy_5.pth')) ##does\n",
        "#x.eval()\n",
        "#y.eval() ##eval it is..\n",
        "\n",
        "#x(teacher_toy.val_inputs[0:10])[0], F.softmax(x(teacher_toy.val_inputs[0:4])[0]), y(teacher_toy.val_inputs[0:10])[0]\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z7WIT88kfOLD",
        "outputId": "cc68fa3a-20a3-4815-aad6-1c6c7c6f9180"
      },
      "id": "Z7WIT88kfOLD",
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "class ToyTransformer(nn.Module):\n",
            "    def __init__(self, vocab_size, embedding_dim, num_heads, hidden_dim, num_layers, dropout,sequence_length):\n",
            "        super(ToyTransformer, self).__init__()\n",
            "\n",
            "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
            "        self.transformer_encoder = nn.TransformerEncoder(\n",
            "            nn.TransformerEncoderLayer(embedding_dim, num_heads, hidden_dim, dropout, batch_first=True),\n",
            "            num_layers\n",
            "        )\n",
            "        self.fc1 = nn.Linear(embedding_dim * sequence_length, vocab_size)  # Intermediate linear layer\n",
            "        self.fc2 = nn.Linear(vocab_size, vocab_size)  # Final linear layer\n",
            "\n",
            "    def forward(self, x):\n",
            "        x = self.embedding(x)\n",
            "        x = self.transformer_encoder(x)\n",
            "        x = torch.flatten(x, start_dim=1)  # Flatten the output\n",
            "        x = F.relu(self.fc1(x))  # Apply the intermediate linear layer with ReLU activation\n",
            "        x = self.fc2(x)  # Apply the final linear layer\n",
            "        x = F.softmax(x, dim=1)  # Apply softmax activation\n",
            "        return x\n",
            "\n",
            "class ToyTransformerNoSoft(nn.Module):\n",
            "    def __init__(self, vocab_size, embedding_dim, num_heads, hidden_dim, num_layers, dropout,sequence_length):\n",
            "        super(ToyTransformerNoSoft, self).__init__()\n",
            "\n",
            "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
            "        self.transformer_encoder = nn.TransformerEncoder(\n",
            "            nn.TransformerEncoderLayer(embedding_dim, num_heads, hidden_dim, dropout, batch_first=True),\n",
            "            num_layers\n",
            "        )\n",
            "        self.fc1 = nn.Linear(embedding_dim * sequence_length, vocab_size)  # Intermediate linear layer\n",
            "        self.fc2 = nn.Linear(vocab_size, vocab_size)  # Final linear layer\n",
            "\n",
            "    def forward(self, x):\n",
            "        x = self.embedding(x)\n",
            "        x = self.transformer_encoder(x)\n",
            "        x = torch.flatten(x, start_dim=1)  # Flatten the output\n",
            "        x = F.relu(self.fc1(x))  # Apply the intermediate linear layer with ReLU activation\n",
            "        x = self.fc2(x)  # Apply the final linear layer\n",
            "        # removed softmax  # Apply softmax activation\n",
            "        return x\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "So, i want both the student and the teacher to do logits.  i want loss (mse) for the logits, but then i want accuracy post softmax.  so, lets do it"
      ],
      "metadata": {
        "id": "3xRFE8MPgGrK"
      },
      "id": "3xRFE8MPgGrK"
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "cca2a6b2",
      "metadata": {
        "id": "cca2a6b2"
      },
      "outputs": [],
      "source": [
        "embedding_dim = 16\n",
        "num_heads = 8\n",
        "hidden_dim  = 11\n",
        "num_layers = 2\n",
        "dropout = 0.1\n",
        "vocab_size = 80\n",
        "class_num = vocab_size\n",
        "batch_size = 50\n",
        "sequence_length = 160"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "a7b5a7d7",
      "metadata": {
        "id": "a7b5a7d7"
      },
      "outputs": [],
      "source": [
        "TT = ToyTransformerNoSoft(vocab_size, embedding_dim, num_heads, hidden_dim, num_layers, dropout,sequence_length)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "a3ba38e1",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a3ba38e1",
        "outputId": "c7985b1b-2ba6-45df-dd15-485e6f433f1d"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "215702"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "count_parameters(TT)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "c613c4cc",
      "metadata": {
        "id": "c613c4cc"
      },
      "outputs": [],
      "source": [
        "teacher_toy = Teacher(TT,(sequence_length,)) #don't specify batch!!"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#dir(teacher_toy)\n"
      ],
      "metadata": {
        "id": "BDvU9E8_TNn6"
      },
      "id": "BDvU9E8_TNn6",
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "5e4db9fb",
      "metadata": {
        "id": "5e4db9fb"
      },
      "outputs": [],
      "source": [
        "##some of these configs made for more diverse outputs in teachers:\n",
        "config_args = {\"dist_type\" : \"ints\" ##worked well\n",
        "                      , \"gen_m\" : vocab_size\n",
        "                      , \"gen_n\" : 100\n",
        "                      , \"gen_epochs\" : 100\n",
        "                      , \"gen_lr\" : 0.001\n",
        "                      , \"random_shuffle\" : 0.1\n",
        "                      , \"out_type\" : \"one-hot\" }\n",
        "\n",
        "config_args_high_epochs = {\"dist_type\" : \"ints\" ##okay, but not as well as config_args.\n",
        "                      , \"gen_m\" : vocab_size\n",
        "                      , \"gen_n\" : 1000\n",
        "                      , \"gen_epochs\" : 100\n",
        "                      , \"gen_lr\" : 0.001\n",
        "                      , \"random_shuffle\" : 0.1\n",
        "                      , \"out_type\" : \"one-hot\" }\n",
        "\n",
        "config_args_higher_lr = {\"dist_type\" : \"ints\" ##lower was worse.  raise it. 0.003 looks great.  this is the best.\n",
        "                      , \"gen_m\" : vocab_size\n",
        "                      , \"gen_n\" : 2000\n",
        "                      , \"gen_epochs\" : 50\n",
        "                      , \"gen_lr\" :  0.003 ##0.003\n",
        "                      , \"random_shuffle\" : 0.8\n",
        "                      , \"out_type\" : \"one-hot\" }\n",
        "\n",
        "config_args_less_data = {\"dist_type\" : \"ints\" ##worse\n",
        "                      , \"gen_m\" : vocab_size\n",
        "                      , \"gen_n\" : 500\n",
        "                      , \"gen_epochs\" : 100\n",
        "                      , \"gen_lr\" : 0.003\n",
        "                      , \"random_shuffle\" : 0.1\n",
        "                      , \"out_type\" : \"one-hot\" }\n",
        "\n",
        "config_args_high_shuffle = {\"dist_type\" : \"ints\" ##one bar..\n",
        "                      , \"gen_m\" : vocab_size\n",
        "                      , \"gen_n\" : 10_000\n",
        "                      , \"gen_epochs\" : 10\n",
        "                      , \"gen_lr\" : 0.05\n",
        "                      , \"random_shuffle\" : 0.9\n",
        "                      , \"out_type\" : \"one-hot\" }\n",
        "\n",
        "config_args_small_batch = {\"dist_type\" : \"ints\" ##this one was the first to do well.  not just one bar and the rest nearly zero.\n",
        "                      , \"gen_m\" : vocab_size\n",
        "                      , \"gen_n\" : 10_000\n",
        "                      , \"gen_epochs\" : 10\n",
        "                      , \"gen_lr\" : 0.05\n",
        "                      , \"random_shuffle\" : 0.5\n",
        "                      , \"batch_size\" : 10\n",
        "                      , \"out_type\" : \"one-hot\" }\n",
        "\n",
        "config_ab = {\"dist_type\" : \"ints\" ##worked well\n",
        "                      , \"gen_m\" : vocab_size\n",
        "                      , \"gen_n\" : 5000\n",
        "                      , \"gen_epochs\" : 200\n",
        "                      , \"gen_lr\" : 0.005\n",
        "                      , \"random_shuffle\" : 0.0\n",
        "                      , \"out_type\" : \"one-hot\"\n",
        "                      , \"dist_type\" : 'hetero'\n",
        "                      , \"alpha\" : 1\n",
        "                      , \"beta\" : 4} #maybe increase epochs?\n",
        "config_debug = {\"dist_type\" : \"ints\" ##worked well\n",
        "                      , \"gen_m\" : vocab_size\n",
        "                      , \"gen_n\" : 5\n",
        "                      , \"gen_epochs\" : 2\n",
        "                      , \"gen_lr\" : 0.005\n",
        "                      , \"random_shuffle\" : 0.0\n",
        "                      , \"out_type\" : \"one-hot\"\n",
        "                      , \"dist_type\" : 'hetero'\n",
        "                      , \"alpha\" : 1\n",
        "                      , \"beta\" : 4} #maybe increase epochs?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "0fb0ccfa",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0fb0ccfa",
        "outputId": "1543b9c6-1801-4f8f-8c46-02ace68401a5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Expected tensor for argument #1 'indices' to have one of the following scalar types: Long, Int; but got torch.FloatTensor instead (while checking arguments for embedding)\n",
            "lets try ints!\n"
          ]
        }
      ],
      "source": [
        "#teacher_toy.configure(**config_ab) #this is dying.  might be time for colab!!\n",
        "teacher_toy.load_state_dict('/content/drive/MyDrive/Research/good_toy_4.pth')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "teacher_toy.linearize_embedding()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pbjq_8WeTYqu",
        "outputId": "5262bb8a-3188-4ca1-8876-690bfe4a16b9"
      },
      "id": "Pbjq_8WeTYqu",
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Teacher Embedding Linearized!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "77f954d3",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "77f954d3",
        "outputId": "2e06795f-d910-45ce-ff24-f134c947817e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Generating val data :: 100%|██████████| 200/200 [00:08<00:00, 24.19it/s]\n"
          ]
        }
      ],
      "source": [
        "\n",
        "args = { 'val_train' : \"val\"\n",
        "                      , 'n' : 10_000\n",
        "                      , 'dist_type' : 'normal_clipped'\n",
        "                      , 'batch_size' : 50\n",
        "                      , 'store_outputs': True\n",
        "        }\n",
        "\n",
        "teacher_toy.generate_data(**args)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#teacher_toy.val_targets[0:10] ##logits!!\n",
        "#teacher_toy.val_inputs[0:10] #one-hots!!"
      ],
      "metadata": {
        "id": "SwXUOExMguyR"
      },
      "id": "SwXUOExMguyR",
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "e221c7c0",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "e221c7c0",
        "outputId": "959d176a-8980-4d4d-96a1-ae364f356df4"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "' Training loop\\nfor step in range(total_steps):\\n    optimizer.zero_grad()\\n    # Compute your loss and backpropagation here\\n    loss.backward()\\n    optimizer.step()\\n    scheduler.step()\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 19
        }
      ],
      "source": [
        "from torch.optim.lr_scheduler import LambdaLR\n",
        "import math\n",
        "\n",
        "warmup_steps = 10_000  # Adjust this as needed steps is total items passed through\n",
        "total_steps = 20_000  # Adjust this as needed i'd like to calculate this....\n",
        "\n",
        "\n",
        "# Define a learning rate scheduler with warmup\n",
        "def lr_lambda(current_step):\n",
        "    if current_step < warmup_steps:\n",
        "        # During warmup, increase learning rate linearly\n",
        "        return float(current_step) / float(max(1, warmup_steps))\n",
        "    else:\n",
        "        # After warmup, decrease learning rate using some schedule\n",
        "        # You can use any LR schedule you prefer here\n",
        "        # For example, you can use a learning rate schedule like CosineAnnealing\n",
        "        return 0.5 * (1 + math.cos(math.pi * (current_step - warmup_steps) / (total_steps - warmup_steps)))\n",
        "\n",
        "\n",
        "\n",
        "\"\"\" Training loop\n",
        "for step in range(total_steps):\n",
        "    optimizer.zero_grad()\n",
        "    # Compute your loss and backpropagation here\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    scheduler.step()\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.optim import Optimizer\n",
        "\n",
        "class CosineAnnealingLRWithPeriod(Optimizer):\n",
        "    def __init__(self, optimizer, high_lr, low_lr, total_epochs, period_epochs):\n",
        "        defaults = dict(lr=high_lr)\n",
        "        params = optimizer.param_groups\n",
        "        self.optimizer = optimizer\n",
        "        self.high_lr = high_lr\n",
        "        self.low_lr = low_lr\n",
        "        self.total_epochs = total_epochs\n",
        "        self.period_epochs = period_epochs\n",
        "        self.current_epoch = 0\n",
        "\n",
        "        self.lr_schedule = []\n",
        "\n",
        "        super(CosineAnnealingLRWithPeriod, self).__init__(params,defaults)\n",
        "\n",
        "    def step(self, epoch=None):\n",
        "        if epoch is None:\n",
        "            epoch = self.current_epoch\n",
        "        else:\n",
        "            self.current_epoch = epoch\n",
        "\n",
        "        if self.current_epoch <= self.total_epochs:\n",
        "            self.current_epoch += 1\n",
        "            lr = self.low_lr + 0.5 * (self.high_lr - self.low_lr) * (\n",
        "                1 + math.cos((self.current_epoch / self.period_epochs) * math.pi)\n",
        "            )\n",
        "            for param_group in self.param_groups:\n",
        "                param_group['lr'] = lr\n",
        "            self.lr_schedule.append(lr)\n",
        "            self.optimizer.step()\n",
        "        else:\n",
        "          print(self.current_epoch, epoch)\n",
        "          raise ValueError('Epoch out of range.')\n",
        "\n",
        "# optimizer = torch.optim.SGD(model.parameters(), lr=0.1)\n",
        "# scheduler = CosineAnnealingLRWithPeriod(optimizer, high_lr=0.1, low_lr=0.01, total_epochs=100, period_epochs=20)\n",
        "\n",
        "# During training loop:\n",
        "# for epoch in range(num_epochs):\n",
        "#     scheduler.step(epoch)\n",
        "#     train_one_epoch()"
      ],
      "metadata": {
        "id": "fwInBANTeMyt"
      },
      "id": "fwInBANTeMyt",
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class UniformRandomLRScheduler:\n",
        "    def __init__(self, optimizer, low_lr, high_lr):\n",
        "        self.optimizer = optimizer\n",
        "        self.low_lr = low_lr\n",
        "        self.high_lr = high_lr\n",
        "\n",
        "\n",
        "    def step(self):\n",
        "\n",
        "\n",
        "        new_lr = np.random.uniform(self.low_lr, self.high_lr)\n",
        "        for param_group in self.optimizer.param_groups:\n",
        "            param_group['lr'] = new_lr\n",
        "        #print(f\"Epoch {epoch + 1}: Learning Rate = {new_lr}\")\n",
        "\n",
        "# Example usage:\n",
        "import torch.optim as optim\n",
        "\"\"\" This is what i used to get to 85%.\n",
        "low_lr = 0.00001\n",
        "high_lr = 0.01\n",
        "\"\"\"\n",
        "low_lr = 0.000001\n",
        "high_lr = 0.0001\n",
        "\n",
        "\n",
        "# Create the scheduler and pass in the optimizer\n",
        "###I want to have the bounds be able to decay.  i think the upper bound specifically.  have it decay to a set value, linearly? thats probs fine.\n"
      ],
      "metadata": {
        "id": "vNv-_u1sFMGU"
      },
      "id": "vNv-_u1sFMGU",
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class NormalRandomLRScheduler:\n",
        "    def __init__(self, optimizer, mean_lr, std_lr, low_lr, high_lr):\n",
        "        self.optimizer = optimizer\n",
        "        self.mean_lr = mean_lr\n",
        "        self.std_lr = std_lr\n",
        "        self.low_lr = low_lr\n",
        "        self.high_lr = high_lr\n",
        "\n",
        "    def step(self, epoch):\n",
        "        new_lr = max(min(np.random.normal(self.mean_lr, self.std_lr), self.high_lr), self.low_lr)\n",
        "        for param_group in self.optimizer.param_groups:\n",
        "            param_group['lr'] = new_lr\n",
        "        print(f\"Epoch {epoch + 1}: Learning Rate = {new_lr:.6f}\")\n",
        "\n",
        "# Example usage:\n",
        "import torch.optim as optim\n",
        "\n",
        "mean_lr = 0.005\n",
        "std_lr = 0.001\n",
        "low_lr = 0.001\n",
        "high_lr = 0.01\n",
        "\n",
        "# Create your optimizer\n",
        "#optimizer = optim.SGD(model.parameters(), lr=mean_lr)\n",
        "\n",
        "# Create the scheduler and pass in the optimizer and other parameters\n",
        "#scheduler = NormalRandomLRScheduler(optimizer, mean_lr, std_lr, low_lr, high_lr)\n"
      ],
      "metadata": {
        "id": "zA8jwX9wFaCY"
      },
      "id": "zA8jwX9wFaCY",
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import copy\n",
        "import torch.nn.init as init"
      ],
      "metadata": {
        "id": "dZkFEI1QUckg"
      },
      "id": "dZkFEI1QUckg",
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "id": "b44d8243",
      "metadata": {
        "id": "b44d8243"
      },
      "outputs": [],
      "source": [
        "if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda\")\n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n",
        "student = copy.deepcopy(teacher_toy.model)\n",
        "\n",
        "for layer in student.modules():\n",
        "    if isinstance(layer, nn.Linear):\n",
        "        init.xavier_uniform_(layer.weight)  # You can use different initialization methods if desired\n",
        "        if layer.bias is not None:\n",
        "          init.zeros_(layer.bias)\n",
        "#load_path =  \"/content/drive/MyDrive/KD/toymodel.pth\"\n",
        "#load_path =  \"/content/drive/MyDrive/KD/toymodel_1.pth\" #fine tuned with lr 0.001, loss is still decreasing...86.83 val acc\n",
        "#student.load_state_dict(torch.load(load_path))\n",
        "#student = student.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "19e56b2b",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "19e56b2b",
        "outputId": "8124b640-0f28-4040-f38a-e682c0d3f268"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/10], Loss: 75783.08829879761, Train Accuracy: 0.6840\n",
            "LRs used: 0.00100000, 0.00177766, 0.00269001, 0.00979551, 0.00873374, 0.00526782, 0.00576070, 0.00375225, 0.00548270, 0.00416836, 0.00395719, 0.00732165, 0.00227913, 0.00439654, 0.00773749, 0.00993419, 0.00582806, 0.00142963, 0.00455607, 0.00686450, 0.00724673, 0.00407176, 0.00834669, 0.00387624, 0.00514292, 0.00849533, 0.00562158, 0.00857256, 0.00564680, 0.00495111, 0.00860492, 0.00878360, 0.00501876, 0.00363042, 0.00822828, 0.00810781, 0.00734728, 0.00816577, 0.00160856, 0.00792911, 0.00341421, 0.00893016, 0.00440328, 0.00663410, 0.00972333, 0.00536467, 0.00132673, 0.00312020, 0.00458858, 0.00404320, 0.00280850, 0.00299593, 0.00453607, 0.00817054, 0.00392018, 0.00957808, 0.00293959, 0.00502374, 0.00452451, 0.00850023, 0.00651132, 0.00891118, 0.00937993, 0.00997166, 0.00973178, 0.00101050, 0.00924280, 0.00324779, 0.00284437, 0.00200305, 0.00213162, 0.00209423, 0.00856006, 0.00439372, 0.00957742, 0.00245421, 0.00884338, 0.00195579, 0.00997040, 0.00439774, 0.00600007, 0.00990732, 0.00105217, 0.00517889, 0.00578008, 0.00593178, 0.00760186, 0.00367845, 0.00615925, 0.00991945, 0.00789869, 0.00662935, 0.00380819, 0.00929437, 0.00275396, 0.00108249, 0.00192465, 0.00422947, 0.00268745, 0.00286361, 0.00397312, 0.00146459, 0.00711711, 0.00469795, 0.00346901, 0.00579633, 0.00937432, 0.00394229, 0.00350317, 0.00904599, 0.00585452, 0.00900318, 0.00612251, 0.00529655, 0.00794799, 0.00920534, 0.00657295, 0.00666830, 0.00992122, 0.00718838, 0.00516745, 0.00215459, 0.00813121, 0.00655966, 0.00249538, 0.00192912, 0.00290548, 0.00912765, 0.00765534, 0.00232336, 0.00747557, 0.00896223, 0.00728679, 0.00550469, 0.00612626, 0.00294913, 0.00744422, 0.00823485, 0.00367051, 0.00600618, 0.00675418, 0.00904749, 0.00735800, 0.00410558, 0.00521618, 0.00890832, 0.00116890, 0.00124439, 0.00483800, 0.00190837, 0.00891447, 0.00335356, 0.00671552, 0.00465831, 0.00167102, 0.00841491, 0.00550124, 0.00264500, 0.00187969, 0.00646658, 0.00982679, 0.00250886, 0.00153531, 0.00716366, 0.00635183, 0.00496308, 0.00771330, 0.00105601, 0.00915985, 0.00528132, 0.00782396, 0.00408033, 0.00400845, 0.00384374, 0.00806261, 0.00473242, 0.00567635, 0.00516144, 0.00514073, 0.00555619, 0.00235809, 0.00328453, 0.00705966, 0.00162357, 0.00550828, 0.00949131, 0.00221438, 0.00565568, 0.00606455, 0.00895637, 0.00336445, 0.00460654, 0.00694627, 0.00642394, 0.00697253, 0.00191383, 0.00768739, 0.00229310, 0.00385518, 0.00173844, 0.00112749, 0.00547207, 0.00442518, 0.00362198, 0.00601838, 0.00666530, 0.00169552, 0.00583881, 0.00249426, 0.00960035, 0.00890973, 0.00729937, 0.00526380, 0.00156233, 0.00783579, 0.00231005, 0.00601270, 0.00646937, 0.00117785, 0.00423568, 0.00422053, 0.00528261, 0.00810366, 0.00875254, 0.00358764, 0.00938613, 0.00186618, 0.00263355, 0.00274760, 0.00926254, 0.00296282, 0.00781385, 0.00585754, 0.00839571, 0.00674285, 0.00630915, 0.00255581, 0.00878261, 0.00707484, 0.00688086, 0.00580103, 0.00226489, 0.00713414, 0.00586083, 0.00566144, 0.00938147, 0.00626159, 0.00605523, 0.00566459, 0.00560827, 0.00594210, 0.00812465, 0.00886634, 0.00437618, 0.00931593, 0.00384214, 0.00967198, 0.00270813, 0.00868484, 0.00885831, 0.00804978, 0.00625491, 0.00319843, 0.00773157, 0.00341815, 0.00689039, 0.00996983, 0.00142642, 0.00637449, 0.00462432, 0.00655200, 0.00347384, 0.00858742, 0.00581869, 0.00910237, 0.00637958, 0.00213798, 0.00688524, 0.00108917, 0.00948057, 0.00260312, 0.00701669, 0.00800297, 0.00668931, 0.00749393, 0.00956259, 0.00457419, 0.00594222, 0.00808434, 0.00290902, 0.00960249, 0.00259238, 0.00901988, 0.00528099, 0.00599470, 0.00433548, 0.00886351, 0.00747998, 0.00826691, 0.00714522, 0.00585202, 0.00853645, 0.00176488, 0.00468682, 0.00154894, 0.00175725, 0.00375098, 0.00998632, 0.00499294, 0.00252714, 0.00796056, 0.00535815, 0.00927325, 0.00980509, 0.00918726, 0.00233601, 0.00279699, 0.00193402, 0.00811924, 0.00763031, 0.00841627, 0.00981453, 0.00709758, 0.00517694, 0.00644264, 0.00196660, 0.00575411, 0.00730127, 0.00154848, 0.00624230, 0.00304523, 0.00777331, 0.00184019, 0.00250638, 0.00206518, 0.00104000, 0.00759134, 0.00174301, 0.00988995, 0.00982163, 0.00338177, 0.00707379, 0.00717939, 0.00359258, 0.00777490, 0.00730617, 0.00393993, 0.00752575, 0.00877082, 0.00953424, 0.00154340, 0.00644118, 0.00382934, 0.00640486, 0.00744781, 0.00165449, 0.00100448, 0.00652593, 0.00905878, 0.00832284, 0.00133486, 0.00197144, 0.00219904, 0.00414132, 0.00476607, 0.00956272, 0.00840794, 0.00655175, 0.00739636, 0.00188533, 0.00219539, 0.00611209, 0.00306341, 0.00924643, 0.00914087, 0.00705930, 0.00925089, 0.00155806, 0.00946747, 0.00219403, 0.00648682, 0.00821038, 0.00284641, 0.00246144, 0.00890079, 0.00293924, 0.00545627, 0.00687124, 0.00726947, 0.00993964, 0.00203396, 0.00357397, 0.00254034, 0.00619685, 0.00850006, 0.00953829, 0.00392100, 0.00914296, 0.00798085, 0.00610216, 0.00546872, 0.00697086, 0.00657328, 0.00464967, 0.00545390, 0.00936894, 0.00505420, 0.00311153, 0.00145046, 0.00559748, 0.00428196, 0.00976872, 0.00590535, 0.00572339, 0.00314383, 0.00940204, 0.00905444, 0.00675359, 0.00658567, 0.00560443, 0.00432006, 0.00302619, 0.00251514, 0.00265027, 0.00473776, 0.00178296, 0.00632253, 0.00953356, 0.00116662, 0.00922667, 0.00928996, 0.00563683, 0.00827520, 0.00791673, 0.00364009, 0.00465293, 0.00347325, 0.00729443, 0.00825512, 0.00922920, 0.00298358, 0.00421364, 0.00308471, 0.00489648, 0.00270383, 0.00777507, 0.00481005, 0.00897138, 0.00130032, 0.00282875, 0.00487762, 0.00660724, 0.00772006, 0.00721376, 0.00211699, 0.00202024, 0.00657568, 0.00526988, 0.00956989, 0.00482313, 0.00978394, 0.00828881, 0.00337220, 0.00218464, 0.00446239, 0.00132513, 0.00845289, 0.00769923, 0.00845082, 0.00499890, 0.00378925, 0.00672944, 0.00138231, 0.00835302, 0.00736596, 0.00552885, 0.00295488, 0.00196368, 0.00720030, 0.00726871, 0.00454105, 0.00158525, 0.00966240, 0.00694074, 0.00891298, 0.00677637, 0.00534153, 0.00611239, 0.00133168, 0.00775728, 0.00425699, 0.00742179, 0.00255718, 0.00406860, 0.00309429, 0.00713188, 0.00901660, 0.00632443, 0.00497865, 0.00308682, 0.00879573, 0.00808607, 0.00496790, 0.00868686, 0.00418527, 0.00236449, 0.00342903, 0.00846729, 0.00401086, 0.00764515, 0.00270920, 0.00391582, 0.00137347, 0.00833434, 0.00643991, 0.00347525, 0.00766853, 0.00498034, 0.00599836, 0.00840463, 0.00770828, 0.00871255, 0.00521004, 0.00387346, 0.00868664, 0.00139924, 0.00718357, 0.00890439, 0.00541891, 0.00516170, 0.00321251, 0.00432607, 0.00549407, 0.00415689, 0.00557977, 0.00584586, 0.00762052, 0.00398746, 0.00660055, 0.00527325, 0.00945324, 0.00292866, 0.00122162, 0.00609482, 0.00354834, 0.00905886, 0.00391270, 0.00219551, 0.00168052, 0.00960604, 0.00772427, 0.00678466, 0.00443168, 0.00379984, 0.00651310, 0.00267375, 0.00187855, 0.00315806, 0.00813791, 0.00370856, 0.00657421, 0.00505314, 0.00596286, 0.00186705, 0.00829275, 0.00518536, 0.00435456, 0.00789031, 0.00179298, 0.00808237, 0.00883710, 0.00598797, 0.00327600, 0.00891254, 0.00853475, 0.00307904, 0.00141297, 0.00697121, 0.00287036, 0.00505934, 0.00206600, 0.00229538, 0.00416133, 0.00414234, 0.00842633, 0.00300432, 0.00460578, 0.00929855, 0.00823410, 0.00716293, 0.00321563, 0.00476938, 0.00831849, 0.00489129, 0.00300172, 0.00414881, 0.00300288, 0.00166344, 0.00816647, 0.00147675, 0.00846443, 0.00415551, 0.00400145, 0.00245700, 0.00612390, 0.00479640, 0.00270017, 0.00922966, 0.00876025, 0.00796259, 0.00997067, 0.00580648, 0.00765074, 0.00400155, 0.00460722, 0.00603290, 0.00466951, 0.00948058, 0.00716233, 0.00783139, 0.00737848, 0.00636168, 0.00292380, 0.00351811, 0.00811440, 0.00247719, 0.00659887, 0.00268444, 0.00554963, 0.00907021, 0.00428670, 0.00476201, 0.00752750, 0.00189391, 0.00336418, 0.00989387, 0.00162763, 0.00344742, 0.00723028, 0.00365803, 0.00760347, 0.00205079, 0.00706969, 0.00415447, 0.00125611, 0.00707909, 0.00993443, 0.00804645, 0.00276586, 0.00841309, 0.00897429, 0.00111297, 0.00172314, 0.00976327, 0.00305755, 0.00686138, 0.00263403, 0.00306782, 0.00153422, 0.00571390, 0.00791670, 0.00212073, 0.00620842, 0.00279285, 0.00734125, 0.00598431, 0.00189967, 0.00947068, 0.00828798, 0.00419069, 0.00257734, 0.00978680, 0.00580999, 0.00375801, 0.00404027, 0.00102321, 0.00359999, 0.00773716, 0.00397040, 0.00235697, 0.00667816, 0.00773589, 0.00352878, 0.00262924, 0.00645759, 0.00158494, 0.00921458, 0.00847782, 0.00648783, 0.00820955, 0.00459031, 0.00927060, 0.00269794, 0.00304877, 0.00128544, 0.00902978, 0.00562319, 0.00931452, 0.00495042, 0.00934414, 0.00724127, 0.00647133, 0.00449515, 0.00835315, 0.00898362, 0.00405072, 0.00644654, 0.00542048, 0.00665361, 0.00663806, 0.00672038, 0.00565545, 0.00821720, 0.00286031, 0.00194937, 0.00508299, 0.00901723, 0.00161847, 0.00174679, 0.00398234, 0.00443759, 0.00837532, 0.00594283, 0.00727030, 0.00200448, 0.00831172, 0.00849163, 0.00622950, 0.00757157, 0.00321639, 0.00659823, 0.00112320, 0.00878758, 0.00967272, 0.00166889, 0.00298456, 0.00111002, 0.00483001, 0.00737023, 0.00157036, 0.00605739, 0.00473889, 0.00645235, 0.00295065, 0.00421607, 0.00704100, 0.00638624, 0.00685852, 0.00759332, 0.00796262, 0.00718515, 0.00102261, 0.00875971, 0.00559591, 0.00829704, 0.00927201, 0.00684429, 0.00930550, 0.00164868, 0.00137318, 0.00894703, 0.00724688, 0.00188142, 0.00194015, 0.00811762, 0.00621114, 0.00108127, 0.00792832, 0.00653893, 0.00338711, 0.00480283, 0.00377938, 0.00239154, 0.00376067, 0.00152658, 0.00798811, 0.00687517, 0.00106125, 0.00539001, 0.00282415, 0.00276563, 0.00424410, 0.00928943, 0.00787728, 0.00113812, 0.00783817, 0.00970459, 0.00378839, 0.00160700, 0.00773245, 0.00106650, 0.00175281, 0.00720477, 0.00150939, 0.00827266, 0.00105958, 0.00700745, 0.00107614, 0.00642291, 0.00710472, 0.00307352, 0.00323320, 0.00587851, 0.00180519, 0.00104109, 0.00603774, 0.00112093, 0.00862336, 0.00830928, 0.00806828, 0.00712985, 0.00476309, 0.00669848, 0.00356011, 0.00111697, 0.00239348, 0.00412513, 0.00766967, 0.00601726, 0.00465249, 0.00419864, 0.00213433, 0.00485652, 0.00123194, 0.00426944, 0.00913030, 0.00131183, 0.00815871, 0.00369741, 0.00137592, 0.00250198, 0.00403359, 0.00191044, 0.00844260, 0.00601170, 0.00489682, 0.00635579, 0.00718371, 0.00720138, 0.00126614, 0.00698606, 0.00959535, 0.00545209, 0.00420753, 0.00807535, 0.00715483, 0.00533806, 0.00174017, 0.00929364, 0.00738966, 0.00420668, 0.00175980, 0.00848271, 0.00949522, 0.00790663, 0.00698609, 0.00328148, 0.00444125, 0.00465971, 0.00585502, 0.00854921, 0.00280822, 0.00142029, 0.00229290, 0.00753577, 0.00714835, 0.00964861, 0.00155048, 0.00471312, 0.00414896, 0.00791929, 0.00793790, 0.00494036, 0.00805425, 0.00120838, 0.00112275, 0.00190957, 0.00404408, 0.00439610, 0.00769642, 0.00287973, 0.00708600, 0.00815844, 0.00486791, 0.00782180, 0.00579018, 0.00621261, 0.00170783, 0.00525636, 0.00937893, 0.00566964, 0.00859432, 0.00410164, 0.00881487, 0.00880534, 0.00446761, 0.00688410, 0.00937075, 0.00182026, 0.00960278, 0.00854151, 0.00682303, 0.00240197, 0.00579182, 0.00513763, 0.00957180, 0.00835377, 0.00424771, 0.00290117, 0.00951723, 0.00267429, 0.00662737, 0.00918855, 0.00531983, 0.00421030, 0.00224554, 0.00407453, 0.00128814, 0.00236883, 0.00367585, 0.00267724, 0.00655993, 0.00690815, 0.00274650, 0.00561003, 0.00481440, 0.00748969, 0.00292801, 0.00579454, 0.00786711, 0.00689362, 0.00900453, 0.00617746, 0.00779906, 0.00485367, 0.00320108, 0.00638804, 0.00336492, 0.00101733, 0.00945689, 0.00574980, 0.00606586, 0.00897144, 0.00471564, 0.00246604, 0.00151943, 0.00974357, 0.00859779, 0.00545159, 0.00655674, 0.00236780, 0.00863473, 0.00519829, 0.00223728, 0.00856379, 0.00480470, 0.00759805, 0.00315418, 0.00893167, 0.00254537, 0.00793109, 0.00450759, 0.00903045, 0.00771315, 0.00445803, 0.00377965, 0.00946020, 0.00872877, 0.00802309, 0.00231196, 0.00659427, 0.00831926, 0.00852907, 0.00128720, 0.00709309, 0.00796746, 0.00553690, 0.00925490, 0.00483774, 0.00846854, 0.00413438, 0.00958316, 0.00168515, 0.00889573, 0.00221013, 0.00287832, 0.00141424, 0.00685623, 0.00315102, 0.00331395, 0.00422373, 0.00124066, 0.00438501, 0.00634250, 0.00535429, 0.00876948, 0.00768048, 0.00740842, 0.00387006, 0.00789636\n",
            "Epoch [2/10], Loss: 63587.38940048218, Train Accuracy: 0.6925\n",
            "LRs used: 0.00537105, 0.00129060, 0.00787388, 0.00555718, 0.00900436, 0.00730999, 0.00636865, 0.00652397, 0.00620324, 0.00280173, 0.00426299, 0.00143546, 0.00287330, 0.00914429, 0.00891463, 0.00534289, 0.00592846, 0.00886711, 0.00269403, 0.00180276, 0.00622777, 0.00501726, 0.00202196, 0.00104781, 0.00414359, 0.00752476, 0.00694718, 0.00820257, 0.00868612, 0.00397171, 0.00895372, 0.00600705, 0.00253763, 0.00675263, 0.00873057, 0.00712908, 0.00495996, 0.00344557, 0.00865213, 0.00577169, 0.00481888, 0.00863267, 0.00274050, 0.00283296, 0.00645665, 0.00718340, 0.00145814, 0.00110882, 0.00778026, 0.00572102, 0.00128251, 0.00314275, 0.00662562, 0.00685621, 0.00799188, 0.00223978, 0.00451770, 0.00505567, 0.00681726, 0.00185348, 0.00415658, 0.00255446, 0.00116324, 0.00746697, 0.00351034, 0.00231391, 0.00743997, 0.00930363, 0.00584393, 0.00107893, 0.00310985, 0.00489285, 0.00342207, 0.00690718, 0.00221873, 0.00442830, 0.00321865, 0.00248723, 0.00120631, 0.00963246, 0.00181279, 0.00228933, 0.00717254, 0.00631157, 0.00903707, 0.00337679, 0.00225553, 0.00264430, 0.00912387, 0.00302396, 0.00740755, 0.00801311, 0.00865286, 0.00779642, 0.00304318, 0.00377508, 0.00451089, 0.00848386, 0.00192373, 0.00471287, 0.00417992, 0.00712436, 0.00730017, 0.00438306, 0.00469490, 0.00635877, 0.00425591, 0.00884319, 0.00574850, 0.00617724, 0.00935743, 0.00460788, 0.00115738, 0.00668017, 0.00475551, 0.00201275, 0.00425057, 0.00552224, 0.00986500, 0.00400995, 0.00641215, 0.00703361, 0.00617206, 0.00536164, 0.00202759, 0.00859728, 0.00245019, 0.00684752, 0.00250724, 0.00644350, 0.00943412, 0.00394040, 0.00190238, 0.00323441, 0.00489919, 0.00732942, 0.00234416, 0.00330469, 0.00759207, 0.00181268, 0.00632497, 0.00605881, 0.00129098, 0.00133359, 0.00975649, 0.00343258, 0.00667224, 0.00201741, 0.00596831, 0.00433883, 0.00815152, 0.00177572, 0.00492864, 0.00808396, 0.00590498, 0.00670066, 0.00956841, 0.00928711, 0.00601793, 0.00280554, 0.00420891, 0.00539982, 0.00235333, 0.00500327, 0.00649383, 0.00943238, 0.00909021, 0.00396664, 0.00413282, 0.00929499, 0.00231579, 0.00532024, 0.00757144, 0.00325478, 0.00342208, 0.00754520, 0.00370755, 0.00872037, 0.00678616, 0.00770504, 0.00826178, 0.00634123, 0.00409490, 0.00593132, 0.00992453, 0.00473268, 0.00947841, 0.00521856, 0.00360089, 0.00409581, 0.00287907, 0.00644122, 0.00562882, 0.00566207, 0.00774434, 0.00235475, 0.00960527, 0.00346425, 0.00457503, 0.00477745, 0.00992622, 0.00798632, 0.00935738, 0.00701673, 0.00765035, 0.00609596, 0.00884546, 0.00934246, 0.00283035, 0.00750206, 0.00908558, 0.00229907, 0.00907567, 0.00933783, 0.00834389, 0.00932736, 0.00995688, 0.00480380, 0.00791103, 0.00321993, 0.00583813, 0.00838050, 0.00570026, 0.00677688, 0.00184385, 0.00793590, 0.00743811, 0.00574103, 0.00321815, 0.00205562, 0.00706758, 0.00379741, 0.00235633, 0.00890254, 0.00260467, 0.00699135, 0.00179426, 0.00707451, 0.00464928, 0.00471191, 0.00898749, 0.00376557, 0.00157745, 0.00397467, 0.00854282, 0.00109196, 0.00160976, 0.00790585, 0.00775421, 0.00359114, 0.00460557, 0.00439377, 0.00638682, 0.00977393, 0.00387023, 0.00904017, 0.00683068, 0.00724231, 0.00470113, 0.00231781, 0.00806392, 0.00955976, 0.00781971, 0.00806509, 0.00291182, 0.00486288, 0.00853783, 0.00758344, 0.00123903, 0.00747806, 0.00714349, 0.00104767, 0.00423007, 0.00406108, 0.00941000, 0.00520775, 0.00582560, 0.00529080, 0.00831327, 0.00725324, 0.00644471, 0.00298271, 0.00276818, 0.00746064, 0.00712116, 0.00123258, 0.00507472, 0.00766102, 0.00697649, 0.00347468, 0.00990218, 0.00440774, 0.00899729, 0.00211866, 0.00748007, 0.00189376, 0.00858839, 0.00972879, 0.00700996, 0.00771385, 0.00124335, 0.00536102, 0.00961616, 0.00451996, 0.00678304, 0.00106764, 0.00337945, 0.00129779, 0.00610337, 0.00132380, 0.00285055, 0.00820987, 0.00156256, 0.00531559, 0.00794982, 0.00530431, 0.00946309, 0.00315789, 0.00399148, 0.00710704, 0.00690002, 0.00836039, 0.00651719, 0.00508099, 0.00592838, 0.00472152, 0.00993052, 0.00400263, 0.00210627, 0.00171991, 0.00827590, 0.00671125, 0.00314235, 0.00594593, 0.00529346, 0.00975940, 0.00775815, 0.00133558, 0.00783185, 0.00789380, 0.00751357, 0.00516404, 0.00234824, 0.00177788, 0.00770524, 0.00694446, 0.00938944, 0.00900029, 0.00730147, 0.00342821, 0.00679884, 0.00655892, 0.00451511, 0.00396436, 0.00586656, 0.00149634, 0.00639892, 0.00145512, 0.00472336, 0.00712664, 0.00160995, 0.00356963, 0.00222280, 0.00707700, 0.00184924, 0.00475875, 0.00108662, 0.00701004, 0.00559392, 0.00726573, 0.00696282, 0.00116649, 0.00465529, 0.00385386, 0.00832748, 0.00319805, 0.00682364, 0.00758538, 0.00925526, 0.00411957, 0.00897459, 0.00999177, 0.00787369, 0.00278154, 0.00399656, 0.00302569, 0.00733220, 0.00907157, 0.00121548, 0.00401403, 0.00872799, 0.00341075, 0.00449139, 0.00329952, 0.00448848, 0.00575529, 0.00720709, 0.00996348, 0.00110824, 0.00237509, 0.00548888, 0.00646030, 0.00700941, 0.00271491, 0.00962777, 0.00476863, 0.00492050, 0.00717838, 0.00721362, 0.00255000, 0.00872973, 0.00539934, 0.00237439, 0.00117337, 0.00314547, 0.00605670, 0.00426789, 0.00744058, 0.00963352, 0.00458519, 0.00175749, 0.00451099, 0.00424960, 0.00541087, 0.00216815, 0.00688221, 0.00643673, 0.00970455, 0.00119168, 0.00466808, 0.00657991, 0.00884876, 0.00412742, 0.00752617, 0.00319433, 0.00452241, 0.00590006, 0.00802611, 0.00562493, 0.00607843, 0.00788654, 0.00905986, 0.00262548, 0.00153805, 0.00816100, 0.00943619, 0.00940251, 0.00652030, 0.00898019, 0.00896360, 0.00449352, 0.00116773, 0.00850097, 0.00497679, 0.00599108, 0.00766128, 0.00710456, 0.00340008, 0.00651061, 0.00238858, 0.00680475, 0.00521314, 0.00930156, 0.00453259, 0.00634121, 0.00171744, 0.00282322, 0.00118219, 0.00606716, 0.00898208, 0.00678618, 0.00778551, 0.00485645, 0.00354484, 0.00692164, 0.00259717, 0.00354867, 0.00609306, 0.00364399, 0.00353896, 0.00571384, 0.00402404, 0.00485458, 0.00866622, 0.00180815, 0.00127359, 0.00334029, 0.00730998, 0.00412818, 0.00715530, 0.00755569, 0.00548915, 0.00866909, 0.00254382, 0.00149902, 0.00755364, 0.00122092, 0.00349091, 0.00479216, 0.00105824, 0.00574693, 0.00555548, 0.00308447, 0.00246648, 0.00524462, 0.00959547, 0.00931437, 0.00715832, 0.00700085, 0.00425279, 0.00144452, 0.00857958, 0.00274801, 0.00474185, 0.00342266, 0.00748338, 0.00845969, 0.00759920, 0.00583062, 0.00132479, 0.00542996, 0.00602751, 0.00978418, 0.00821321, 0.00463771, 0.00243051, 0.00328276, 0.00836916, 0.00475606, 0.00472052, 0.00910298, 0.00891375, 0.00570514, 0.00698062, 0.00101625, 0.00424517, 0.00186985, 0.00648341, 0.00872851, 0.00663715, 0.00389223, 0.00342349, 0.00336740, 0.00990943, 0.00801639, 0.00811161, 0.00679605, 0.00192268, 0.00164974, 0.00703283, 0.00794177, 0.00306454, 0.00567254, 0.00925410, 0.00885231, 0.00893479, 0.00879391, 0.00378139, 0.00899245, 0.00292676, 0.00594693, 0.00518350, 0.00498314, 0.00215056, 0.00960121, 0.00418901, 0.00516128, 0.00820242, 0.00975753, 0.00163151, 0.00180644, 0.00278877, 0.00938486, 0.00348291, 0.00795426, 0.00814025, 0.00489081, 0.00311872, 0.00504066, 0.00983436, 0.00691509, 0.00736936, 0.00367151, 0.00124694, 0.00740011, 0.00936801, 0.00204318, 0.00108071, 0.00777446, 0.00677995, 0.00749652, 0.00926226, 0.00894531, 0.00526370, 0.00636968, 0.00930383, 0.00543112, 0.00818533, 0.00510674, 0.00555063, 0.00177549, 0.00524219, 0.00109250, 0.00238055, 0.00397552, 0.00316926, 0.00947500, 0.00469216, 0.00113556, 0.00677987, 0.00765216, 0.00553525, 0.00354305, 0.00107376, 0.00940448, 0.00903705, 0.00923968, 0.00292816, 0.00563650, 0.00186255, 0.00705285, 0.00665413, 0.00685495, 0.00137916, 0.00157051, 0.00204635, 0.00632618, 0.00118555, 0.00701181, 0.00346338, 0.00706622, 0.00776945, 0.00252949, 0.00761738, 0.00731833, 0.00178840, 0.00162002, 0.00446714, 0.00970643, 0.00520057, 0.00464756, 0.00539517, 0.00527320, 0.00607489, 0.00859565, 0.00897170, 0.00196744, 0.00646009, 0.00620222, 0.00405783, 0.00303419, 0.00859115, 0.00534173, 0.00970472, 0.00199076, 0.00997331, 0.00309339, 0.00685377, 0.00667332, 0.00268599, 0.00978455, 0.00926350, 0.00992597, 0.00999715, 0.00316170, 0.00982352, 0.00120165, 0.00649541, 0.00634170, 0.00860181, 0.00138148, 0.00394018, 0.00862325, 0.00832675, 0.00886293, 0.00657625, 0.00166559, 0.00961855, 0.00705444, 0.00449839, 0.00844031, 0.00468472, 0.00911576, 0.00551100, 0.00161793, 0.00751440, 0.00449796, 0.00179346, 0.00670880, 0.00950384, 0.00462934, 0.00276906, 0.00121209, 0.00565543, 0.00563543, 0.00753367, 0.00323494, 0.00554533, 0.00895845, 0.00673966, 0.00726308, 0.00400101, 0.00419210, 0.00544850, 0.00915077, 0.00431393, 0.00135658, 0.00648619, 0.00388335, 0.00161908, 0.00691341, 0.00258320, 0.00761668, 0.00632369, 0.00267000, 0.00632103, 0.00829701, 0.00637144, 0.00240589, 0.00167112, 0.00471659, 0.00992605, 0.00161597, 0.00689559, 0.00107632, 0.00464232, 0.00626345, 0.00360532, 0.00789280, 0.00598111, 0.00260699, 0.00957836, 0.00435019, 0.00932008, 0.00232258, 0.00206614, 0.00901606, 0.00486514, 0.00393835, 0.00570147, 0.00554013, 0.00351364, 0.00334614, 0.00130658, 0.00982781, 0.00924353, 0.00876420, 0.00592745, 0.00293997, 0.00915479, 0.00906955, 0.00832559, 0.00730047, 0.00467168, 0.00112120, 0.00255710, 0.00707525, 0.00706648, 0.00648773, 0.00213009, 0.00488878, 0.00820355, 0.00712982, 0.00998722, 0.00463839, 0.00957254, 0.00377473, 0.00288331, 0.00609288, 0.00666964, 0.00200147, 0.00763832, 0.00301855, 0.00463089, 0.00761701, 0.00852410, 0.00715967, 0.00343605, 0.00449854, 0.00248896, 0.00817712, 0.00505586, 0.00394313, 0.00992534, 0.00798091, 0.00699858, 0.00202827, 0.00547758, 0.00620130, 0.00412529, 0.00346501, 0.00647210, 0.00207375, 0.00928537, 0.00809070, 0.00945294, 0.00245949, 0.00439175, 0.00789757, 0.00161682, 0.00840812, 0.00471363, 0.00794144, 0.00625249, 0.00517040, 0.00592874, 0.00720143, 0.00934133, 0.00379420, 0.00605154, 0.00399807, 0.00973478, 0.00352939, 0.00954846, 0.00155134, 0.00705995, 0.00617607, 0.00957470, 0.00801099, 0.00987891, 0.00706027, 0.00997262, 0.00231038, 0.00518393, 0.00771353, 0.00889747, 0.00665884, 0.00116459, 0.00392440, 0.00928085, 0.00978625, 0.00318037, 0.00737833, 0.00733408, 0.00225117, 0.00650354, 0.00489868, 0.00528928, 0.00705174, 0.00397414, 0.00476139, 0.00611927, 0.00740138, 0.00124160, 0.00328377, 0.00250124, 0.00374501, 0.00963077, 0.00832735, 0.00635758, 0.00571117, 0.00419893, 0.00230259, 0.00588176, 0.00375231, 0.00818613, 0.00981549, 0.00363824, 0.00970622, 0.00275307, 0.00256532, 0.00678897, 0.00778645, 0.00731772, 0.00833894, 0.00603838, 0.00576538, 0.00987267, 0.00547143, 0.00886962, 0.00352417, 0.00287306, 0.00272163, 0.00504947, 0.00643671, 0.00255509, 0.00299466, 0.00762575, 0.00598302, 0.00517690, 0.00663652, 0.00230346, 0.00950046, 0.00148085, 0.00638920, 0.00910338, 0.00164152, 0.00292788, 0.00128945, 0.00452133, 0.00770747, 0.00457690, 0.00485250, 0.00420424, 0.00104579, 0.00849887, 0.00825601, 0.00773045, 0.00784028, 0.00813121, 0.00473939, 0.00748723, 0.00303479, 0.00556210, 0.00833061, 0.00886380, 0.00775162, 0.00689853, 0.00771421, 0.00783323, 0.00192642, 0.00991848, 0.00504399, 0.00472927, 0.00731662, 0.00369455, 0.00530176, 0.00909415, 0.00681276, 0.00551881, 0.00778557, 0.00133607, 0.00548532, 0.00599439, 0.00157616, 0.00535683, 0.00481737, 0.00644108, 0.00783067, 0.00140096, 0.00884570, 0.00746911, 0.00739873, 0.00932185, 0.00331087, 0.00452428, 0.00145526, 0.00350054, 0.00652941, 0.00290497, 0.00839817, 0.00338785, 0.00923860, 0.00191094, 0.00889899, 0.00712866, 0.00978619, 0.00466508, 0.00612896, 0.00589109, 0.00361647, 0.00508818, 0.00477411, 0.00176094, 0.00608995, 0.00140315, 0.00457578, 0.00743352, 0.00632532, 0.00862088, 0.00154982, 0.00172836, 0.00858876, 0.00554792, 0.00538112, 0.00607696, 0.00842220, 0.00698017, 0.00501678, 0.00524050, 0.00599409, 0.00393232, 0.00379525, 0.00938669, 0.00845256, 0.00291242, 0.00525936, 0.00677517, 0.00664127, 0.00775201, 0.00466073, 0.00777901, 0.00688566, 0.00342118, 0.00754121, 0.00991660, 0.00922267, 0.00316054, 0.00581930, 0.00609450, 0.00239345, 0.00347689, 0.00938802, 0.00532636, 0.00915336, 0.00983891, 0.00760831, 0.00475342, 0.00563088, 0.00243191\n",
            "Epoch [3/10], Loss: 70589.95196151733, Train Accuracy: 0.6902\n",
            "LRs used: 0.00782392, 0.00879127, 0.00196478, 0.00752305, 0.00683149, 0.00609792, 0.00446797, 0.00867828, 0.00489429, 0.00997028, 0.00532409, 0.00465778, 0.00551542, 0.00839591, 0.00677755, 0.00451927, 0.00410101, 0.00820594, 0.00876847, 0.00127487, 0.00695029, 0.00338332, 0.00743985, 0.00712338, 0.00433451, 0.00921037, 0.00511559, 0.00977908, 0.00442596, 0.00488053, 0.00855190, 0.00231909, 0.00208474, 0.00842183, 0.00467129, 0.00510213, 0.00513072, 0.00958248, 0.00179195, 0.00323393, 0.00320627, 0.00177307, 0.00316940, 0.00364962, 0.00502180, 0.00570188, 0.00754767, 0.00103865, 0.00927234, 0.00608880, 0.00398624, 0.00544936, 0.00458661, 0.00717731, 0.00656547, 0.00634100, 0.00528436, 0.00433581, 0.00306006, 0.00897132, 0.00848143, 0.00916660, 0.00401771, 0.00742638, 0.00250304, 0.00409487, 0.00673893, 0.00902548, 0.00207207, 0.00658677, 0.00889905, 0.00929314, 0.00920768, 0.00416325, 0.00134332, 0.00905129, 0.00486091, 0.00251933, 0.00395928, 0.00337987, 0.00167738, 0.00439018, 0.00600004, 0.00446400, 0.00831277, 0.00771648, 0.00325931, 0.00518411, 0.00105826, 0.00700417, 0.00406329, 0.00147031, 0.00616505, 0.00467505, 0.00169415, 0.00714879, 0.00241516, 0.00764099, 0.00250016, 0.00449284, 0.00118643, 0.00175064, 0.00956380, 0.00751629, 0.00596817, 0.00712294, 0.00365179, 0.00104717, 0.00482897, 0.00872308, 0.00936293, 0.00319955, 0.00295512, 0.00656500, 0.00115870, 0.00441987, 0.00348502, 0.00461867, 0.00306275, 0.00810671, 0.00432125, 0.00887059, 0.00351419, 0.00726032, 0.00771272, 0.00787811, 0.00403626, 0.00257087, 0.00482255, 0.00675338, 0.00119752, 0.00920802, 0.00680634, 0.00724452, 0.00410678, 0.00132906, 0.00864829, 0.00187966, 0.00912085, 0.00296316, 0.00559902, 0.00972468, 0.00398335, 0.00585836, 0.00681284, 0.00371706, 0.00299400, 0.00866186, 0.00738787, 0.00340686, 0.00896932, 0.00855704, 0.00113188, 0.00965004, 0.00755578, 0.00477347, 0.00187818, 0.00808677, 0.00985627, 0.00282039, 0.00410731, 0.00340086, 0.00642436, 0.00578759, 0.00753584, 0.00960035, 0.00968650, 0.00725310, 0.00788653, 0.00286728, 0.00442928, 0.00153271, 0.00213900, 0.00776849, 0.00481893, 0.00112149, 0.00923483, 0.00697391, 0.00322511, 0.00886994, 0.00248988, 0.00463033, 0.00293106, 0.00510380, 0.00182872, 0.00700031, 0.00796492, 0.00816359, 0.00188472, 0.00838832, 0.00894601, 0.00818489, 0.00291372, 0.00870292, 0.00416565, 0.00608038, 0.00121289, 0.00546676, 0.00916351, 0.00823813, 0.00618364, 0.00534754, 0.00726667, 0.00393587, 0.00434971, 0.00236989, 0.00151858, 0.00780433, 0.00417869, 0.00438622, 0.00388651, 0.00149108, 0.00728481, 0.00341109, 0.00459704, 0.00434474, 0.00758933, 0.00817888, 0.00403453, 0.00812889, 0.00329518, 0.00337884, 0.00154456, 0.00989663, 0.00232043, 0.00657207, 0.00560670, 0.00279881, 0.00414269, 0.00416066, 0.00319875, 0.00875104, 0.00564808, 0.00730502, 0.00880769, 0.00516150, 0.00425036, 0.00448233, 0.00960543, 0.00209650, 0.00956993, 0.00108817, 0.00280561, 0.00377332, 0.00877069, 0.00501739, 0.00888213, 0.00329011, 0.00423896, 0.00927445, 0.00259414, 0.00160057, 0.00110595, 0.00584982, 0.00484102, 0.00767627, 0.00582305, 0.00599759, 0.00576196, 0.00306050, 0.00246903, 0.00284337, 0.00382568, 0.00251953, 0.00882960, 0.00363637, 0.00329574, 0.00288348, 0.00995040, 0.00950982, 0.00909090, 0.00522088, 0.00873730, 0.00777546, 0.00226704, 0.00556403, 0.00917824, 0.00161479, 0.00923934, 0.00118707, 0.00668871, 0.00772456, 0.00959220, 0.00322262, 0.00715324, 0.00306148, 0.00475710, 0.00703430, 0.00267104, 0.00115578, 0.00679757, 0.00976261, 0.00489664, 0.00224902, 0.00614054, 0.00677405, 0.00326146, 0.00568425, 0.00548326, 0.00642752, 0.00666102, 0.00185858, 0.00856955, 0.00240057, 0.00409677, 0.00880633, 0.00505312, 0.00944836, 0.00711605, 0.00375806, 0.00528533, 0.00966646, 0.00603200, 0.00432766, 0.00209613, 0.00964881, 0.00104871, 0.00591209, 0.00964705, 0.00775720, 0.00532148, 0.00514642, 0.00235825, 0.00549661, 0.00694043, 0.00475495, 0.00582391, 0.00788129, 0.00891892, 0.00319073, 0.00765427, 0.00972925, 0.00895825, 0.00423607, 0.00402939, 0.00703518, 0.00593617, 0.00803524, 0.00990903, 0.00477649, 0.00197844, 0.00934733, 0.00521836, 0.00549977, 0.00637032, 0.00196934, 0.00336581, 0.00430789, 0.00593069, 0.00377103, 0.00494243, 0.00662846, 0.00656920, 0.00999031, 0.00718258, 0.00300928, 0.00451278, 0.00112710, 0.00624200, 0.00561068, 0.00420733, 0.00549796, 0.00502118, 0.00691714, 0.00294251, 0.00877481, 0.00691218, 0.00497664, 0.00536919, 0.00737342, 0.00997632, 0.00895242, 0.00762402, 0.00175302, 0.00229012, 0.00425590, 0.00827683, 0.00240604, 0.00565621, 0.00999667, 0.00892025, 0.00218936, 0.00769092, 0.00441659, 0.00764872, 0.00145505, 0.00307932, 0.00309191, 0.00735729, 0.00494232, 0.00535633, 0.00456291, 0.00393021, 0.00558942, 0.00181398, 0.00794997, 0.00163953, 0.00799868, 0.00339961, 0.00595636, 0.00970405, 0.00547028, 0.00140776, 0.00909818, 0.00751506, 0.00584183, 0.00874772, 0.00328171, 0.00703839, 0.00903861, 0.00879225, 0.00378818, 0.00528315, 0.00117909, 0.00896893, 0.00149910, 0.00420671, 0.00267193, 0.00148011, 0.00977177, 0.00713533, 0.00805252, 0.00248504, 0.00864340, 0.00458578, 0.00972174, 0.00352204, 0.00746315, 0.00770092, 0.00439347, 0.00922961, 0.00221950, 0.00329491, 0.00541198, 0.00909044, 0.00509825, 0.00863402, 0.00275012, 0.00544750, 0.00401016, 0.00405311, 0.00780242, 0.00706834, 0.00503625, 0.00250633, 0.00348196, 0.00518595, 0.00863378, 0.00962681, 0.00781322, 0.00839931, 0.00890869, 0.00264438, 0.00753208, 0.00688062, 0.00107553, 0.00434371, 0.00605548, 0.00191741, 0.00773634, 0.00917291, 0.00509216, 0.00167612, 0.00851461, 0.00124992, 0.00485723, 0.00353800, 0.00230765, 0.00793326, 0.00342896, 0.00136932, 0.00400549, 0.00191081, 0.00830357, 0.00285047, 0.00532230, 0.00899082, 0.00358226, 0.00398865, 0.00605032, 0.00182953, 0.00548098, 0.00896951, 0.00740272, 0.00697286, 0.00811233, 0.00306289, 0.00777689, 0.00911858, 0.00820174, 0.00207644, 0.00462701, 0.00737837, 0.00259395, 0.00261756, 0.00536800, 0.00763181, 0.00648201, 0.00398416, 0.00534282, 0.00243909, 0.00784209, 0.00396553, 0.00359866, 0.00565405, 0.00665859, 0.00951390, 0.00269151, 0.00822627, 0.00128874, 0.00952444, 0.00618672, 0.00870961, 0.00903039, 0.00444379, 0.00298591, 0.00290826, 0.00704752, 0.00203138, 0.00800862, 0.00624281, 0.00858789, 0.00929661, 0.00132012, 0.00871951, 0.00936125, 0.00168942, 0.00421072, 0.00220131, 0.00479265, 0.00358608, 0.00204825, 0.00361382, 0.00596078, 0.00909739, 0.00420588, 0.00425276, 0.00660284, 0.00799908, 0.00231051, 0.00909320, 0.00420721, 0.00202687, 0.00104812, 0.00343002, 0.00723624, 0.00532187, 0.00999074, 0.00744540, 0.00753579, 0.00799049, 0.00305773, 0.00906045, 0.00302468, 0.00528730, 0.00211120, 0.00407348, 0.00169151, 0.00893307, 0.00364544, 0.00752517, 0.00102947, 0.00320138, 0.00592901, 0.00500011, 0.00283078, 0.00615119, 0.00813170, 0.00725091, 0.00249845, 0.00262552, 0.00931299, 0.00105400, 0.00139324, 0.00402936, 0.00143011, 0.00529026, 0.00749537, 0.00388171, 0.00604920, 0.00446500, 0.00346363, 0.00821483, 0.00588581, 0.00844030, 0.00901619, 0.00637940, 0.00622240, 0.00723381, 0.00602489, 0.00634434, 0.00541912, 0.00661886, 0.00393239, 0.00481386, 0.00806448, 0.00911228, 0.00184382, 0.00572087, 0.00518388, 0.00279986, 0.00966580, 0.00681643, 0.00793312, 0.00557734, 0.00401308, 0.00398401, 0.00100461, 0.00585295, 0.00819570, 0.00266197, 0.00944755, 0.00701621, 0.00949941, 0.00372366, 0.00765067, 0.00649453, 0.00750432, 0.00703573, 0.00245948, 0.00197213, 0.00626300, 0.00295655, 0.00465129, 0.00714093, 0.00176852, 0.00329501, 0.00225101, 0.00460250, 0.00898887, 0.00226761, 0.00477547, 0.00733631, 0.00291221, 0.00630710, 0.00751479, 0.00689612, 0.00173974, 0.00563776, 0.00815998, 0.00167250, 0.00229697, 0.00325716, 0.00470858, 0.00527702, 0.00818098, 0.00148282, 0.00462232, 0.00933316, 0.00138857, 0.00238555, 0.00762633, 0.00303552, 0.00645273, 0.00649140, 0.00497317, 0.00336784, 0.00249245, 0.00535528, 0.00422402, 0.00225921, 0.00522585, 0.00487342, 0.00807282, 0.00609264, 0.00336792, 0.00345293, 0.00788991, 0.00403721, 0.00707319, 0.00586000, 0.00808540, 0.00224812, 0.00645656, 0.00707232, 0.00461068, 0.00178801, 0.00915959, 0.00261246, 0.00102230, 0.00183417, 0.00233915, 0.00556746, 0.00195742, 0.00988389, 0.00474246, 0.00887435, 0.00659964, 0.00399890, 0.00527403, 0.00472421, 0.00603230, 0.00571263, 0.00231824, 0.00366935, 0.00686331, 0.00915387, 0.00617993, 0.00165323, 0.00735396, 0.00276596, 0.00885354, 0.00139034, 0.00584418, 0.00870543, 0.00140136, 0.00555091, 0.00994500, 0.00796552, 0.00123512, 0.00727996, 0.00187685, 0.00461095, 0.00691361, 0.00617324, 0.00146695, 0.00831509, 0.00975937, 0.00382908, 0.00606202, 0.00680835, 0.00345313, 0.00913755, 0.00785995, 0.00154024, 0.00595731, 0.00926722, 0.00525166, 0.00187137, 0.00993530, 0.00110090, 0.00530838, 0.00717348, 0.00553414, 0.00956775, 0.00687600, 0.00286008, 0.00955099, 0.00342994, 0.00158185, 0.00599290, 0.00787803, 0.00157913, 0.00369156, 0.00461887, 0.00513937, 0.00118272, 0.00338230, 0.00992099, 0.00274419, 0.00530916, 0.00407709, 0.00829721, 0.00250107, 0.00138998, 0.00641586, 0.00957573, 0.00561827, 0.00227332, 0.00330008, 0.00446903, 0.00102483, 0.00768782, 0.00884840, 0.00267209, 0.00652778, 0.00229974, 0.00459435, 0.00312035, 0.00725888, 0.00107404, 0.00329798, 0.00801435, 0.00395276, 0.00950695, 0.00118980, 0.00737230, 0.00951804, 0.00462734, 0.00978697, 0.00411482, 0.00832840, 0.00837395, 0.00241747, 0.00762460, 0.00611307, 0.00251857, 0.00416109, 0.00940038, 0.00465761, 0.00788766, 0.00300499, 0.00598759, 0.00212078, 0.00828283, 0.00187614, 0.00365771, 0.00567390, 0.00528210, 0.00308582, 0.00864102, 0.00832541, 0.00907093, 0.00117779, 0.00146844, 0.00921378, 0.00889000, 0.00450780, 0.00976029, 0.00294925, 0.00583799, 0.00277696, 0.00500300, 0.00271714, 0.00270548, 0.00644773, 0.00905395, 0.00989993, 0.00320034, 0.00229714, 0.00642335, 0.00444562, 0.00204962, 0.00446752, 0.00193771, 0.00724549, 0.00534741, 0.00600885, 0.00579815, 0.00666095, 0.00288257, 0.00639931, 0.00512519, 0.00701920, 0.00686818, 0.00958405, 0.00162831, 0.00319433, 0.00673055, 0.00377208, 0.00104867, 0.00149511, 0.00844135, 0.00484930, 0.00197672, 0.00852013, 0.00275031, 0.00989074, 0.00524397, 0.00859707, 0.00489577, 0.00649415, 0.00550680, 0.00466283, 0.00315506, 0.00865488, 0.00419042, 0.00170256, 0.00232449, 0.00862353, 0.00280026, 0.00383933, 0.00564242, 0.00314641, 0.00530247, 0.00110687, 0.00738816, 0.00991362, 0.00239710, 0.00861307, 0.00950210, 0.00130719, 0.00843133, 0.00660208, 0.00427159, 0.00504300, 0.00786250, 0.00310553, 0.00663611, 0.00635661, 0.00910151, 0.00192686, 0.00610693, 0.00432834, 0.00758671, 0.00875902, 0.00592813, 0.00663162, 0.00998891, 0.00505437, 0.00103262, 0.00202743, 0.00400369, 0.00532255, 0.00419738, 0.00945737, 0.00279675, 0.00511330, 0.00771092, 0.00900712, 0.00480016, 0.00650175, 0.00188292, 0.00620472, 0.00866457, 0.00575271, 0.00804685, 0.00733988, 0.00869799, 0.00795910, 0.00116468, 0.00156812, 0.00371899, 0.00605941, 0.00153659, 0.00804676, 0.00376321, 0.00717160, 0.00340987, 0.00845786, 0.00736360, 0.00850548, 0.00511247, 0.00666645, 0.00377913, 0.00381497, 0.00396724, 0.00562007, 0.00491680, 0.00868764, 0.00678898, 0.00149033, 0.00216523, 0.00595742, 0.00937757, 0.00944176, 0.00190024, 0.00752959, 0.00675951, 0.00470449, 0.00832547, 0.00388846, 0.00929698, 0.00665588, 0.00844239, 0.00873215, 0.00835193, 0.00926793, 0.00359986, 0.00235781, 0.00954912, 0.00614717, 0.00255363, 0.00507612, 0.00587819, 0.00638463, 0.00648562, 0.00868702, 0.00325627, 0.00310146, 0.00934448, 0.00816357, 0.00976914, 0.00615380, 0.00667371, 0.00230142, 0.00281906, 0.00986508, 0.00575196, 0.00505755, 0.00954046, 0.00109702, 0.00141029, 0.00447813, 0.00674441, 0.00465170, 0.00411572, 0.00169732, 0.00312729, 0.00157140, 0.00616913, 0.00707098, 0.00514679, 0.00952701, 0.00706055, 0.00527826, 0.00127352, 0.00466548, 0.00672333, 0.00570752, 0.00421160, 0.00757313, 0.00174458, 0.00556022, 0.00625916, 0.00115015, 0.00558897, 0.00506614, 0.00570281, 0.00832367\n"
          ]
        }
      ],
      "source": [
        "#############non-repeating train\n",
        "learning_rate =  0.001 #0.001 is working great with this toy model.\n",
        "momentum = 0.95\n",
        "val_batch_size = 10\n",
        "\n",
        "file_path = \"/content/drive/MyDrive/KD/toymodel_2.pth\" #toymodel.pth was made with teacher with goodtoy4.pth, 85% after 500 epochs\n",
        "num_epochs = 10\n",
        "batches_per_epoch = 1000 #more data is better yo.  i cranked it up again and i like this.  i think reduce lr.\n",
        "batch_size = 100\n",
        "data_per_batch = batch_size\n",
        "\n",
        "criterion =  nn.MSELoss() #nn.CrossEntropyLoss() # #nn.KLDivLoss() #nn.CrossEntropyLoss()  #i think stick to mse for now.  this probs just needs lots of time to start learning.  like s4 lol.\n",
        "optimizer = optim.Adam(student.parameters(), lr=learning_rate)\n",
        "#optimizer = optim.SGD(student.parameters(), lr=learning_rate, momentum=momentum)\n",
        "#optimizer = optim.Adagrad(student.parameters(), lr=learning_rate)\n",
        "\n",
        "#scheduler = LambdaLR(optimizer, lr_lambda=lr_lambda)\n",
        "#scheduler = CosineAnnealingLRWithPeriod(optimizer, high_lr=0.01, low_lr=0.00001, total_epochs=num_epochs*batches_per_epoch, period_epochs=20)\n",
        "scheduler = UniformRandomLRScheduler(optimizer, low_lr, high_lr)\n",
        "\n",
        "\n",
        "#no train data loader here.\n",
        "val_data = list(zip(teacher_toy.val_inputs, teacher_toy.val_targets))\n",
        "val_input_tensors = torch.stack([torch.Tensor(x[0]) for x in val_data])\n",
        "val_target_tensors = torch.stack([torch.Tensor(x[1]) for x in val_data])\n",
        "val_dataset = TensorDataset(val_input_tensors, val_target_tensors)\n",
        "val_dataloader = DataLoader(val_dataset, batch_size=val_batch_size, shuffle=True)\n",
        "accuracy_threshold = 0.5\n",
        "print_every = 1 #its working, its just lots of data mama.\n",
        "validation_every = 5\n",
        "save_every = 10\n",
        "\n",
        "gen_args  = { 'val_train' : \"train\"\n",
        "              , 'n' : data_per_batch\n",
        "              , 'dist_type' : 'normal_clipped'\n",
        "              , 'm' : vocab_size\n",
        "              , 'std': 1.0\n",
        "              , 'display_progress' : False\n",
        "              , 'store_outputs' : True\n",
        "        }\n",
        "\n",
        "losses = []  # List to store losses\n",
        "accuracies = []\n",
        "\n",
        "\n",
        "\n",
        "student = student.to(device)\n",
        "for epoch in range(num_epochs):\n",
        "    student.train()\n",
        "    teacher_toy.model.eval()\n",
        "    total_loss = 0\n",
        "    correct_predictions = 0\n",
        "    total_samples = 0\n",
        "    lr_list = []\n",
        "    for i in range(batches_per_epoch):\n",
        "\n",
        "        teacher_toy.generate_data(**gen_args)\n",
        "        data_e = list(zip(teacher_toy.train_inputs, teacher_toy.train_targets))\n",
        "        input_tensors_e = torch.stack([torch.Tensor(x[0]) for x in data_e])\n",
        "        target_tensors_e = torch.stack([torch.Tensor(x[1]) for x in data_e])\n",
        "        dataset_e = TensorDataset(input_tensors_e, target_tensors_e)\n",
        "        dataloader_e = DataLoader(dataset_e, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "        for input_batch_e, target_batch_e in dataloader_e:\n",
        "            optimizer.zero_grad()  # Zero the gradients\n",
        "            input_batch_e = input_batch_e.to(device)\n",
        "            target_batch_e = target_batch_e.to(device)\n",
        "            output = student(input_batch_e)  # Forward pass\n",
        "            loss = criterion(output, target_batch_e)  # Compute the loss\n",
        "            loss.backward()  # Backpropagation\n",
        "            optimizer.step()  # Update the weights\n",
        "\n",
        "            current_lr = optimizer.param_groups[0]['lr']\n",
        "            lr_list.append(current_lr)\n",
        "\n",
        "            scheduler.step()\n",
        "\n",
        "            total_loss += loss.item()\n",
        "            #add early stopping...\n",
        "            #and validation at each step.\n",
        "            # Calculate accuracy\n",
        "            soft_output = torch.argmax(F.softmax(output, dim=1),axis=1) #F.softmax(\n",
        "            soft_targets = torch.argmax(F.softmax(target_batch_e,dim=1),axis=1)\n",
        "\n",
        "            correct_predictions += (soft_output == soft_targets).sum().item()\n",
        "            total_samples += input_batch_e.size(0)\n",
        "\n",
        "\n",
        "    # Print the average loss for this epoch\n",
        "    avg_loss = total_loss / ( batches_per_epoch * data_per_batch)\n",
        "    losses.append(avg_loss)\n",
        "\n",
        "    accuracy = correct_predictions / total_samples\n",
        "    accuracies.append(accuracy)\n",
        "\n",
        "    if (epoch + 1) % print_every == 0:\n",
        "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {total_loss}, Train Accuracy: {accuracy:.4f}')\n",
        "\n",
        "        formatted_lr_list = [f'{i:.8f}' for i in lr_list]\n",
        "        formatted_lr_str = ', '.join(formatted_lr_list)\n",
        "        print(f'LRs used: {formatted_lr_str}')\n",
        "\n",
        "        #current_lr = optimizer.param_groups[0]['lr']\n",
        "        #print(current_lr) ##this is misleading.  it has used several different lrs.  write to a list me thinks.\n",
        "    if (epoch + 1) % save_every == 0:\n",
        "      torch.save(student.state_dict(), file_path)\n",
        "    if (epoch + 1) % validation_every == 0:\n",
        "\n",
        "        student.eval()\n",
        "\n",
        "        total_val_samples = 0\n",
        "        correct_val_predictions = 0\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for val_input_batch, val_target_batch in val_dataloader:\n",
        "              val_input_batch = val_input_batch.to(device)\n",
        "              val_target_batch = val_target_batch.to(device)\n",
        "              val_output = student(val_input_batch)\n",
        "\n",
        "              soft_output_val = torch.argmax(F.softmax(val_output, dim=1),axis=1) #F.softmax(\n",
        "              soft_targets_val = torch.argmax(F.softmax(val_target_batch,dim=1),axis=1)\n",
        "\n",
        "              correct_val_predictions += (soft_output_val == soft_targets_val).sum().item()\n",
        "              total_val_samples += val_input_batch.size(0)\n",
        "\n",
        "        # Calculate validation accuracy\n",
        "        val_accuracy = correct_val_predictions / total_val_samples\n",
        "\n",
        "        # Print the validation accuracy for this epoch\n",
        "        print(f'\\t\\tValidation Accuracy: {val_accuracy:.4f}')\n",
        "\n",
        "        # Set the model back to training mode\n",
        "        student.train()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "student.load_state_dict(torch.load(file_path))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8i1pEa2iUFps",
        "outputId": "2e952e6c-fcd4-4d37-f414-e16275d60a38"
      },
      "id": "8i1pEa2iUFps",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "student.eval()\n",
        "i = teacher_toy.train_inputs[0:5]\n",
        "i = i.to(device)\n",
        "teacher_toy.model(i)[0:1] > 0.5, teacher_toy.train_targets[0:1] > 0.5,student(i)[0:1] > 0.5\n",
        "F.softmax(teacher_toy.model(i)[0:1]) > 0.5 , F.softmax(teacher_toy.train_targets[0:1]) > 0.5,F.softmax(student(i)[0:1]) > 0.5\n",
        "#teacher_toy.model(i)[0:3],teacher_toy.train_targets[0:3] ,student(i)[0:3]\n",
        "(F.softmax(teacher_toy.model(i)[0:1])>0.5) == (F.softmax(student(i)[0:1]) > 0.5)\n",
        "#yeah, my accuracy is just bad.."
      ],
      "metadata": {
        "id": "PYtdiiFIfZEE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1403df9b-cb0b-4f81-a9dd-2cd154ed80f8"
      },
      "id": "PYtdiiFIfZEE",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-44-9573ad58335a>:5: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  F.softmax(teacher_toy.model(i)[0:1]) > 0.5 , F.softmax(teacher_toy.train_targets[0:1]) > 0.5,F.softmax(student(i)[0:1]) > 0.5\n",
            "<ipython-input-44-9573ad58335a>:7: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  torch.all((F.softmax(teacher_toy.model(i)[0:1])>0.5) == (F.softmax(student(i)[0:1]) > 0.5))\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(True, device='cuda:0')"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "total_val_samples = 0\n",
        "correct_val_predictions = 0\n",
        "for val_input_batch, val_target_batch in val_dataloader:\n",
        "  val_input_batch = val_input_batch.to(device)\n",
        "  val_target_batch = val_target_batch.to(device)\n",
        "  val_output = student(val_input_batch)\n",
        "\n",
        "  soft_output_val = torch.argmax(F.softmax(val_output, dim=1),axis=1) #F.softmax(\n",
        "  soft_targets_val = torch.argmax(F.softmax(val_target_batch,dim=1),axis=1)\n",
        "\n",
        "  correct_val_predictions += (soft_output_val == soft_targets_val).sum().item()\n",
        "  total_val_samples += val_input_batch.size(0)\n",
        "\n",
        "val_accuracy = correct_val_predictions / total_val_samples\n",
        "print(val_accuracy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pk5O-mScWtCf",
        "outputId": "3ad784d2-9784-46e3-ed50-7dff8438884f"
      },
      "id": "Pk5O-mScWtCf",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.7647\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda\")\n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n",
        "i = 30\n",
        "m = 50\n",
        "teacher_toy.model(teacher_toy.val_inputs[:m][i:i+1].to(device))\n",
        "teacher_toy.val_targets[:m][i:1+2] #they match"
      ],
      "metadata": {
        "id": "wGsH8KmpT55v",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "827647a0-e09b-4b24-923a-669b280ed7b9"
      },
      "id": "wGsH8KmpT55v",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([], device='cuda:0', size=(0, 80))"
            ]
          },
          "metadata": {},
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4a4c7d9d",
      "metadata": {
        "id": "4a4c7d9d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e21928b4-bf58-465c-a525-68d1b684b5af"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([[ 2.0586e+02,  3.4992e+01,  8.5352e+01, -1.1358e+02,  1.5227e+02,\n",
              "          -5.4810e+01, -3.7290e+01, -9.5843e+01, -1.6989e+02,  3.7378e+01,\n",
              "           2.2404e+01, -5.8327e+00, -2.6117e+02, -7.3296e+01,  4.3739e+01,\n",
              "           1.0842e+02,  1.0811e+02, -2.5372e+02, -1.4633e+01, -1.6201e+02,\n",
              "          -3.2133e+01, -2.0622e+01, -8.5095e+01, -3.6187e+01, -1.1476e+02,\n",
              "           1.5680e+01, -1.2444e+02,  5.8280e+01, -3.9752e+01, -1.6273e+01,\n",
              "          -1.7848e+02, -4.9088e+00, -7.4937e-03, -9.0707e+01,  1.1688e+02,\n",
              "          -6.3936e+01,  7.5031e+01,  8.1228e+01,  8.8059e+01,  5.1724e+01,\n",
              "          -8.0736e+01, -6.4757e+01,  1.2481e+02,  1.0759e+02,  1.3555e+01,\n",
              "           7.9394e+01, -3.8735e+01, -1.6845e+02, -3.3377e+01, -6.8988e+01,\n",
              "          -1.0492e+02, -7.4577e+01, -1.0604e+01,  1.0371e+02, -1.1510e+02,\n",
              "          -2.3550e+02,  9.1118e+01, -1.7592e+02, -1.5868e+02, -2.2437e+02,\n",
              "           1.6668e+02,  2.8419e+01,  1.3112e+02, -3.9683e+00, -6.1722e+01,\n",
              "           2.1326e+02,  2.6447e+00,  8.7837e+01, -2.7229e+02,  1.2668e+01,\n",
              "           2.9051e+01,  1.9617e+01, -1.0207e+02, -1.9624e+02,  3.4191e+01,\n",
              "           1.7814e+02, -1.6205e+02,  1.0571e+02, -3.8533e+01, -1.0055e+02]],\n",
              "        device='cuda:0', grad_fn=<AddmmBackward0>),\n",
              " tensor([ 2.0586e+02,  3.4992e+01,  8.5352e+01, -1.1358e+02,  1.5227e+02,\n",
              "         -5.4810e+01, -3.7290e+01, -9.5843e+01, -1.6989e+02,  3.7378e+01,\n",
              "          2.2404e+01, -5.8327e+00, -2.6117e+02, -7.3296e+01,  4.3739e+01,\n",
              "          1.0842e+02,  1.0811e+02, -2.5372e+02, -1.4633e+01, -1.6201e+02,\n",
              "         -3.2133e+01, -2.0622e+01, -8.5095e+01, -3.6187e+01, -1.1476e+02,\n",
              "          1.5680e+01, -1.2444e+02,  5.8280e+01, -3.9751e+01, -1.6273e+01,\n",
              "         -1.7848e+02, -4.9089e+00, -7.4737e-03, -9.0707e+01,  1.1688e+02,\n",
              "         -6.3936e+01,  7.5031e+01,  8.1228e+01,  8.8059e+01,  5.1724e+01,\n",
              "         -8.0736e+01, -6.4757e+01,  1.2481e+02,  1.0759e+02,  1.3555e+01,\n",
              "          7.9394e+01, -3.8735e+01, -1.6845e+02, -3.3377e+01, -6.8988e+01,\n",
              "         -1.0492e+02, -7.4577e+01, -1.0604e+01,  1.0371e+02, -1.1510e+02,\n",
              "         -2.3550e+02,  9.1118e+01, -1.7592e+02, -1.5868e+02, -2.2437e+02,\n",
              "          1.6668e+02,  2.8419e+01,  1.3112e+02, -3.9683e+00, -6.1722e+01,\n",
              "          2.1326e+02,  2.6447e+00,  8.7837e+01, -2.7229e+02,  1.2668e+01,\n",
              "          2.9051e+01,  1.9617e+01, -1.0207e+02, -1.9624e+02,  3.4191e+01,\n",
              "          1.7814e+02, -1.6205e+02,  1.0571e+02, -3.8533e+01, -1.0055e+02],\n",
              "        device='cuda:0'))"
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ],
      "source": [
        "if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda\")\n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n",
        "i = 30\n",
        "m = 50\n",
        "teacher_toy.model(teacher_toy.val_inputs[:m][i:i+1].to(device)) ,teacher_toy.val_targets[:m][i] #they match"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    },
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "V100"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}