{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "411e4bc7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nokay, tasks for this notebook:\\n\\n-make a teacher with descent class distribution\\n    --make a student of same architecture, train it (i have all these parts. ez.)\\n        ---using ints as input (default) and post-sigmoid as output\\n        ---remove last layer and use logits, then compare with sigmoide added on\\n        ---both?\\n    --swap the embedding layer for something that can take random noise.  add that to noiseKD?\\n        ---this should enable us to train on noise, not just int indexes.  compare all\\n        \\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "okay, tasks for this notebook:\n",
    "\n",
    "-make a teacher with descent class distribution\n",
    "    --make a student of same architecture, train it (i have all these parts. ez.)\n",
    "        ---using ints as input (default) and post-sigmoid as output\n",
    "        ---remove last layer and use logits, then compare with sigmoide added on\n",
    "        ---both?\n",
    "    --swap the embedding layer for something that can take random noise.  add that to noiseKD? --on another nb\n",
    "        ---this should enable us to train on noise, not just int indexes.  compare all\n",
    "        \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cd773674",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch.optim as optim\n",
    "from NoiseKD import Teacher, SimpleLanguageModel, slm_init_config, slm_model_config, count_parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c3483e3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "90d7a820",
   "metadata": {},
   "outputs": [],
   "source": [
    "SLM = SimpleLanguageModel(**slm_init_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bc940caa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3105922"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_parameters(SLM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "855daf7b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "80"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "slm_init_config['vocab_size']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3f84dc87",
   "metadata": {},
   "outputs": [],
   "source": [
    "teacher_slm = Teacher(SLM,(160,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a14b2f56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for comparison, i need to be starting them off with the same init.  do that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2529c38a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expected tensor for argument #1 'indices' to have one of the following scalar types: Long, Int; but got torch.FloatTensor instead (while checking arguments for embedding)\n",
      "lets try ints!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Configuring Teacher:: 100%|█████████████████████| 50/50 [01:37<00:00,  1.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Teacher Configured\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "teacher_slm.configure(**slm_model_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1446eb5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Configuring Teacher:: 100%|█████████████████| 2000/2000 [00:51<00:00, 38.73it/s]\n"
     ]
    }
   ],
   "source": [
    "args = { 'val_train' : \"train\"\n",
    "                      , 'n' : 100_000\n",
    "                      , 'dist_type' : 'ints'\n",
    "                      , 'm' : slm_init_config['vocab_size']\n",
    "                      , 'std': 1.0\n",
    "        }\n",
    "teacher_slm.generate_data(**args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a683075",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = { 'val_train' : \"val\"\n",
    "                      , 'n' : 100_000\n",
    "                      , 'dist_type' : 'ints'\n",
    "                      , 'm' : slm_init_config['vocab_size']\n",
    "                      , 'std': 1.0\n",
    "        }\n",
    "teacher_slm.generate_data(**args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "283d99d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "student = SimpleLanguageModel(**slm_init_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "44c081c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SimpleLanguageModel(\n",
       "  (embedding): Embedding(80, 16)\n",
       "  (transformer_encoder): TransformerEncoder(\n",
       "    (transformer): TransformerEncoder(\n",
       "      (layers): ModuleList(\n",
       "        (0): TransformerEncoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=16, out_features=16, bias=True)\n",
       "          )\n",
       "          (linear1): Linear(in_features=16, out_features=11, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (linear2): Linear(in_features=11, out_features=16, bias=True)\n",
       "          (norm1): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout1): Dropout(p=0.1, inplace=False)\n",
       "          (dropout2): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (1): TransformerEncoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=16, out_features=16, bias=True)\n",
       "          )\n",
       "          (linear1): Linear(in_features=16, out_features=11, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (linear2): Linear(in_features=11, out_features=16, bias=True)\n",
       "          (norm1): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout1): Dropout(p=0.1, inplace=False)\n",
       "          (dropout2): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (fc1): Linear(in_features=2560, out_features=1000, bias=True)\n",
       "  (fc2): Linear(in_features=1000, out_features=500, bias=True)\n",
       "  (output_layer): Linear(in_features=500, out_features=80, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "student"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f207cc31",
   "metadata": {},
   "outputs": [],
   "source": [
    "##to train form the teacher object was written already"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e2e29454",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 100\n",
    "batch_size = 32\n",
    "learning_rate = 0.00025 #raise this.   this shit is slooooow\n",
    "momentum = 0.95\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(student.parameters(), lr=learning_rate)\n",
    "\n",
    "optimizer = optim.SGD(student.parameters(), lr=learning_rate, momentum=momentum)\n",
    "data = list(zip(teacher_slm.train_inputs, teacher_slm.train_targets))\n",
    "input_tensors = torch.stack([torch.Tensor(x[0]) for x in data])\n",
    "target_tensors = torch.stack([torch.Tensor(x[1]) for x in data])\n",
    "dataset = TensorDataset(input_tensors, target_tensors)\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b8d1977",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/100], Loss: 0.01218199\n",
      "Epoch [2/100], Loss: 0.01215971\n",
      "Epoch [3/100], Loss: 0.01213271\n",
      "Epoch [4/100], Loss: 0.01209857\n",
      "Epoch [5/100], Loss: 0.01205400\n",
      "Epoch [6/100], Loss: 0.01199203\n",
      "Epoch [7/100], Loss: 0.01189930\n",
      "Epoch [8/100], Loss: 0.01174310\n",
      "Epoch [9/100], Loss: 0.01144297\n",
      "Epoch [10/100], Loss: 0.01098119\n",
      "Epoch [11/100], Loss: 0.01060179\n",
      "Epoch [12/100], Loss: 0.01033759\n",
      "Epoch [13/100], Loss: 0.01018839\n",
      "Epoch [14/100], Loss: 0.01007457\n",
      "Epoch [15/100], Loss: 0.01000466\n",
      "Epoch [16/100], Loss: 0.00993143\n",
      "Epoch [17/100], Loss: 0.00986348\n",
      "Epoch [18/100], Loss: 0.00983032\n",
      "Epoch [19/100], Loss: 0.00981814\n",
      "Epoch [20/100], Loss: 0.00981172\n",
      "Epoch [21/100], Loss: 0.00980867\n",
      "Epoch [22/100], Loss: 0.00980651\n",
      "Epoch [23/100], Loss: 0.00980508\n",
      "Epoch [24/100], Loss: 0.00980358\n",
      "Epoch [25/100], Loss: 0.00980265\n",
      "Epoch [26/100], Loss: 0.00980144\n",
      "Epoch [27/100], Loss: 0.00980076\n",
      "Epoch [28/100], Loss: 0.00980110\n",
      "Epoch [29/100], Loss: 0.00980042\n",
      "Epoch [30/100], Loss: 0.00979980\n",
      "Epoch [31/100], Loss: 0.00979864\n",
      "Epoch [32/100], Loss: 0.00979832\n",
      "Epoch [33/100], Loss: 0.00979780\n",
      "Epoch [34/100], Loss: 0.00979705\n",
      "Epoch [35/100], Loss: 0.00979695\n"
     ]
    }
   ],
   "source": [
    "#conventional train, repeats throught the dataset\n",
    "\n",
    "print_every = 1 #its working, its just lots of data mama.\n",
    "\n",
    "losses = []  # List to store losses\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    total_loss = 0\n",
    "    for input_batch, target_batch in dataloader:\n",
    "        optimizer.zero_grad()  # Zero the gradients\n",
    "        output = student(input_batch)  # Forward pass\n",
    "        loss = criterion(output, target_batch)  # Compute the loss\n",
    "        loss.backward()  # Backpropagation\n",
    "        optimizer.step()  # Update the weights\n",
    "        total_loss += loss.item()\n",
    "        #add early stopping...\n",
    "\n",
    "    # Print the average loss for this epoch\n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    losses.append(avg_loss)\n",
    "    if (epoch + 1) % print_every == 0:\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {avg_loss:.8f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d872499",
   "metadata": {},
   "outputs": [],
   "source": [
    "#all unique train, \n",
    "print_every = 1\n",
    "batch_eternal = 50\n",
    "data_n = 5000\n",
    "num_epochs = 400\n",
    "losses = []  # List to store losses\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    total_loss = 0\n",
    "    teacher.generate_data(\n",
    "    \"train\"\n",
    "    ,data_n\n",
    "    ,'normal'\n",
    "    , m =0.0\n",
    "    , std=1.0\n",
    "    )\n",
    "    data = list(zip(teacher.train_inputs, teacher.train_targets))\n",
    "    input_tensors = torch.stack([torch.Tensor(x[0]) for x in data])\n",
    "    target_tensors = torch.stack([torch.Tensor(x[1]) for x in data])\n",
    "    dataset = TensorDataset(input_tensors, target_tensors)\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_eternal, shuffle=True)\n",
    "    \n",
    "    for input_batch, target_batch in dataloader:\n",
    "        optimizer.zero_grad()  # Zero the gradients\n",
    "        output = student(input_batch)  # Forward pass\n",
    "        loss = criterion(output, target_batch)  # Compute the loss\n",
    "        loss.backward()  # Backpropagation\n",
    "        optimizer.step()  # Update the weights\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    # Print the average loss for this epoch\n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    losses.append(avg_loss)\n",
    "    if (epoch + 1) % print_every == 0:\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {avg_loss:.8f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
