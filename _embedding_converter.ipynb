{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "661af10a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch.optim as optim\n",
    "from NoiseKD import Teacher, SimpleLanguageModel, slm_init_config, slm_model_config, count_parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9e5d21bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "SLM = SimpleLanguageModel(**slm_init_config) #use the slm_init_config to configure the model, then pass to Teacher.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "5bd5e03e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'embedding_dim': 16,\n",
       " 'num_heads': 8,\n",
       " 'hidden_dim': 11,\n",
       " 'num_layers': 2,\n",
       " 'dropout': 0.1,\n",
       " 'vocab_size': 80,\n",
       " 'class_num': 80,\n",
       " 'sequence_length': 160}"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "slm_init_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ff8f4789",
   "metadata": {},
   "outputs": [],
   "source": [
    "teacher_slm = Teacher(SLM,(slm_init_config['sequence_length'],))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "87a20dd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expected tensor for argument #1 'indices' to have one of the following scalar types: Long, Int; but got torch.FloatTensor instead (while checking arguments for embedding)\n",
      "lets try ints!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Configuring Teacher:: 100%|█████████████████████| 50/50 [01:39<00:00,  1.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Teacher Configured\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "teacher_slm.configure(**slm_model_config) #this is the gen config.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7328894c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Configuring Teacher:: 100%|█████████████████| 2000/2000 [00:52<00:00, 38.35it/s]\n",
      "Configuring Teacher:: 100%|█████████████████| 2000/2000 [00:49<00:00, 40.14it/s]\n"
     ]
    }
   ],
   "source": [
    "#we want to have a configured teacher, then be able to duplicate it with the embedding layer converted to something\n",
    "#that can take floats.\n",
    "train_args = { 'val_train' : \"train\"\n",
    "                      , 'n' : 100_000\n",
    "                      , 'dist_type' : 'ints'\n",
    "                      , 'm' : slm_init_config['vocab_size']\n",
    "                      , 'std': 1.0\n",
    "        }\n",
    "teacher_slm.generate_data(**train_args)\n",
    "\n",
    "val_args = { 'val_train' : \"val\"\n",
    "                      , 'n' : 100_000\n",
    "                      , 'dist_type' : 'ints'\n",
    "                      , 'm' : slm_init_config['vocab_size']\n",
    "                      , 'std': 1.0\n",
    "        }\n",
    "teacher_slm.generate_data(**val_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "812b4fef",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_weight_tenso = teacher_slm.model.embedding.weight.detach() #.numpy().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "e654c506",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_layer = teacher_slm.model.embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "0f6bbaf9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 160])"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data = teacher_slm.train_inputs[0:10]#this is a batch of 10 inputs, indexes.  \n",
    "test_data.shape #this is batch of 10, seq_len of 160, each corresponding to an index.  \n",
    "#so, if we do onehot, it should be b x 160 x vocab_size which is 80 in this case.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "07005478",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 160, 16])"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_layer(test_data).shape#works.  slm_init_config['embedding_dim'] is where the 16 is coming from."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "fdaa0ec3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([80, 16])"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_weight_tensor.shape #with onehot, it should be b x 160 x 80 * 80 * 16 = b x 160 x 16.  just linear! 1d convolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "d6770f6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_one_hot(input_sequences, vocab_size):\n",
    "    batch_size = input_sequences.size(0)\n",
    "    max_seq_length = input_sequences.size(1)\n",
    "    \n",
    "    # Create a tensor to store the one-hot encodings\n",
    "    one_hot_input = torch.zeros(batch_size, max_seq_length, vocab_size)\n",
    "    \n",
    "    # Use scatter_ to set the appropriate elements to 1 in each batch\n",
    "    one_hot_input.scatter_(2, input_sequences.unsqueeze(2), 1)\n",
    "    \n",
    "    return one_hot_input\n",
    "\n",
    "oh = batch_one_hot(test_data, vocab_size = 80)\n",
    "##test_data == torch.argmax(oh, dim=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "364d6e0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80 16\n"
     ]
    }
   ],
   "source": [
    "def Linearize_Embedding(embedding_layer):\n",
    "    embedding_weight_tensor = embedding_layer.weight.detach() \n",
    "    shape = embedding_weight_tensor.shape\n",
    "    vocab_size = shape[0]\n",
    "    embedding_dim = shape[1]\n",
    "    print(vocab_size,embedding_dim)\n",
    "    lin = nn.Linear(vocab_size,embedding_dim)\n",
    "    lin.weight = nn.Parameter(embedding_weight_tensor.T) #not sure about this transpose\n",
    "    return lin\n",
    "L = Linearize_Embedding(embedding_layer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "e91356d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 80])"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "L.weight.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "cee0e821",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 80])"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_weight_tenso.T.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "9d171f21",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 160, 80])"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "oh.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "4edc6246",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.2922,  0.9590, -0.3747,  ..., -0.8251, -1.4937,  0.8997],\n",
       "         [-0.6123, -0.4356,  0.2401,  ..., -1.1940, -1.5489,  0.5022],\n",
       "         [-1.3747,  0.5595,  2.5854,  ..., -0.8407,  1.8977, -0.2874],\n",
       "         ...,\n",
       "         [ 0.7573, -0.0439, -0.9874,  ..., -0.6674,  1.1896, -0.7050],\n",
       "         [ 1.1906, -0.9558,  0.1148,  ..., -1.9944,  0.2102, -1.5978],\n",
       "         [ 0.8662, -0.5946,  0.4465,  ...,  0.6806,  0.0759, -0.7071]],\n",
       "\n",
       "        [[ 0.4890, -0.8238,  0.0754,  ...,  0.6244, -1.7895, -2.2388],\n",
       "         [ 0.7802,  1.0969,  0.5488,  ..., -1.4447, -0.0410,  1.5863],\n",
       "         [ 1.1503,  1.0996,  1.1508,  ..., -0.1844,  0.1494,  0.9812],\n",
       "         ...,\n",
       "         [ 2.1915, -1.4852,  1.7001,  ..., -0.1682,  0.2967,  0.9008],\n",
       "         [-0.0962,  0.5417, -0.1710,  ...,  1.5816, -0.8062,  0.7614],\n",
       "         [ 1.1021,  0.3538, -0.7704,  ..., -1.1417, -0.0236,  0.5138]],\n",
       "\n",
       "        [[ 0.1323,  0.4150, -0.6024,  ..., -1.2214, -0.7204, -0.0747],\n",
       "         [-1.3448, -0.3103, -1.0887,  ...,  0.7148, -0.3811, -0.9730],\n",
       "         [ 0.7802,  1.0969,  0.5488,  ..., -1.4447, -0.0410,  1.5863],\n",
       "         ...,\n",
       "         [ 0.3220, -0.6561, -0.5159,  ..., -0.3152,  1.4498, -0.1207],\n",
       "         [ 0.3407,  0.0629,  0.4250,  ...,  0.9508, -0.9996, -0.5107],\n",
       "         [-1.3747,  0.5595,  2.5854,  ..., -0.8407,  1.8977, -0.2874]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[-1.3747,  0.5595,  2.5854,  ..., -0.8407,  1.8977, -0.2874],\n",
       "         [ 0.4890, -0.8238,  0.0754,  ...,  0.6244, -1.7895, -2.2388],\n",
       "         [ 1.1021,  0.3538, -0.7704,  ..., -1.1417, -0.0236,  0.5138],\n",
       "         ...,\n",
       "         [-0.6543,  1.1369,  1.5722,  ...,  0.5087,  1.6105, -0.0214],\n",
       "         [ 0.3058,  0.2337, -0.9753,  ...,  0.3559,  0.3478,  1.0139],\n",
       "         [ 0.2087, -1.3108, -0.3174,  ..., -1.1043,  1.7695, -0.6394]],\n",
       "\n",
       "        [[-0.1212, -1.1608,  1.6955,  ...,  0.7073,  0.8508, -0.4645],\n",
       "         [-0.6517,  0.1372, -0.5916,  ...,  1.5091,  0.5015,  1.4900],\n",
       "         [ 1.5916, -1.0352, -0.9725,  ...,  0.3760,  0.6603,  0.3414],\n",
       "         ...,\n",
       "         [ 0.3058,  0.2337, -0.9753,  ...,  0.3559,  0.3478,  1.0139],\n",
       "         [ 1.1021,  0.3538, -0.7704,  ..., -1.1417, -0.0236,  0.5138],\n",
       "         [-1.3448, -0.3103, -1.0887,  ...,  0.7148, -0.3811, -0.9730]],\n",
       "\n",
       "        [[-0.5802,  1.3018, -1.4418,  ..., -0.6666,  0.1016,  0.5425],\n",
       "         [ 0.5581, -0.5894,  1.1004,  ...,  0.1430,  0.8642,  0.0587],\n",
       "         [ 0.0720, -0.8367,  1.0951,  ..., -0.0161, -1.1050, -0.4427],\n",
       "         ...,\n",
       "         [ 1.5916, -1.0352, -0.9725,  ...,  0.3760,  0.6603,  0.3414],\n",
       "         [ 0.8662, -0.5946,  0.4465,  ...,  0.6806,  0.0759, -0.7071],\n",
       "         [-0.6248, -0.6629,  0.3225,  ...,  0.0835, -1.0347, -0.6185]]],\n",
       "       grad_fn=<EmbeddingBackward0>)"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_layer(test_data) #this isn't changing.   good."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "f605f6cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.3137,  0.9066, -0.3850,  ..., -0.7290, -1.3897,  0.8688],\n",
       "         [-0.6337, -0.4881,  0.2298,  ..., -1.0980, -1.4449,  0.4713],\n",
       "         [-1.3961,  0.5070,  2.5751,  ..., -0.7447,  2.0017, -0.3183],\n",
       "         ...,\n",
       "         [ 0.7359, -0.0963, -0.9977,  ..., -0.5714,  1.2936, -0.7359],\n",
       "         [ 1.1691, -1.0083,  0.1044,  ..., -1.8983,  0.3142, -1.6288],\n",
       "         [ 0.8448, -0.6471,  0.4362,  ...,  0.7767,  0.1799, -0.7381]],\n",
       "\n",
       "        [[ 0.4676, -0.8763,  0.0651,  ...,  0.7204, -1.6855, -2.2698],\n",
       "         [ 0.7587,  1.0445,  0.5385,  ..., -1.3487,  0.0630,  1.5553],\n",
       "         [ 1.1289,  1.0472,  1.1405,  ..., -0.0883,  0.2534,  0.9503],\n",
       "         ...,\n",
       "         [ 2.1701, -1.5377,  1.6898,  ..., -0.0721,  0.4007,  0.8699],\n",
       "         [-0.1177,  0.4893, -0.1813,  ...,  1.6777, -0.7023,  0.7304],\n",
       "         [ 1.0806,  0.3013, -0.7807,  ..., -1.0456,  0.0804,  0.4828]],\n",
       "\n",
       "        [[ 0.1109,  0.3625, -0.6128,  ..., -1.1254, -0.6165, -0.1056],\n",
       "         [-1.3662, -0.3628, -1.0990,  ...,  0.8109, -0.2772, -1.0039],\n",
       "         [ 0.7587,  1.0445,  0.5385,  ..., -1.3487,  0.0630,  1.5553],\n",
       "         ...,\n",
       "         [ 0.3005, -0.7086, -0.5262,  ..., -0.2191,  1.5538, -0.1516],\n",
       "         [ 0.3193,  0.0105,  0.4147,  ...,  1.0469, -0.8956, -0.5416],\n",
       "         [-1.3961,  0.5070,  2.5751,  ..., -0.7447,  2.0017, -0.3183]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[-1.3961,  0.5070,  2.5751,  ..., -0.7447,  2.0017, -0.3183],\n",
       "         [ 0.4676, -0.8763,  0.0651,  ...,  0.7204, -1.6855, -2.2698],\n",
       "         [ 1.0806,  0.3013, -0.7807,  ..., -1.0456,  0.0804,  0.4828],\n",
       "         ...,\n",
       "         [-0.6758,  1.0845,  1.5619,  ...,  0.6048,  1.7144, -0.0523],\n",
       "         [ 0.2844,  0.1812, -0.9856,  ...,  0.4520,  0.4518,  0.9829],\n",
       "         [ 0.1872, -1.3633, -0.3277,  ..., -1.0083,  1.8735, -0.6703]],\n",
       "\n",
       "        [[-0.1427, -1.2132,  1.6852,  ...,  0.8034,  0.9548, -0.4954],\n",
       "         [-0.6731,  0.0847, -0.6019,  ...,  1.6052,  0.6055,  1.4591],\n",
       "         [ 1.5701, -1.0876, -0.9828,  ...,  0.4721,  0.7643,  0.3105],\n",
       "         ...,\n",
       "         [ 0.2844,  0.1812, -0.9856,  ...,  0.4520,  0.4518,  0.9829],\n",
       "         [ 1.0806,  0.3013, -0.7807,  ..., -1.0456,  0.0804,  0.4828],\n",
       "         [-1.3662, -0.3628, -1.0990,  ...,  0.8109, -0.2772, -1.0039]],\n",
       "\n",
       "        [[-0.6017,  1.2493, -1.4521,  ..., -0.5705,  0.2056,  0.5115],\n",
       "         [ 0.5367, -0.6418,  1.0901,  ...,  0.2391,  0.9681,  0.0278],\n",
       "         [ 0.0505, -0.8892,  1.0848,  ...,  0.0799, -1.0010, -0.4737],\n",
       "         ...,\n",
       "         [ 1.5701, -1.0876, -0.9828,  ...,  0.4721,  0.7643,  0.3105],\n",
       "         [ 0.8448, -0.6471,  0.4362,  ...,  0.7767,  0.1799, -0.7381],\n",
       "         [-0.6462, -0.7153,  0.3122,  ...,  0.1796, -0.9307, -0.6494]]],\n",
       "       grad_fn=<ViewBackward0>)"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "L(oh) #not changing now...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "83a48a3b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[True, True, True,  ..., True, True, True],\n",
       "        [True, True, True,  ..., True, True, True],\n",
       "        [True, True, True,  ..., True, True, True],\n",
       "        ...,\n",
       "        [True, True, True,  ..., True, True, True],\n",
       "        [True, True, True,  ..., True, True, True],\n",
       "        [True, True, True,  ..., True, True, True]])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn.Parameter(embedding_weight_tensor) == L.weight #init works. not sure about having a bias in linear layer tho..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b20abe72",
   "metadata": {},
   "outputs": [],
   "source": [
    "#i think i just need to convolve over the sequence lenght mama.  \n",
    "#not too hard.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "42817147",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SimpleLanguageModel(\n",
       "  (embedding): Embedding(80, 16)\n",
       "  (transformer_encoder): TransformerEncoder(\n",
       "    (transformer): TransformerEncoder(\n",
       "      (layers): ModuleList(\n",
       "        (0): TransformerEncoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=16, out_features=16, bias=True)\n",
       "          )\n",
       "          (linear1): Linear(in_features=16, out_features=11, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (linear2): Linear(in_features=11, out_features=16, bias=True)\n",
       "          (norm1): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout1): Dropout(p=0.1, inplace=False)\n",
       "          (dropout2): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (1): TransformerEncoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=16, out_features=16, bias=True)\n",
       "          )\n",
       "          (linear1): Linear(in_features=16, out_features=11, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (linear2): Linear(in_features=11, out_features=16, bias=True)\n",
       "          (norm1): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout1): Dropout(p=0.1, inplace=False)\n",
       "          (dropout2): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (fc1): Linear(in_features=2560, out_features=1000, bias=True)\n",
       "  (fc2): Linear(in_features=1000, out_features=500, bias=True)\n",
       "  (output_layer): Linear(in_features=500, out_features=80, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13a83bca",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
