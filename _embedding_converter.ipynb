{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "661af10a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch.optim as optim\n",
    "from NoiseKD import Teacher, ToyTransformer, slm_init_config, slm_model_config, count_parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9e5d21bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#SLM = SimpleLanguageModel(**slm_init_config) #THIS GRADIENT ACCUMULATES!!!!BUGGGGGG "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5bd5e03e",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 16\n",
    "num_heads = 8\n",
    "hidden_dim  = 11\n",
    "num_layers = 2\n",
    "dropout = 0.1\n",
    "vocab_size = 80\n",
    "class_num = vocab_size\n",
    "batch_size = 50\n",
    "sequence_length = 160"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ff8f4789",
   "metadata": {},
   "outputs": [],
   "source": [
    "TT = ToyTransformer(vocab_size, embedding_dim, num_heads, hidden_dim, num_layers, dropout,sequence_length)\n",
    "teacher_toy = Teacher(TT,(sequence_length,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "87a20dd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expected tensor for argument #1 'indices' to have one of the following scalar types: Long, Int; but got torch.FloatTensor instead (while checking arguments for embedding)\n",
      "lets try ints!\n"
     ]
    }
   ],
   "source": [
    "teacher_toy.load_state_dict('good_toy_5.pth')\n",
    "#teacher_toy.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ff47d483",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Teacher Embedding Linearized!\n"
     ]
    }
   ],
   "source": [
    "teacher_toy.linearize_embedding()\n",
    "#teacher_toy.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7328894c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#we want to have a configured teacher, then be able to duplicate it with the embedding layer converted to something\n",
    "#that can take floats.\n",
    "train_args = { 'val_train' : \"train\"\n",
    "                      , 'n' : 1000\n",
    "                      , 'dist_type' : 'ints'\n",
    "                      , 'm' : slm_init_config['vocab_size'] ##this should be inferred in the method.\n",
    "                      , 'std': 1.0\n",
    "        }\n",
    "teacher_toy.generate_data(**train_args)\n",
    "\n",
    "val_args = { 'val_train' : \"val\"\n",
    "                      , 'n' : 1000\n",
    "                      , 'dist_type' : 'ints'\n",
    "                      , 'm' : slm_init_config['vocab_size']\n",
    "                      , 'std': 1.0\n",
    "        }\n",
    "teacher_toy.generate_data(**val_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b7517050",
   "metadata": {},
   "outputs": [],
   "source": [
    "lin_val_args = { 'val_train' : \"val\"\n",
    "                      , 'n' : 100\n",
    "                      , 'dist_type' : 'normal_clipped_inverse'\n",
    "        }\n",
    "teacher_toy.generate_data(**lin_val_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fb06ddcc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.0000, 0.3923, 0.5651,  ..., 0.4751, 0.0420, 0.1020],\n",
       "         [0.6553, 0.4580, 0.6098,  ..., 0.2148, 0.7105, 0.5373],\n",
       "         [0.2807, 0.5236, 0.6551,  ..., 0.4721, 0.7736, 0.6352],\n",
       "         ...,\n",
       "         [0.4922, 0.4807, 0.5986,  ..., 0.8236, 0.4456, 0.2389],\n",
       "         [0.4042, 0.4483, 0.6076,  ..., 0.2904, 0.5027, 0.2318],\n",
       "         [0.4612, 0.4757, 0.1363,  ..., 0.2545, 0.5905, 0.1655]],\n",
       "\n",
       "        [[0.6465, 0.5841, 0.6192,  ..., 0.5369, 0.5969, 0.8158],\n",
       "         [0.5441, 0.6152, 0.4416,  ..., 0.9398, 0.6739, 0.4845],\n",
       "         [0.3579, 1.0000, 0.4938,  ..., 0.5562, 0.4871, 0.0000],\n",
       "         ...,\n",
       "         [0.3428, 0.6276, 0.3583,  ..., 0.0000, 0.6080, 0.3671],\n",
       "         [0.6180, 0.3862, 0.3719,  ..., 0.3041, 0.5790, 0.6470],\n",
       "         [0.5926, 0.5293, 0.3705,  ..., 0.3968, 0.3862, 0.4139]],\n",
       "\n",
       "        [[0.7075, 0.4595, 0.3828,  ..., 0.4934, 0.7570, 0.5298],\n",
       "         [0.5745, 0.4999, 0.1969,  ..., 0.7156, 0.4552, 0.8030],\n",
       "         [0.4113, 0.6515, 0.5642,  ..., 0.6872, 0.4792, 0.4961],\n",
       "         ...,\n",
       "         [0.4222, 0.4934, 0.3701,  ..., 0.4973, 0.7713, 0.2873],\n",
       "         [0.5719, 0.7036, 0.6442,  ..., 0.5853, 0.6555, 0.4100],\n",
       "         [0.5729, 0.3344, 0.7079,  ..., 0.2820, 0.3337, 0.1980]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[0.5185, 0.1293, 0.6402,  ..., 0.2110, 0.3503, 0.8332],\n",
       "         [0.5176, 0.7952, 0.7641,  ..., 0.4392, 0.3014, 0.5193],\n",
       "         [0.3061, 0.3768, 0.2780,  ..., 0.4010, 0.4162, 0.4612],\n",
       "         ...,\n",
       "         [0.7138, 0.4142, 0.8768,  ..., 0.0000, 0.3702, 0.4388],\n",
       "         [0.3162, 0.3833, 0.4922,  ..., 0.2576, 0.8548, 0.6196],\n",
       "         [0.6951, 0.8337, 0.7905,  ..., 0.4705, 0.3285, 0.5578]],\n",
       "\n",
       "        [[0.7337, 0.5588, 0.4825,  ..., 0.0000, 0.6513, 0.4725],\n",
       "         [0.6250, 0.6410, 0.7845,  ..., 0.5068, 0.3806, 0.6704],\n",
       "         [0.2795, 0.6051, 0.3780,  ..., 0.3817, 0.7217, 0.0000],\n",
       "         ...,\n",
       "         [0.6514, 0.1653, 0.5303,  ..., 0.4822, 0.3939, 0.5078],\n",
       "         [0.3232, 0.1843, 0.6273,  ..., 0.1823, 0.3470, 0.4503],\n",
       "         [0.1675, 0.7172, 0.3785,  ..., 0.8293, 0.2736, 0.2602]],\n",
       "\n",
       "        [[0.7105, 0.8184, 0.4012,  ..., 0.4810, 0.6073, 0.9341],\n",
       "         [0.6954, 0.9401, 0.4938,  ..., 0.4303, 0.1760, 0.5210],\n",
       "         [0.4521, 0.5921, 0.3697,  ..., 0.4227, 0.3179, 0.5215],\n",
       "         ...,\n",
       "         [0.4664, 0.2980, 0.5270,  ..., 0.8524, 0.1204, 0.2694],\n",
       "         [0.5758, 0.5277, 0.6224,  ..., 0.6018, 0.2286, 0.8115],\n",
       "         [0.8412, 0.6169, 0.0174,  ..., 0.4591, 0.7680, 0.4321]]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "teacher_toy.val_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be303915",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "gen_shape = (3, 4,7) \n",
    "samples = np.random.normal(0.5, 0.2, gen_shape)\n",
    "samples = 1-np.clip(samples, 0, 1)\n",
    "samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76d3034d",
   "metadata": {},
   "outputs": [],
   "source": [
    "teacher_toy.val_inputs[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6770f6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_one_hot(input_sequences, vocab_size):\n",
    "    batch_size = input_sequences.size(0)\n",
    "    max_seq_length = input_sequences.size(1)\n",
    "    \n",
    "    # Create a tensor to store the one-hot encodings\n",
    "    one_hot_input = torch.zeros(batch_size, max_seq_length, vocab_size)\n",
    "    \n",
    "    # Use scatter_ to set the appropriate elements to 1 in each batch\n",
    "    one_hot_input.scatter_(2, input_sequences.unsqueeze(2), 1)\n",
    "    \n",
    "    return one_hot_input\n",
    "\n",
    "\n",
    "##test_data == torch.argmax(oh, dim=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85066504",
   "metadata": {},
   "outputs": [],
   "source": [
    "aa = (10,)\n",
    "bb = (23,)\n",
    "n = 100\n",
    "gen_shape = (n,) + aa + bb\n",
    "gen_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da798c89",
   "metadata": {},
   "outputs": [],
   "source": [
    "oh = batch_one_hot(teacher_toy.val_inputs, vocab_size = 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90943015",
   "metadata": {},
   "outputs": [],
   "source": [
    "#oh[0:10].shape  torch.Size([10, 160, 80])\n",
    "teacher_toy.model(oh[0:10]) #this works.  add vocab_size at the end.....i think i can get that from the embedding layer?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1d68d1d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "364d6e0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def has_embedding_layer(model):\n",
    "    for module in model.modules():\n",
    "        if isinstance(module, nn.Embedding):\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def Linearize_Embedding(embedding_layer):\n",
    "    embedding_weight_tensor = embedding_layer.weight.detach() \n",
    "    shape = embedding_weight_tensor.shape\n",
    "    vocab_size = shape[0]\n",
    "    embedding_dim = shape[1]\n",
    "    lin = nn.Linear(vocab_size,embedding_dim, bias = False)\n",
    "    #print(lin.weight.shape)\n",
    "    #print(embedding_weight_tensor.shape)\n",
    "    lin.weight = nn.Parameter(embedding_weight_tensor.T) #not sure about this transpose\n",
    "    return lin\n",
    "\n",
    "def Linearize_Model_Embedding(model):\n",
    "    if not has_embedding_layer(model):\n",
    "        raise ValueError(\"Your model should have an Embedding layer, or maybe try an art degree.\")\n",
    "    embedding_layer = model.embedding\n",
    "    linearized_embedding = Linearize_Embedding(embedding_layer)\n",
    "    model_copy = copy.deepcopy(model)\n",
    "    model_copy.embedding = linearized_embedding \n",
    "    return model_copy\n",
    "\n",
    "L = Linearize_Embedding(embedding_layer)\n",
    "# i want a function in KDNoise that takes a model with an embedding layer, and linearizes it. no, add it as an attribute maybe?\n",
    "#so, the model should have an embedding layer, if it does you make a new instance of the model, and store it in like\n",
    "#Teacher.model_linearized.  and, it needs to be able to be instantiated with the state_dict of the of teacher.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c74f9da9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "embedding_layer = teacher_toy.model.embedding\n",
    "L = Linearize_Embedding(embedding_layer)\n",
    "copy_TT = copy.deepcopy(TT)\n",
    "copy_TT.embedding = L #swap out the embedding\n",
    "#TT,copy_TT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa72e1bb",
   "metadata": {},
   "outputs": [],
   "source": [
    " #should just set self.model i think.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4edc6246",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_layer(test_data).shape,L(oh).shape\n",
    "#normal takes the test data (ints), linearized takes onehotted data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e1b059d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#embedding_layer(test_data),L(oh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f605f6cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.sum((L(oh) - embedding_layer(test_data))**2) #ha.  it was the bias.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83a48a3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "nn.Parameter(embedding_weight_tensor) == L.weight.T #init works. not sure about having a bias in linear layer tho..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13a83bca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4b5446a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48be0b6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "TT(test_data[0:10]) #,test_data[0:10] ##accumulating gradient!!!!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da056e01",
   "metadata": {},
   "outputs": [],
   "source": [
    "oh = batch_one_hot(test_data, vocab_size = 80) #i do this above, this is just so i can remeber what it is doing\n",
    "copy_TT(oh[0:10]) #now it takes one hot, not indexes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "995c39dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.sum((TT(test_data) - copy_TT(oh))**2) #asdflkajsdglfonsdg;aogh so the parameter layer is working, but the final bit isnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce745f31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# final check, is the batch bug fixed\"\n",
    "TT(test_data[0:10])[3:4],TT(test_data[3:4]) ##just floating point!!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95a7cd94",
   "metadata": {},
   "outputs": [],
   "source": [
    "copy_TT(oh[0:10])[3:4],copy_TT(oh[3:4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1076c88e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
