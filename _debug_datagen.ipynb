{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6a04cb7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "99b445af",
   "metadata": {},
   "outputs": [],
   "source": [
    "from NoiseKD import Teacher"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5a11b24",
   "metadata": {},
   "source": [
    "First hurdle. the embedding layer.  It requires the ints as inputs, not one hot.  i need one hot.  i will also need to rewire the LLM i use to do the same.  hmm.  save the weights, load it into a model that has the same shapes, but accepts one hot arrays, not vocab_indexes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dad448f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEncoder(nn.Module):\n",
    "    def __init__(self, embedding_dim, num_heads, hidden_dim, num_layers, dropout):\n",
    "        super(TransformerEncoder, self).__init__()\n",
    "        self.transformer = nn.TransformerEncoder(\n",
    "            nn.TransformerEncoderLayer(embedding_dim, num_heads, hidden_dim, dropout, batch_first = True),\n",
    "            num_layers\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.transformer(x)\n",
    "\n",
    "class SimpleLanguageModel(nn.Module):\n",
    "    def __init__(self, vocab_size, sequence_length, embedding_dim, class_num, num_heads, hidden_dim, num_layers, dropout):\n",
    "        super(SimpleLanguageModel, self).__init__()\n",
    "\n",
    "        # Define the embedding layer\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        \n",
    "        # Define the transformer encoder\n",
    "        self.transformer_encoder = TransformerEncoder(embedding_dim, num_heads, hidden_dim, num_layers, dropout)\n",
    "        \n",
    "        self.fc1 = nn.Linear(sequence_length*embedding_dim, 1000)\n",
    "        self.fc2 = nn.Linear(1000, 500)\n",
    "        \n",
    "        self.output_layer = nn.Linear(500, class_num)\n",
    "        \n",
    "        \n",
    "\n",
    "    def forward(self, input_data):\n",
    "        # Input_data is of shape (batch_size, sequence_length)\n",
    "        # Apply embedding layer\n",
    "        #print(input_data.shape)\n",
    "        embedded = self.embedding(input_data)\n",
    "        #print(embedded.shape)\n",
    "        # Pass through the transformer encoder\n",
    "        transformed = self.transformer_encoder(embedded)\n",
    "        #print(transformed.shape) #same as input duh: batch x sequence_length x embedding_dim\n",
    "        flattened_tensor = transformed.view(-1,sequence_length*embedding_dim)\n",
    "        f1 = F.relu(self.fc1(flattened_tensor))\n",
    "        \n",
    "        f2 = F.relu(self.fc2(f1))\n",
    "        out = self.output_layer(f2)\n",
    "        # Apply the output layer\n",
    "        output = F.softmax(out) # should be output = F.softmax(out,dim=-1) or output = F.softmax(out,dim=1)\n",
    "\n",
    "        return output\n",
    "    \n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cca2a6b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 16\n",
    "num_heads = 8\n",
    "hidden_dim  = 11\n",
    "num_layers = 2\n",
    "dropout = 0.1 #tried with zero, same prob...\n",
    "vocab_size = 80\n",
    "class_num = vocab_size\n",
    "batch_size = 23\n",
    "sequence_length = 160"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "a7b5a7d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "SLM = SimpleLanguageModel(vocab_size, sequence_length, embedding_dim, class_num, num_heads, hidden_dim, num_layers, dropout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "d8fc6fde",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SimpleLanguageModel(\n",
       "  (embedding): Embedding(80, 16)\n",
       "  (transformer_encoder): TransformerEncoder(\n",
       "    (transformer): TransformerEncoder(\n",
       "      (layers): ModuleList(\n",
       "        (0): TransformerEncoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=16, out_features=16, bias=True)\n",
       "          )\n",
       "          (linear1): Linear(in_features=16, out_features=11, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (linear2): Linear(in_features=11, out_features=16, bias=True)\n",
       "          (norm1): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout1): Dropout(p=0.1, inplace=False)\n",
       "          (dropout2): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (1): TransformerEncoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=16, out_features=16, bias=True)\n",
       "          )\n",
       "          (linear1): Linear(in_features=16, out_features=11, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (linear2): Linear(in_features=11, out_features=16, bias=True)\n",
       "          (norm1): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout1): Dropout(p=0.1, inplace=False)\n",
       "          (dropout2): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (fc1): Linear(in_features=2560, out_features=1000, bias=True)\n",
       "  (fc2): Linear(in_features=1000, out_features=500, bias=True)\n",
       "  (output_layer): Linear(in_features=500, out_features=80, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "85578ee4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([23, 160])\n",
      "torch.Size([23, 160, 16])\n",
      "torch.Size([23, 160, 16])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/hq/zz_5zb251633yqqbh6076p782zzx8q/T/ipykernel_23559/767507129.py:44: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  output = F.softmax(out) # should be output = F.softmax(out,dim=-1) or output = F.softmax(out,dim=1)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([23, 80])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_data = torch.randint(low=0, high=vocab_size, size=(batch_size, sequence_length))\n",
    "print(input_data.shape)\n",
    "SLM(input_data).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a3ba38e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3105922"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_parameters(SLM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c613c4cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "teacher_slm = Teacher(SLM,(sequence_length,)) #don't specify batch!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a25b374a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expected tensor for argument #1 'indices' to have one of the following scalar types: Long, Int; but got torch.FloatTensor instead (while checking arguments for embedding)\n",
      "lets try ints!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/hq/zz_5zb251633yqqbh6076p782zzx8q/T/ipykernel_23559/1194099902.py:44: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  output = F.softmax(out) # should be output = F.softmax(out,dim=-1) or output = F.softmax(out,dim=1)\n"
     ]
    }
   ],
   "source": [
    "teacher_slm.load_state_dict('good_teacher.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb01c427",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.load_state_dict(torch.load('model_state.pth'))\n",
    "#model.eval()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5e4db9fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "##some of these configs made for more diverse outputs in teachers:\n",
    "config_args = {\"dist_type\" : \"ints\" ##worked well\n",
    "                      , \"gen_m\" : vocab_size\n",
    "                      , \"gen_n\" : 100\n",
    "                      , \"gen_epochs\" : 100\n",
    "                      , \"gen_lr\" : 0.001\n",
    "                      , \"random_shuffle\" : 0.1\n",
    "                      , \"out_type\" : \"one-hot\" }\n",
    "\n",
    "config_args_high_epochs = {\"dist_type\" : \"ints\" ##okay, but not as well as config_args.\n",
    "                      , \"gen_m\" : vocab_size\n",
    "                      , \"gen_n\" : 1000\n",
    "                      , \"gen_epochs\" : 100\n",
    "                      , \"gen_lr\" : 0.001\n",
    "                      , \"random_shuffle\" : 0.1\n",
    "                      , \"out_type\" : \"one-hot\" }\n",
    "\n",
    "config_args_higher_lr = {\"dist_type\" : \"ints\" ##lower was worse.  raise it. 0.003 looks great.  this is the best.\n",
    "                      , \"gen_m\" : vocab_size\n",
    "                      , \"gen_n\" : 2000\n",
    "                      , \"gen_epochs\" : 50\n",
    "                      , \"gen_lr\" :  0.003 ##0.003\n",
    "                      , \"random_shuffle\" : 0.8\n",
    "                      , \"out_type\" : \"one-hot\" }\n",
    "\n",
    "config_args_less_data = {\"dist_type\" : \"ints\" ##worse\n",
    "                      , \"gen_m\" : vocab_size\n",
    "                      , \"gen_n\" : 500\n",
    "                      , \"gen_epochs\" : 100\n",
    "                      , \"gen_lr\" : 0.003\n",
    "                      , \"random_shuffle\" : 0.1\n",
    "                      , \"out_type\" : \"one-hot\" }\n",
    "\n",
    "config_args_high_shuffle = {\"dist_type\" : \"ints\" ##one bar..\n",
    "                      , \"gen_m\" : vocab_size\n",
    "                      , \"gen_n\" : 10_000\n",
    "                      , \"gen_epochs\" : 10\n",
    "                      , \"gen_lr\" : 0.05\n",
    "                      , \"random_shuffle\" : 0.9\n",
    "                      , \"out_type\" : \"one-hot\" }\n",
    "\n",
    "config_args_small_batch = {\"dist_type\" : \"ints\" ##this one was the first to do well.  not just one bar and the rest nearly zero.\n",
    "                      , \"gen_m\" : vocab_size\n",
    "                      , \"gen_n\" : 10_000\n",
    "                      , \"gen_epochs\" : 10\n",
    "                      , \"gen_lr\" : 0.05\n",
    "                      , \"random_shuffle\" : 0.5\n",
    "                      , \"batch_size\" : 10\n",
    "                      , \"out_type\" : \"one-hot\" }\n",
    "\n",
    "config_ab = {\"dist_type\" : \"ints\" ##worked well\n",
    "                      , \"gen_m\" : vocab_size\n",
    "                      , \"gen_n\" : 5000\n",
    "                      , \"gen_epochs\" : 200\n",
    "                      , \"gen_lr\" : 0.005\n",
    "                      , \"random_shuffle\" : 0.0\n",
    "                      , \"out_type\" : \"one-hot\" \n",
    "                      , \"dist_type\" : 'hetero'\n",
    "                      , \"alpha\" : 1\n",
    "                      , \"beta\" : 4} #maybe increase epochs?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fb0ccfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#teacher_slm.configure(**config_ab) #this is dying.  might be time for colab!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "77f954d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating val data ::   0%|                            | 0/200 [00:00<?, ?it/s]/var/folders/hq/zz_5zb251633yqqbh6076p782zzx8q/T/ipykernel_23559/1194099902.py:44: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  output = F.softmax(out) # should be output = F.softmax(out,dim=-1) or output = F.softmax(out,dim=1)\n",
      "Generating val data :: 100%|██████████████████| 200/200 [00:04<00:00, 47.96it/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "args = { 'val_train' : \"val\"\n",
    "                      , 'n' : 10_000\n",
    "                      , 'dist_type' : 'ints'\n",
    "                      , 'm' : vocab_size\n",
    "                      , 'std': 1.0\n",
    "                      , 'alpha' : 1\n",
    "                      , 'beta' : 3\n",
    "                      , 'store_outputs' : True\n",
    "        }\n",
    "teacher_slm.generate_data(**args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fb63843",
   "metadata": {},
   "outputs": [],
   "source": [
    "##torch.save(teacher_slm.model.state_dict(), 'good_teacher.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f3ca1f01",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.5425419 ,  0.35136947, -0.03018094, ..., -0.15405135,\n",
       "         1.7438477 , -0.98630935],\n",
       "       [-1.6164193 , -1.8916837 , -0.8887659 , ..., -0.15044953,\n",
       "        -0.22066197, -1.287303  ],\n",
       "       [ 0.34314165,  0.25870025, -0.93632895, ...,  1.0878774 ,\n",
       "         0.89934933, -0.45346054],\n",
       "       ...,\n",
       "       [ 0.64084125, -1.4381758 ,  1.0926732 , ...,  2.0109196 ,\n",
       "         0.51697874, -0.04919771],\n",
       "       [-0.22420782,  1.1749339 ,  0.90297115, ...,  0.2145888 ,\n",
       "        -0.39479327,  0.79994994],\n",
       "       [-0.10181472, -0.19704515,  0.46629068, ..., -0.37408283,\n",
       "         1.2688403 , -0.0953627 ]], dtype=float32)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "teacher_slm.model.embedding.weight.detach().numpy() #use this to make the matrix, just linear? not sure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "513eec1f",
   "metadata": {},
   "outputs": [],
   "source": [
    " #the code is right, i just need this to be a better dist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "24c7711d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 2880x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "teacher_slm.graph_dataset_dist(val_train = 'val')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "8154cdf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/hq/zz_5zb251633yqqbh6076p782zzx8q/T/ipykernel_23559/1194099902.py:44: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  output = F.softmax(out) # should be output = F.softmax(out,dim=-1) or output = F.softmax(out,dim=1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(False)\n",
      "tensor(True)\n",
      "tensor(True)\n",
      "tensor(True)\n",
      "tensor(True)\n",
      "tensor(True)\n",
      "tensor(False)\n",
      "tensor(True)\n",
      "tensor(True)\n",
      "tensor(False)\n",
      "tensor(True)\n",
      "tensor(False)\n",
      "tensor(True)\n",
      "tensor(False)\n",
      "tensor(True)\n",
      "tensor(True)\n",
      "tensor(False)\n",
      "tensor(False)\n",
      "tensor(False)\n",
      "tensor(False)\n",
      "tensor(True)\n",
      "tensor(True)\n",
      "tensor(True)\n",
      "tensor(False)\n",
      "tensor(False)\n",
      "tensor(True)\n",
      "tensor(False)\n",
      "tensor(False)\n",
      "tensor(True)\n",
      "tensor(True)\n",
      "tensor(True)\n",
      "tensor(True)\n",
      "tensor(True)\n",
      "tensor(False)\n",
      "tensor(False)\n",
      "tensor(True)\n",
      "tensor(False)\n",
      "tensor(True)\n",
      "tensor(True)\n",
      "tensor(True)\n",
      "tensor(True)\n",
      "tensor(True)\n",
      "tensor(True)\n",
      "tensor(True)\n",
      "tensor(True)\n",
      "tensor(False)\n",
      "tensor(False)\n",
      "tensor(False)\n",
      "tensor(True)\n",
      "tensor(True)\n",
      "tensor(False)\n",
      "tensor(True)\n",
      "tensor(True)\n",
      "tensor(True)\n",
      "tensor(False)\n",
      "tensor(True)\n",
      "tensor(True)\n",
      "tensor(True)\n",
      "tensor(False)\n",
      "tensor(False)\n",
      "tensor(True)\n",
      "tensor(True)\n",
      "tensor(False)\n",
      "tensor(True)\n",
      "tensor(False)\n",
      "tensor(False)\n",
      "tensor(True)\n",
      "tensor(True)\n",
      "tensor(False)\n",
      "tensor(True)\n",
      "tensor(False)\n",
      "tensor(False)\n",
      "tensor(True)\n",
      "tensor(True)\n",
      "tensor(False)\n",
      "tensor(False)\n",
      "tensor(True)\n",
      "tensor(True)\n",
      "tensor(True)\n",
      "tensor(True)\n",
      "tensor(False)\n",
      "tensor(False)\n",
      "tensor(False)\n",
      "tensor(False)\n",
      "tensor(True)\n",
      "tensor(False)\n",
      "tensor(True)\n",
      "tensor(False)\n",
      "tensor(False)\n",
      "tensor(False)\n",
      "tensor(True)\n",
      "tensor(False)\n",
      "tensor(False)\n",
      "tensor(False)\n",
      "tensor(True)\n",
      "tensor(False)\n",
      "tensor(False)\n",
      "tensor(False)\n",
      "tensor(True)\n",
      "tensor(True)\n"
     ]
    }
   ],
   "source": [
    "#teacher_slm.model.eval() #irrelevant\n",
    "for i in range(100):\n",
    "    print(torch.all(teacher_slm.model(teacher_slm.val_inputs[i]) == teacher_slm.val_targets[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0b6de08b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(True)\n",
      "tensor(True)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/hq/zz_5zb251633yqqbh6076p782zzx8q/T/ipykernel_23559/1194099902.py:44: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  output = F.softmax(out) # should be output = F.softmax(out,dim=-1) or output = F.softmax(out,dim=1)\n"
     ]
    }
   ],
   "source": [
    "print(torch.all(teacher_slm.model(teacher_slm.val_inputs[:50]) == teacher_slm.val_targets[:50]))\n",
    "#in test is concatenated the same way as the outputs in KDNoise.  only difference.  doesn't appear to be different.  \n",
    "print(torch.all(teacher_slm.model(teacher_slm.val_inputs[50:100]) == teacher_slm.val_targets[50:100])) ##yeah, val_inputs and in_test appear to be the same.\n",
    "#torch.all(teacher_slm.model(teacher_slm.in_test[:49]) == teacher_slm.val_targets[:49]) ##this doesn't/.  huh???!!\n",
    "#torch.all(teacher_slm.model(teacher_slm.in_test[:50]) == teacher_slm.model(teacher_slm.val_inputs[:50])) ##okay, these match too\n",
    "#torch.all(teacher_slm.model(teacher_slm.in_test[223:409]) == teacher_slm.model(teacher_slm.val_inputs[223:409])) #these match.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b7d79bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "teacher_slm.val_inputs.shape, teacher_slm.val_targets.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9bc7ec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.all(teacher_slm.model(teacher_slm.val_inputs[1]) ==  teacher_slm.val_targets[1]) ##torch.allclose doesn't work either"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "909987b0",
   "metadata": {},
   "source": [
    "so, it looks like it is predicting correctly, but there is something wrong with the batch? they match when you pass in 50, but more, or not at exactly the 50 marks, it doesn't all match.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b1e92fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#added teacher_slm.in_test and teacher_slm.dataloader\n",
    "teacher_slm.data_loader\n",
    "x = None\n",
    "y = None\n",
    "for batch_samples in teacher_slm.data_loader:\n",
    "    # Perform inference on each batch\n",
    "    batch_outputs = teacher_slm.model(batch_samples[0])  # Assuming samples are in the first element of the batch\n",
    "    print(batch_samples[0])\n",
    "    x = batch_samples[0]\n",
    "    print(batch_outputs)\n",
    "    y = batch_outputs\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7e63715",
   "metadata": {},
   "outputs": [],
   "source": [
    "#okay, the first 50 match.  do they \n",
    "torch.all(x == teacher_slm.val_inputs[0:50]),torch.all(y == teacher_slm.val_targets[0:50]),torch.all(teacher_slm.model(x) == y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92b1012b",
   "metadata": {},
   "outputs": [],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "444f7b2a",
   "metadata": {},
   "source": [
    "specify gen outputs.  we need one hots\n",
    "i can do this better than chat gpt lol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5916bb4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "##this function works, it is in LABNET.  \n",
    "def one_hot_last_dim(tensor_shape):\n",
    "    num_classes = tensor_shape[-1]\n",
    "    random_idx = np.random.randint(1, num_classes + 1, size=tensor_shape[:-1])\n",
    "    zero_tensor = np.zeros(tensor_shape, dtype=int)\n",
    "    last_dim_indices = np.arange(num_classes)\n",
    "    zero_tensor[..., :, last_dim_indices] = (random_idx[..., np.newaxis] == last_dim_indices)\n",
    "    return zero_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67fabd52",
   "metadata": {},
   "outputs": [],
   "source": [
    "axis_to_shuffle = 0\n",
    "\n",
    "# Shuffle the tensor along the specified axis\n",
    "shuffled_tensor = np.take(x, np.random.permutation(x.shape[0]), axis=0)\n",
    "\n",
    "print(shuffled_tensor)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba3f68eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "\n",
    "for i in range(10):\n",
    "    random_number = random.random()\n",
    "    print(random_number)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb166fea",
   "metadata": {},
   "outputs": [],
   "source": [
    "#next is to extract the parts.  we need to take out the embedding layer, make it linear, and then take out the layer before softmax.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef060bed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.distributions as dist\n",
    "def generate_heteroskedastic_ints(n, alpha, beta, total_classes):\n",
    "    # Generate random values from the beta distribution\n",
    "    values = np.random.beta(alpha, beta, n)\n",
    "    min_value = 0\n",
    "    max_value = total_classes\n",
    "    scaled_values = min_value + (max_value - min_value) * values\n",
    "    \n",
    "    # Round the scaled values to integers\n",
    "    ints = np.round(scaled_values).astype(int)\n",
    "    \n",
    "    return ints\n",
    "\n",
    "# Set your desired alpha and beta values\n",
    "alpha = 1  # Adjust this to control skewness\n",
    "beta = 10  # Adjust this to control skewness\n",
    "\n",
    "# Generate 100 random integers with the specified heteroskedasticity\n",
    "random_integers = generate_heteroskedastic_ints(10_000, alpha, beta, 100)\n",
    "\n",
    "# Print the generated integers\n",
    "print(random_integers)\n",
    "plt.hist(random_integers, bins=20, edgecolor='k')\n",
    "plt.xlabel('Value')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Frequency Distribution of Random Integers')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d87621b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_heteroskedastic_ints(shape, alpha, beta, total_classes):\n",
    "    # Generate random values from the beta distribution\n",
    "    values = np.random.beta(alpha, beta, shape)\n",
    "    \n",
    "    # Scale the values to integers in the desired range (e.g., 0 to 100)\n",
    "    min_value = 0\n",
    "    max_value = total_classes\n",
    "    scaled_values = min_value + (max_value - min_value) * values\n",
    "    \n",
    "    # Round the scaled values to integers\n",
    "    ints = np.round(scaled_values).astype(int)\n",
    "    \n",
    "    return ints\n",
    "\n",
    "# Set your desired alpha and beta values\n",
    "alpha = 1  # Adjust this to control skewness\n",
    "beta = 1   # Adjust this to control skewness\n",
    "\n",
    "# Specify the shape of the tensor (e.g., b x 200)\n",
    "b = 5\n",
    "shape = (b, 200)\n",
    "\n",
    "# Generate a tensor of random integers with the specified heteroskedasticity\n",
    "random_integers_tensor = generate_heteroskedastic_ints(shape, alpha, beta,20)\n",
    "\n",
    "# Print the generated tensor\n",
    "random_integers_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "19e56b2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([23, 160, 16])\n",
      "Model Output Shape: torch.Size([23, 80])\n"
     ]
    }
   ],
   "source": [
    "class ToyTransformer(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, num_heads, hidden_dim, num_layers, dropout,sequence_length):\n",
    "        super(ToyTransformer, self).__init__()\n",
    "\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(\n",
    "            nn.TransformerEncoderLayer(embedding_dim, num_heads, hidden_dim, dropout, batch_first=True),\n",
    "            num_layers\n",
    "        )\n",
    "        self.fc1 = nn.Linear(embedding_dim * sequence_length, vocab_size)  # Intermediate linear layer\n",
    "        self.fc2 = nn.Linear(vocab_size, vocab_size)  # Final linear layer\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        print(x.shape)\n",
    "        x = self.transformer_encoder(x)\n",
    "        x = torch.flatten(x, start_dim=1)  # Flatten the output\n",
    "        x = F.relu(self.fc1(x))  # Apply the intermediate linear layer with ReLU activation\n",
    "        x = self.fc2(x)  # Apply the final linear layer\n",
    "        x = F.softmax(x, dim=1)  # Apply softmax activation\n",
    "        return x\n",
    "\n",
    "# Set the hyperparameters\n",
    "embedding_dim = 16\n",
    "num_heads = 8\n",
    "hidden_dim = 11\n",
    "num_layers = 2\n",
    "dropout = 0.1\n",
    "vocab_size = 80\n",
    "class_num = vocab_size\n",
    "batch_size = 23\n",
    "sequence_length = 160\n",
    "\n",
    "# Create an instance of the toy Transformer model\n",
    "model = ToyTransformer(vocab_size, embedding_dim, num_heads, hidden_dim, num_layers, dropout,sequence_length)\n",
    "\n",
    "# Generate random input data for testing\n",
    "input_data = torch.randint(0, vocab_size, (batch_size, sequence_length))\n",
    "\n",
    "# Forward pass through the model\n",
    "output = model(input_data)\n",
    "\n",
    "print(\"Model Output Shape:\", output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "b66b46d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "TT = ToyTransformer(vocab_size, embedding_dim, num_heads, hidden_dim, num_layers, dropout,sequence_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "d58d3d50",
   "metadata": {},
   "outputs": [],
   "source": [
    "teacher_toy = Teacher(TT,(sequence_length,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9a1c4bd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expected tensor for argument #1 'indices' to have one of the following scalar types: Long, Int; but got torch.FloatTensor instead (while checking arguments for embedding)\n",
      "lets try ints!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Configuring Teacher:: 100%|███████████████████| 200/200 [12:54<00:00,  3.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Teacher Configured\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "teacher_toy.configure(**config_ab) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "602e9173",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating val data :: 100%|██████████████████| 200/200 [00:03<00:00, 55.66it/s]\n"
     ]
    }
   ],
   "source": [
    "args = { 'val_train' : \"val\"\n",
    "                      , 'n' : 10_000\n",
    "                      , 'dist_type' : 'ints'\n",
    "                      , 'm' : vocab_size\n",
    "                      , 'std': 1.0\n",
    "                      , 'alpha' : 1\n",
    "                      , 'beta' : 3\n",
    "                      , 'store_outputs' : True\n",
    "        }\n",
    "teacher_toy.model.eval()\n",
    "teacher_toy.generate_data(**args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "49a2446c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#torch.save(teacher_toy.model.state_dict(), 'good_toy.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7fec079e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 80])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "teacher_toy.model(teacher_toy.val_inputs[0:10]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6a385b02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expected tensor for argument #1 'indices' to have one of the following scalar types: Long, Int; but got torch.FloatTensor instead (while checking arguments for embedding)\n",
      "lets try ints!\n"
     ]
    }
   ],
   "source": [
    "teacher_toy.load_state_dict('good_toy.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e3dc939a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ToyTransformer(\n",
       "  (embedding): Embedding(80, 16)\n",
       "  (transformer_encoder): TransformerEncoder(\n",
       "    (layers): ModuleList(\n",
       "      (0): TransformerEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=16, out_features=16, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=16, out_features=11, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (linear2): Linear(in_features=11, out_features=16, bias=True)\n",
       "        (norm1): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (1): TransformerEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=16, out_features=16, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=16, out_features=11, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (linear2): Linear(in_features=11, out_features=16, bias=True)\n",
       "        (norm1): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (fc1): Linear(in_features=2560, out_features=80, bias=True)\n",
       "  (fc2): Linear(in_features=80, out_features=80, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "teacher_toy.model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "56202d91",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0000e+00, 1.4262e-28, 0.0000e+00, 0.0000e+00, 0.0000e+00, 7.6707e-42,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 1.4013e-44, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 1.1222e-26, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 2.1546e-33,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00]], grad_fn=<SoftmaxBackward0>)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "teacher_toy.model(teacher_toy.val_inputs[0:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "4fc6be98",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0000e+00, 9.2603e-41, 2.9759e-10, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 1.7814e-22, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 3.3963e-30, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.0000e+00,\n",
       "         0.0000e+00, 1.3444e-12, 0.0000e+00, 0.0000e+00, 9.0623e-19, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 4.8619e-15, 0.0000e+00, 0.0000e+00, 1.5204e-09, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 3.9358e-07, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.3352e-35,\n",
       "         1.8217e-44, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00]])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "teacher_toy.val_targets[0:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "d7997667",
   "metadata": {},
   "outputs": [],
   "source": [
    "TT = ToyTransformer(vocab_size, embedding_dim, num_heads, hidden_dim, num_layers, dropout,sequence_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "7847f42d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TT.load_state_dict(torch.load('good_toy.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "e7dc8124",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ToyTransformer(\n",
       "  (embedding): Embedding(80, 16)\n",
       "  (transformer_encoder): TransformerEncoder(\n",
       "    (layers): ModuleList(\n",
       "      (0): TransformerEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=16, out_features=16, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=16, out_features=11, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (linear2): Linear(in_features=11, out_features=16, bias=True)\n",
       "        (norm1): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (1): TransformerEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=16, out_features=16, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=16, out_features=11, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (linear2): Linear(in_features=11, out_features=16, bias=True)\n",
       "        (norm1): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (fc1): Linear(in_features=2560, out_features=80, bias=True)\n",
       "  (fc2): Linear(in_features=80, out_features=80, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TT.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "311f2638",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "gen_shape = (100, sequence_length)\n",
    "samples = np.random.randint(0, high=vocab_size, size=gen_shape)\n",
    "samples = torch.from_numpy(samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "54cdd7c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "ttall = TT(samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "c45c87e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 160, 16])\n",
      "torch.Size([100, 160, 16])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([[0.0000e+00, 0.0000e+00, 1.0000e+00, 0.0000e+00, 0.0000e+00, 7.4626e-36,\n",
       "          0.0000e+00, 4.2039e-45, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "          0.0000e+00, 3.0272e-21, 0.0000e+00, 3.4208e-24, 2.8026e-45, 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 2.8026e-44,\n",
       "          0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 2.5504e-43, 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00, 4.0892e-37, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "          1.9452e-34, 0.0000e+00, 0.0000e+00, 4.0638e-44, 0.0000e+00, 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00]], grad_fn=<SliceBackward0>),\n",
       " tensor([[0.0000e+00, 0.0000e+00, 1.0000e+00, 0.0000e+00, 0.0000e+00, 7.4626e-36,\n",
       "          0.0000e+00, 4.2039e-45, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "          0.0000e+00, 3.0272e-21, 0.0000e+00, 3.4208e-24, 2.8026e-45, 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 2.8026e-44,\n",
       "          0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 2.5504e-43, 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00, 4.0892e-37, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "          1.9452e-34, 0.0000e+00, 0.0000e+00, 4.0638e-44, 0.0000e+00, 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00]], grad_fn=<SliceBackward0>))"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TT(samples[:10])[0:1],TT(samples[:])[0:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "0674e709",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['T_destination',\n",
       " '__annotations__',\n",
       " '__call__',\n",
       " '__class__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattr__',\n",
       " '__getattribute__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__le__',\n",
       " '__lt__',\n",
       " '__module__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__setstate__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__weakref__',\n",
       " '_apply',\n",
       " '_backward_hooks',\n",
       " '_buffers',\n",
       " '_call_impl',\n",
       " '_forward_hooks',\n",
       " '_forward_pre_hooks',\n",
       " '_get_backward_hooks',\n",
       " '_get_name',\n",
       " '_is_full_backward_hook',\n",
       " '_load_from_state_dict',\n",
       " '_load_state_dict_post_hooks',\n",
       " '_load_state_dict_pre_hooks',\n",
       " '_maybe_warn_non_full_backward_hook',\n",
       " '_modules',\n",
       " '_named_members',\n",
       " '_non_persistent_buffers_set',\n",
       " '_parameters',\n",
       " '_register_load_state_dict_pre_hook',\n",
       " '_register_state_dict_hook',\n",
       " '_replicate_for_data_parallel',\n",
       " '_save_to_state_dict',\n",
       " '_slow_forward',\n",
       " '_state_dict_hooks',\n",
       " '_version',\n",
       " 'add_module',\n",
       " 'apply',\n",
       " 'bfloat16',\n",
       " 'buffers',\n",
       " 'children',\n",
       " 'cpu',\n",
       " 'cuda',\n",
       " 'double',\n",
       " 'dump_patches',\n",
       " 'embedding',\n",
       " 'eval',\n",
       " 'extra_repr',\n",
       " 'fc1',\n",
       " 'fc2',\n",
       " 'float',\n",
       " 'forward',\n",
       " 'get_buffer',\n",
       " 'get_extra_state',\n",
       " 'get_parameter',\n",
       " 'get_submodule',\n",
       " 'half',\n",
       " 'ipu',\n",
       " 'load_state_dict',\n",
       " 'modules',\n",
       " 'named_buffers',\n",
       " 'named_children',\n",
       " 'named_modules',\n",
       " 'named_parameters',\n",
       " 'parameters',\n",
       " 'register_backward_hook',\n",
       " 'register_buffer',\n",
       " 'register_forward_hook',\n",
       " 'register_forward_pre_hook',\n",
       " 'register_full_backward_hook',\n",
       " 'register_load_state_dict_post_hook',\n",
       " 'register_module',\n",
       " 'register_parameter',\n",
       " 'requires_grad_',\n",
       " 'set_extra_state',\n",
       " 'share_memory',\n",
       " 'state_dict',\n",
       " 'to',\n",
       " 'to_empty',\n",
       " 'train',\n",
       " 'training',\n",
       " 'transformer_encoder',\n",
       " 'type',\n",
       " 'xpu',\n",
       " 'zero_grad']"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(TT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42d92889",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "604e9b4d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method Module.requires_grad_ of ToyTransformer(\n",
       "  (embedding): Embedding(80, 16)\n",
       "  (transformer_encoder): TransformerEncoder(\n",
       "    (layers): ModuleList(\n",
       "      (0): TransformerEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=16, out_features=16, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=16, out_features=11, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "        (linear2): Linear(in_features=11, out_features=16, bias=True)\n",
       "        (norm1): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.0, inplace=False)\n",
       "        (dropout2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (1): TransformerEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=16, out_features=16, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=16, out_features=11, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "        (linear2): Linear(in_features=11, out_features=16, bias=True)\n",
       "        (norm1): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.0, inplace=False)\n",
       "        (dropout2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (fc1): Linear(in_features=2560, out_features=80, bias=True)\n",
       "  (fc2): Linear(in_features=80, out_features=80, bias=True)\n",
       ")>"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TT.requires_grad_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "391f8caa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([160, 23])"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "b02a12db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([100, 160])"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "samples.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5711b5d2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
